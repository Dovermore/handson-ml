{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chapter 9 â€“ Up and running with TensorFlow**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_This notebook contains all the sample code and solutions to the exercises in chapter 9._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's make sure this notebook works well in both python 2 and 3, import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"tensorflow\"\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True):\n",
    "    path = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID, fig_id + \".png\")\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format='png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating and running a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "x = tf.Variable(3, name=\"x\")\n",
    "y = tf.Variable(4, name=\"y\")\n",
    "f = x*x*y + y + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add_1:0' shape=() dtype=int32>"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(x.initializer)\n",
    "sess.run(y.initializer)\n",
    "result = sess.run(f)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    x.initializer.run()\n",
    "    y.initializer.run()\n",
    "    result = f.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    result = f.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "init.run()\n",
    "result = f.eval()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Managing graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "x1 = tf.Variable(1)\n",
    "x1.graph is tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    x2 = tf.Variable(2)\n",
    "\n",
    "x2.graph is graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2.graph is tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "w = tf.constant(3)\n",
    "x = w + 2\n",
    "y = x + 5\n",
    "z = x * 3\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(y.eval())  # 10\n",
    "    print(z.eval())  # 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    y_val, z_val = sess.run([y, z])\n",
    "    print(y_val)  # 10\n",
    "    print(z_val)  # 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Normal Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "m, n = housing.data.shape\n",
    "housing_data_plus_bias = np.c_[np.ones((m, 1)), housing.data]\n",
    "\n",
    "X = tf.constant(housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "XT = tf.transpose(X)\n",
    "theta = tf.matmul(tf.matmul(tf.matrix_inverse(tf.matmul(XT, X)), XT), y)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    theta_value = theta.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -3.74651413e+01],\n",
       "       [  4.35734153e-01],\n",
       "       [  9.33829229e-03],\n",
       "       [ -1.06622010e-01],\n",
       "       [  6.44106984e-01],\n",
       "       [ -4.25131839e-06],\n",
       "       [ -3.77322501e-03],\n",
       "       [ -4.26648885e-01],\n",
       "       [ -4.40514028e-01]], dtype=float32)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare with pure NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -3.69419202e+01]\n",
      " [  4.36693293e-01]\n",
      " [  9.43577803e-03]\n",
      " [ -1.07322041e-01]\n",
      " [  6.45065694e-01]\n",
      " [ -3.97638942e-06]\n",
      " [ -3.78654266e-03]\n",
      " [ -4.21314378e-01]\n",
      " [ -4.34513755e-01]]\n"
     ]
    }
   ],
   "source": [
    "X = housing_data_plus_bias\n",
    "y = housing.target.reshape(-1, 1)\n",
    "theta_numpy = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
    "\n",
    "print(theta_numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare with Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -3.69419202e+01]\n",
      " [  4.36693293e-01]\n",
      " [  9.43577803e-03]\n",
      " [ -1.07322041e-01]\n",
      " [  6.45065694e-01]\n",
      " [ -3.97638942e-06]\n",
      " [ -3.78654265e-03]\n",
      " [ -4.21314378e-01]\n",
      " [ -4.34513755e-01]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(housing.data, housing.target.reshape(-1, 1))\n",
    "\n",
    "print(np.r_[lin_reg.intercept_.reshape(-1, 1), lin_reg.coef_.T])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Descent requires scaling the feature vectors first. We could do this using TF, but let's just use Scikit-Learn for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaled_housing_data = scaler.fit_transform(housing.data)\n",
    "scaled_housing_data_plus_bias = np.c_[np.ones((m, 1)), scaled_housing_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.00000000e+00   6.60969987e-17   5.50808322e-18   6.60969987e-17\n",
      "  -1.06030602e-16  -1.10161664e-17   3.44255201e-18  -1.07958431e-15\n",
      "  -8.52651283e-15]\n",
      "[ 0.38915536  0.36424355  0.5116157  ..., -0.06612179 -0.06360587\n",
      "  0.01359031]\n",
      "0.111111111111\n",
      "(20640, 9)\n"
     ]
    }
   ],
   "source": [
    "print(scaled_housing_data_plus_bias.mean(axis=0))\n",
    "print(scaled_housing_data_plus_bias.mean(axis=1))\n",
    "print(scaled_housing_data_plus_bias.mean())\n",
    "print(scaled_housing_data_plus_bias.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually computing the gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 9.16154\n",
      "Epoch 100 MSE = 0.714501\n",
      "Epoch 200 MSE = 0.566705\n",
      "Epoch 300 MSE = 0.555572\n",
      "Epoch 400 MSE = 0.548812\n",
      "Epoch 500 MSE = 0.543636\n",
      "Epoch 600 MSE = 0.539629\n",
      "Epoch 700 MSE = 0.536509\n",
      "Epoch 800 MSE = 0.534068\n",
      "Epoch 900 MSE = 0.532147\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "gradients = 2/m * tf.matmul(tf.transpose(X), error)\n",
    "training_op = tf.assign(theta, theta - learning_rate * gradients) # can use placeholder to hold (theta - ...) for\n",
    "                                                                  # more concise representation\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval())\n",
    "        sess.run(training_op)\n",
    "    \n",
    "    best_theta = theta.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'theta:0' shape=(9, 1) dtype=float32_ref>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.06855249],\n",
       "       [ 0.88740271],\n",
       "       [ 0.14401658],\n",
       "       [-0.34770882],\n",
       "       [ 0.36178368],\n",
       "       [ 0.00393812],\n",
       "       [-0.04269557],\n",
       "       [-0.66145277],\n",
       "       [-0.63752776]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using autodiff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as above except for the `gradients = ...` line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gradients = tf.gradients(mse, [theta])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 9.16154\n",
      "Epoch 100 MSE = 0.714501\n",
      "Epoch 200 MSE = 0.566705\n",
      "Epoch 300 MSE = 0.555572\n",
      "Epoch 400 MSE = 0.548812\n",
      "Epoch 500 MSE = 0.543636\n",
      "Epoch 600 MSE = 0.539629\n",
      "Epoch 700 MSE = 0.536509\n",
      "Epoch 800 MSE = 0.534068\n",
      "Epoch 900 MSE = 0.532147\n",
      "Best theta:\n",
      "[[ 2.06855249]\n",
      " [ 0.88740271]\n",
      " [ 0.14401658]\n",
      " [-0.34770882]\n",
      " [ 0.36178368]\n",
      " [ 0.00393811]\n",
      " [-0.04269556]\n",
      " [-0.66145277]\n",
      " [-0.6375277 ]]\n"
     ]
    }
   ],
   "source": [
    "training_op = tf.assign(theta, theta - learning_rate * gradients)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval())\n",
    "        sess.run(training_op)\n",
    "    \n",
    "    best_theta = theta.eval()\n",
    "\n",
    "print(\"Best theta:\")\n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How could you find the partial derivatives of the following function with regards to `a` and `b`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_func(a, b):\n",
    "    z = 0\n",
    "    for i in range(100):\n",
    "        z = a * np.cos(z + i) + z * np.sin(b - i)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.21253923284754916"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_func(0.2, 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "a = tf.Variable(0.2, name=\"a\")\n",
    "b = tf.Variable(0.3, name=\"b\")\n",
    "z = tf.constant(0.0, name=\"z0\")\n",
    "for i in range(100):\n",
    "    z = a * tf.cos(z + i) + z * tf.sin(b - i)\n",
    "\n",
    "grads = tf.gradients(z, [a, b])\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the function at $a=0.2$ and $b=0.3$, and the partial derivatives at that point with regards to $a$ and with regards to $b$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.212537\n",
      "[-1.1388494, 0.19671395]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    print(z.eval())\n",
    "    print(sess.run(grads))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a `GradientDescentOptimizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 9.16154\n",
      "Epoch 100 MSE = 0.714501\n",
      "Epoch 200 MSE = 0.566705\n",
      "Epoch 300 MSE = 0.555572\n",
      "Epoch 400 MSE = 0.548812\n",
      "Epoch 500 MSE = 0.543636\n",
      "Epoch 600 MSE = 0.539629\n",
      "Epoch 700 MSE = 0.536509\n",
      "Epoch 800 MSE = 0.534068\n",
      "Epoch 900 MSE = 0.532147\n",
      "Best theta:\n",
      "[[ 2.06855249]\n",
      " [ 0.88740271]\n",
      " [ 0.14401658]\n",
      " [-0.34770882]\n",
      " [ 0.36178368]\n",
      " [ 0.00393811]\n",
      " [-0.04269556]\n",
      " [-0.66145277]\n",
      " [-0.6375277 ]]\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval())\n",
    "        sess.run(training_op)\n",
    "    \n",
    "    best_theta = theta.eval()\n",
    "\n",
    "print(\"Best theta:\")\n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a momentum optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate,\n",
    "                                       momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best theta:\n",
      "[[ 2.06855798]\n",
      " [ 0.82962859]\n",
      " [ 0.11875337]\n",
      " [-0.26554456]\n",
      " [ 0.30571091]\n",
      " [-0.00450251]\n",
      " [-0.03932662]\n",
      " [-0.89986444]\n",
      " [-0.87052065]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        sess.run(training_op)\n",
    "    \n",
    "    best_theta = theta.eval()\n",
    "\n",
    "print(\"Best theta:\")\n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feeding data to the training algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Placeholder nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6.  7.  8.]]\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "A = tf.placeholder(tf.float32, shape=(None, 3))\n",
    "B = A + 5\n",
    "with tf.Session() as sess:\n",
    "    B_val_1 = B.eval(feed_dict={A: [[1, 2, 3]]})\n",
    "    B_val_2 = B.eval(feed_dict={A: [[4, 5, 6], [7, 8, 9]]})\n",
    "\n",
    "print(B_val_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  9.  10.  11.]\n",
      " [ 12.  13.  14.]]\n"
     ]
    }
   ],
   "source": [
    "print(B_val_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 1000\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n + 1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "n_batches = int(np.ceil(m / batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fetch_batch(epoch, batch_index, batch_size):\n",
    "    np.random.seed(epoch * n_batches + batch_index)  # not shown in the book\n",
    "    indices = np.random.randint(m, size=batch_size)  # not shown\n",
    "    X_batch = scaled_housing_data_plus_bias[indices] # not shown\n",
    "    y_batch = housing.target.reshape(-1, 1)[indices] # not shown\n",
    "    return X_batch, y_batch\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "\n",
    "    best_theta = theta.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.07033372],\n",
       "       [ 0.86371452],\n",
       "       [ 0.12255151],\n",
       "       [-0.31211874],\n",
       "       [ 0.38510373],\n",
       "       [ 0.00434168],\n",
       "       [-0.01232954],\n",
       "       [-0.83376896],\n",
       "       [-0.80304712]], dtype=float32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving and restoring a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 9.16154\n",
      "Epoch 100 MSE = 0.714501\n",
      "Epoch 200 MSE = 0.566705\n",
      "Epoch 300 MSE = 0.555572\n",
      "Epoch 400 MSE = 0.548812\n",
      "Epoch 500 MSE = 0.543636\n",
      "Epoch 600 MSE = 0.539629\n",
      "Epoch 700 MSE = 0.536509\n",
      "Epoch 800 MSE = 0.534068\n",
      "Epoch 900 MSE = 0.532147\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_epochs = 1000                                                                       # not shown in the book\n",
    "learning_rate = 0.01                                                                  # not shown\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")            # not shown\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")            # not shown\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")                                      # not shown\n",
    "error = y_pred - y                                                                    # not shown\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")                                    # not shown\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)            # not shown\n",
    "training_op = optimizer.minimize(mse)                                                 # not shown\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval())                                # not shown\n",
    "            save_path = saver.save(sess, \"/tmp/my_model.ckpt\")\n",
    "        sess.run(training_op)\n",
    "    \n",
    "    best_theta = theta.eval()\n",
    "    save_path = saver.save(sess, \"/tmp/my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.06855249],\n",
       "       [ 0.88740271],\n",
       "       [ 0.14401658],\n",
       "       [-0.34770882],\n",
       "       [ 0.36178368],\n",
       "       [ 0.00393811],\n",
       "       [-0.04269556],\n",
       "       [-0.66145277],\n",
       "       [-0.6375277 ]], dtype=float32)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/my_model_final.ckpt\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"/tmp/my_model_final.ckpt\")\n",
    "    best_theta_restored = theta.eval() # not shown in the book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(best_theta, best_theta_restored)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to have a saver that loads and restores `theta` with a different name, such as `\"weights\"`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver({\"weights\": theta})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default the saver also saves the graph structure itself in a second file with the extension `.meta`. You can use the function `tf.train.import_meta_graph()` to restore the graph structure. This function loads the graph into the default graph and returns a `Saver` that can then be used to restore the graph state (i.e., the variable values):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/my_model_final.ckpt\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "# notice that we start with an empty graph.\n",
    "\n",
    "saver = tf.train.import_meta_graph(\"/tmp/my_model_final.ckpt.meta\")  # this loads the graph structure\n",
    "theta = tf.get_default_graph().get_tensor_by_name(\"theta:0\") # not shown in the book\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"/tmp/my_model_final.ckpt\")  # this restores the graph's state\n",
    "    best_theta_restored = theta.eval() # not shown in the book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(best_theta, best_theta_restored)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that you can import a pretrained model without having to have the corresponding Python code to build the graph. This is very handy when you keep tweaking and saving your model: you can load a previously saved model without having to search for the version of the code that built it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the graph\n",
    "## inside Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output, Image, display, HTML\n",
    "\n",
    "def strip_consts(graph_def, max_const_size=32):\n",
    "    \"\"\"Strip large constant values from graph_def.\"\"\"\n",
    "    strip_def = tf.GraphDef()\n",
    "    for n0 in graph_def.node:\n",
    "        n = strip_def.node.add() \n",
    "        n.MergeFrom(n0)\n",
    "        if n.op == 'Const':\n",
    "            tensor = n.attr['value'].tensor\n",
    "            size = len(tensor.tensor_content)\n",
    "            if size > max_const_size:\n",
    "                tensor.tensor_content = b\"<stripped %d bytes>\"%size\n",
    "    return strip_def\n",
    "\n",
    "def show_graph(graph_def, max_const_size=32):\n",
    "    \"\"\"Visualize TensorFlow graph.\"\"\"\n",
    "    if hasattr(graph_def, 'as_graph_def'):\n",
    "        graph_def = graph_def.as_graph_def()\n",
    "    strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n",
    "    code = \"\"\"\n",
    "        <script>\n",
    "          function load() {{\n",
    "            document.getElementById(\"{id}\").pbtxt = {data};\n",
    "          }}\n",
    "        </script>\n",
    "        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n",
    "        <div style=\"height:600px\">\n",
    "          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n",
    "        </div>\n",
    "    \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n",
    "\n",
    "    iframe = \"\"\"\n",
    "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"{}\"></iframe>\n",
    "    \"\"\".format(code.replace('\"', '&quot;'))\n",
    "    display(HTML(iframe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"\n",
       "        <script>\n",
       "          function load() {\n",
       "            document.getElementById(&quot;graph0.3745401188473625&quot;).pbtxt = 'node {\\n  name: &quot;X&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 20640\\n          }\\n          dim {\\n            size: 9\\n          }\\n        }\\n        tensor_content: &quot;<stripped 743040 bytes>&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 20640\\n          }\\n          dim {\\n            size: 1\\n          }\\n        }\\n        tensor_content: &quot;<stripped 82560 bytes>&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\t\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 42\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 42\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;random_uniform/max&quot;\\n  input: &quot;random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;random_uniform/RandomUniform&quot;\\n  input: &quot;random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;random_uniform/mul&quot;\\n  input: &quot;random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;theta&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 9\\n        }\\n        dim {\\n          size: 1\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;theta/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;theta&quot;\\n  input: &quot;random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@theta&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;theta/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;theta&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@theta&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;predictions&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;X&quot;\\n  input: &quot;theta/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;predictions&quot;\\n  input: &quot;y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Square&quot;\\n  op: &quot;Square&quot;\\n  input: &quot;sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\000\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;mse&quot;\\n  op: &quot;Mean&quot;\\n  input: &quot;Square&quot;\\n  input: &quot;Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Fill&quot;\\n  op: &quot;Fill&quot;\\n  input: &quot;gradients/Shape&quot;\\n  input: &quot;gradients/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mse_grad/Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\001\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mse_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Fill&quot;\\n  input: &quot;gradients/mse_grad/Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mse_grad/Tile/multiples&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\240P\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mse_grad/Tile&quot;\\n  op: &quot;Tile&quot;\\n  input: &quot;gradients/mse_grad/Reshape&quot;\\n  input: &quot;gradients/mse_grad/Tile/multiples&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tmultiples&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mse_grad/Shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\240P\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mse_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mse_grad/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mse_grad/Prod&quot;\\n  op: &quot;Prod&quot;\\n  input: &quot;gradients/mse_grad/Shape&quot;\\n  input: &quot;gradients/mse_grad/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mse_grad/Const_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mse_grad/Prod_1&quot;\\n  op: &quot;Prod&quot;\\n  input: &quot;gradients/mse_grad/Shape_1&quot;\\n  input: &quot;gradients/mse_grad/Const_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mse_grad/Maximum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mse_grad/Maximum&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;gradients/mse_grad/Prod_1&quot;\\n  input: &quot;gradients/mse_grad/Maximum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mse_grad/floordiv&quot;\\n  op: &quot;FloorDiv&quot;\\n  input: &quot;gradients/mse_grad/Prod&quot;\\n  input: &quot;gradients/mse_grad/Maximum&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mse_grad/Cast&quot;\\n  op: &quot;Cast&quot;\\n  input: &quot;gradients/mse_grad/floordiv&quot;\\n  attr {\\n    key: &quot;DstT&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;SrcT&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mse_grad/truediv&quot;\\n  op: &quot;RealDiv&quot;\\n  input: &quot;gradients/mse_grad/Tile&quot;\\n  input: &quot;gradients/mse_grad/Cast&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Square_grad/mul/x&quot;\\n  op: &quot;Const&quot;\\n  input: &quot;^gradients/mse_grad/truediv&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 2.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Square_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/Square_grad/mul/x&quot;\\n  input: &quot;sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Square_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/mse_grad/truediv&quot;\\n  input: &quot;gradients/Square_grad/mul&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/sub_grad/Shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\240P\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/sub_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\240P\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/sub_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/sub_grad/Shape&quot;\\n  input: &quot;gradients/sub_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/sub_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Square_grad/mul_1&quot;\\n  input: &quot;gradients/sub_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/sub_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/sub_grad/Sum&quot;\\n  input: &quot;gradients/sub_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/sub_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Square_grad/mul_1&quot;\\n  input: &quot;gradients/sub_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/sub_grad/Neg&quot;\\n  op: &quot;Neg&quot;\\n  input: &quot;gradients/sub_grad/Sum_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/sub_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/sub_grad/Neg&quot;\\n  input: &quot;gradients/sub_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/sub_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/sub_grad/Reshape&quot;\\n  input: &quot;^gradients/sub_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/sub_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/sub_grad/Reshape&quot;\\n  input: &quot;^gradients/sub_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/sub_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/sub_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/sub_grad/Reshape_1&quot;\\n  input: &quot;^gradients/sub_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/sub_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/predictions_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/sub_grad/tuple/control_dependency&quot;\\n  input: &quot;theta/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/predictions_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;X&quot;\\n  input: &quot;gradients/sub_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/predictions_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/predictions_grad/MatMul&quot;\\n  input: &quot;^gradients/predictions_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/predictions_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/predictions_grad/MatMul&quot;\\n  input: &quot;^gradients/predictions_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/predictions_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/predictions_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/predictions_grad/MatMul_1&quot;\\n  input: &quot;^gradients/predictions_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/predictions_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/learning_rate&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.009999999776482582\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_theta/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;theta&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;gradients/predictions_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@theta&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^GradientDescent/update_theta/ApplyGradientDescent&quot;\\n}\\nnode {\\n  name: &quot;init&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^theta/Assign&quot;\\n}\\nnode {\\n  name: &quot;save/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n        }\\n        string_val: &quot;model&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/SaveV2/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;theta&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/SaveV2/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/SaveV2&quot;\\n  op: &quot;SaveV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/SaveV2/tensor_names&quot;\\n  input: &quot;save/SaveV2/shape_and_slices&quot;\\n  input: &quot;theta&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;^save/SaveV2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@save/Const&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;theta&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2/tensor_names&quot;\\n  input: &quot;save/RestoreV2/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;theta&quot;\\n  input: &quot;save/RestoreV2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@theta&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/restore_all&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^save/Assign&quot;\\n}\\n';\n",
       "          }\n",
       "        </script>\n",
       "        <link rel=&quot;import&quot; href=&quot;https://tensorboard.appspot.com/tf-graph-basic.build.html&quot; onload=load()>\n",
       "        <div style=&quot;height:600px&quot;>\n",
       "          <tf-graph-basic id=&quot;graph0.3745401188473625&quot;></tf-graph-basic>\n",
       "        </div>\n",
       "    \"></iframe>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_graph(tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n + 1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mse_summary = tf.summary.scalar('MSE', mse)\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "batch_size = 100\n",
    "n_batches = int(np.ceil(m / batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:                                                        # not shown in the book\n",
    "    sess.run(init)                                                                # not shown\n",
    "\n",
    "    for epoch in range(n_epochs):                                                 # not shown\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "            if batch_index % 10 == 0:\n",
    "                summary_str = mse_summary.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                step = epoch * n_batches + batch_index\n",
    "                file_writer.add_summary(summary_str, step)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "\n",
    "    best_theta = theta.eval()                                                     # not shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.07033372],\n",
       "       [ 0.86371452],\n",
       "       [ 0.12255151],\n",
       "       [-0.31211874],\n",
       "       [ 0.38510373],\n",
       "       [ 0.00434168],\n",
       "       [-0.01232954],\n",
       "       [-0.83376896],\n",
       "       [-0.80304712]], dtype=float32)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name scopes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n + 1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\") as scope:\n",
    "    error = y_pred - y\n",
    "    mse = tf.reduce_mean(tf.square(error), name=\"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "mse_summary = tf.summary.scalar('MSE', mse)\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best theta:\n",
      "[[ 2.07033372]\n",
      " [ 0.86371452]\n",
      " [ 0.12255151]\n",
      " [-0.31211874]\n",
      " [ 0.38510373]\n",
      " [ 0.00434168]\n",
      " [-0.01232954]\n",
      " [-0.83376896]\n",
      " [-0.80304712]]\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "batch_size = 100\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "            if batch_index % 10 == 0:\n",
    "                summary_str = mse_summary.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                step = epoch * n_batches + batch_index\n",
    "                file_writer.add_summary(summary_str, step)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "\n",
    "    best_theta = theta.eval()\n",
    "\n",
    "file_writer.flush()\n",
    "file_writer.close()\n",
    "print(\"Best theta:\")\n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss/sub\n"
     ]
    }
   ],
   "source": [
    "print(error.op.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss/mse\n"
     ]
    }
   ],
   "source": [
    "print(mse.op.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "a_1\n",
      "param/a\n",
      "param_1/a\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "a1 = tf.Variable(0, name=\"a\")      # name == \"a\"\n",
    "a2 = tf.Variable(0, name=\"a\")      # name == \"a_1\"\n",
    "\n",
    "with tf.name_scope(\"param\"):       # name == \"param\"\n",
    "    a3 = tf.Variable(0, name=\"a\")  # name == \"param/a\"\n",
    "\n",
    "with tf.name_scope(\"param\"):       # name == \"param_1\"\n",
    "    a4 = tf.Variable(0, name=\"a\")  # name == \"param_1/a\"\n",
    "\n",
    "for node in (a1, a2, a3, a4):\n",
    "    print(node.op.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modularity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An ugly flat code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_features = 3\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "\n",
    "w1 = tf.Variable(tf.random_normal((n_features, 1)), name=\"weights1\")\n",
    "w2 = tf.Variable(tf.random_normal((n_features, 1)), name=\"weights2\")\n",
    "b1 = tf.Variable(0.0, name=\"bias1\")\n",
    "b2 = tf.Variable(0.0, name=\"bias2\")\n",
    "\n",
    "z1 = tf.add(tf.matmul(X, w1), b1, name=\"z1\")\n",
    "z2 = tf.add(tf.matmul(X, w2), b2, name=\"z2\")\n",
    "\n",
    "relu1 = tf.maximum(z1, 0., name=\"relu1\")\n",
    "relu2 = tf.maximum(z2, 0., name=\"relu2\")\n",
    "# relu2 = tf.maximum(z1, 0., name=\"relu2\")  # Oops, cut&paste error! Did you spot it?\n",
    "\n",
    "output = tf.add(relu1, relu2, name=\"output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better, using a function to build the ReLUs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "def relu(X):\n",
    "    w_shape = (int(X.get_shape()[1]), 1)\n",
    "    w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")\n",
    "    b = tf.Variable(0.0, name=\"bias\")\n",
    "    z = tf.add(tf.matmul(X, w), b, name=\"z\")\n",
    "    return tf.maximum(z, 0., name=\"relu\")\n",
    "\n",
    "n_features = 3\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "relus = [relu(X) for i in range(5)]\n",
    "output = tf.add_n(relus, name=\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_writer = tf.summary.FileWriter(\"logs/relu1\", tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even better using name scopes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "def relu(X):\n",
    "    with tf.name_scope(\"relu\"):\n",
    "        w_shape = (int(X.get_shape()[1]), 1)                          # not shown in the book\n",
    "        w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")    # not shown\n",
    "        b = tf.Variable(0.0, name=\"bias\")                             # not shown\n",
    "        z = tf.add(tf.matmul(X, w), b, name=\"z\")                      # not shown\n",
    "        return tf.maximum(z, 0., name=\"max\")                          # not shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_features = 3\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "relus = [relu(X) for i in range(5)]\n",
    "output = tf.add_n(relus, name=\"output\")\n",
    "\n",
    "file_writer = tf.summary.FileWriter(\"logs/relu2\", tf.get_default_graph())\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sharing Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sharing a `threshold` variable the classic way, by defining it outside of the `relu()` function then passing it as a parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "def relu(X, threshold):\n",
    "    with tf.name_scope(\"relu\"):\n",
    "        w_shape = (int(X.get_shape()[1]), 1)                        # not shown in the book\n",
    "        w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")  # not shown\n",
    "        b = tf.Variable(0.0, name=\"bias\")                           # not shown\n",
    "        z = tf.add(tf.matmul(X, w), b, name=\"z\")                    # not shown\n",
    "        return tf.maximum(z, threshold, name=\"max\")\n",
    "\n",
    "threshold = tf.Variable(0.0, name=\"threshold\")\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "relus = [relu(X, threshold) for i in range(5)]\n",
    "output = tf.add_n(relus, name=\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "def relu(X):\n",
    "    with tf.name_scope(\"relu\"):\n",
    "        print(relu)\n",
    "        if not hasattr(relu, \"threshold\"):\n",
    "            relu.threshold = tf.Variable(0.0, name=\"threshold\")\n",
    "        w_shape = int(X.get_shape()[1]), 1                          # not shown in the book\n",
    "        w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")  # not shown\n",
    "        b = tf.Variable(0.0, name=\"bias\")                           # not shown\n",
    "        z = tf.add(tf.matmul(X, w), b, name=\"z\")                    # not shown\n",
    "        return tf.maximum(z, relu.threshold, name=\"max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function relu at 0x1262eee18>\n",
      "<function relu at 0x1262eee18>\n",
      "<function relu at 0x1262eee18>\n",
      "<function relu at 0x1262eee18>\n",
      "<function relu at 0x1262eee18>\n"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "relus = [relu(X) for i in range(5)]\n",
    "output = tf.add_n(relus, name=\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "with tf.variable_scope(\"relu\"):\n",
    "    threshold = tf.get_variable(\"threshold\", shape=(),\n",
    "                                initializer=tf.constant_initializer(0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"relu\", reuse=True):\n",
    "    threshold = tf.get_variable(\"threshold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"relu\") as scope:\n",
    "    scope.reuse_variables()\n",
    "    threshold = tf.get_variable(\"threshold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "def relu(X):\n",
    "    with tf.variable_scope(\"relu\", reuse=True):\n",
    "        threshold = tf.get_variable(\"threshold\")\n",
    "        w_shape = int(X.get_shape()[1]), 1                          # not shown\n",
    "        w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")  # not shown\n",
    "        b = tf.Variable(0.0, name=\"bias\")                           # not shown\n",
    "        z = tf.add(tf.matmul(X, w), b, name=\"z\")                    # not shown\n",
    "        return tf.maximum(z, threshold, name=\"max\")\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "with tf.variable_scope(\"relu\"):\n",
    "    threshold = tf.get_variable(\"threshold\", shape=(),\n",
    "                                initializer=tf.constant_initializer(0.0))\n",
    "relus = [relu(X) for relu_index in range(5)]\n",
    "output = tf.add_n(relus, name=\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_writer = tf.summary.FileWriter(\"logs/relu6\", tf.get_default_graph())\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "def relu(X):\n",
    "    with tf.variable_scope(\"relu\"):\n",
    "        threshold = tf.get_variable(\"threshold\", shape=(), initializer=tf.constant_initializer(0.0))\n",
    "        w_shape = (int(X.get_shape()[1]), 1)\n",
    "        w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")\n",
    "        b = tf.Variable(0.0, name=\"bias\")\n",
    "        z = tf.add(tf.matmul(X, w), b, name=\"z\")\n",
    "        return tf.maximum(z, threshold, name=\"max\")\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "with tf.variable_scope(\"\", default_name=\"\") as scope:\n",
    "    first_relu = relu(X)     # create the shared variable\n",
    "    scope.reuse_variables()  # then reuse it!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    relus = [first_relu] + [relu(X) for i in range(4)]\n",
    "output = tf.add_n(relus, name=\"output\")\n",
    "\n",
    "file_writer = tf.summary.FileWriter(\"logs/relu8\", tf.get_default_graph())\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "def relu(X):\n",
    "    threshold = tf.get_variable(\"threshold\", shape=(),\n",
    "                                initializer=tf.constant_initializer(0.0))\n",
    "    w_shape = (int(X.get_shape()[1]), 1)                        # not shown in the book\n",
    "    w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")  # not shown\n",
    "    b = tf.Variable(0.0, name=\"bias\")                           # not shown\n",
    "    z = tf.add(tf.matmul(X, w), b, name=\"z\")                    # not shown\n",
    "    return tf.maximum(z, threshold, name=\"max\")\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "relus = []\n",
    "for relu_index in range(5):\n",
    "    with tf.variable_scope(\"relu\", reuse=(relu_index >= 1)) as scope:\n",
    "        relus.append(relu(X))\n",
    "output = tf.add_n(relus, name=\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_writer = tf.summary.FileWriter(\"logs/relu9\", tf.get_default_graph())\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra material"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x0: my_scope/x\n",
      "x1: my_scope/x_1\n",
      "x2: my_scope/x_2\n",
      "x3: my_scope/x\n",
      "x4: my_scope_1/x\n",
      "x5: my_scope/x\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "with tf.variable_scope(\"my_scope\"):\n",
    "    x0 = tf.get_variable(\"x\", shape=(), initializer=tf.constant_initializer(0.))\n",
    "    x1 = tf.Variable(0., name=\"x\")\n",
    "    x2 = tf.Variable(0., name=\"x\")\n",
    "\n",
    "with tf.variable_scope(\"my_scope\", reuse=True):\n",
    "    x3 = tf.get_variable(\"x\")\n",
    "    x4 = tf.Variable(0., name=\"x\")\n",
    "\n",
    "with tf.variable_scope(\"\", default_name=\"\", reuse=True):\n",
    "    x5 = tf.get_variable(\"my_scope/x\")\n",
    "\n",
    "print(\"x0:\", x0.op.name)\n",
    "print(\"x1:\", x1.op.name)\n",
    "print(\"x2:\", x2.op.name)\n",
    "print(\"x3:\", x3.op.name)\n",
    "print(\"x4:\", x4.op.name)\n",
    "print(\"x5:\", x5.op.name)\n",
    "print(x0 is x3 and x3 is x5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first `variable_scope()` block first creates the shared variable `x0`, named `my_scope/x`. For all operations other than shared variables (including non-shared variables), the variable scope acts like a regular name scope, which is why the two variables `x1` and `x2` have a name with a prefix `my_scope/`. Note however that TensorFlow makes their names unique by adding an index: `my_scope/x_1` and `my_scope/x_2`.\n",
    "\n",
    "The second `variable_scope()` block reuses the shared variables in scope `my_scope`, which is why `x0 is x3`. Once again, for all operations other than shared variables it acts as a named scope, and since it's a separate block from the first one, the name of the scope is made unique by TensorFlow (`my_scope_1`) and thus the variable `x4` is named `my_scope_1/x`.\n",
    "\n",
    "The third block shows another way to get a handle on the shared variable `my_scope/x` by creating a `variable_scope()` at the root scope (whose name is an empty string), then calling `get_variable()` with the full name of the shared variable (i.e. `\"my_scope/x\"`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'Do' b'you' b'want' b'some' b'caf\\xc3\\xa9?']\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "text = np.array(\"Do you want some cafÃ©?\".split())\n",
    "text_tensor = tf.constant(text)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(text_tensor.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a Home-Made Computation Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(x,y) = ((x) * (x)) * (y) + y + 2\n",
      "f(3,4) = 42\n"
     ]
    }
   ],
   "source": [
    "class Const(object):\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "    def evaluate(self):\n",
    "        return self.value\n",
    "    def __str__(self):\n",
    "        return str(self.value)\n",
    "\n",
    "class Var(object):\n",
    "    def __init__(self, init_value, name):\n",
    "        self.value = init_value\n",
    "        self.name = name\n",
    "    def evaluate(self):\n",
    "        return self.value\n",
    "    def __str__(self):\n",
    "        return self.name\n",
    "\n",
    "class BinaryOperator(object):\n",
    "    def __init__(self, a, b):\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "\n",
    "class Add(BinaryOperator):\n",
    "    def evaluate(self):\n",
    "        return self.a.evaluate() + self.b.evaluate()\n",
    "    def __str__(self):\n",
    "        return \"{} + {}\".format(self.a, self.b)\n",
    "\n",
    "class Mul(BinaryOperator):\n",
    "    def evaluate(self):\n",
    "        return self.a.evaluate() * self.b.evaluate()\n",
    "    def __str__(self):\n",
    "        return \"({}) * ({})\".format(self.a, self.b)\n",
    "\n",
    "x = Var(3, name=\"x\")\n",
    "y = Var(4, name=\"y\")\n",
    "f = Add(Mul(Mul(x, x), y), Add(y, Const(2))) # f(x,y) = xÂ²y + y + 2\n",
    "print(\"f(x,y) =\", f)\n",
    "print(\"f(3,4) =\", f.evaluate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing gradients\n",
    "### Mathematical differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df/dx(3,4) = 24\n",
      "df/dy(3,4) = 10\n"
     ]
    }
   ],
   "source": [
    "df_dx = Mul(Const(2), Mul(x, y))  # df/dx = 2xy\n",
    "df_dy = Add(Mul(x, x), Const(1))  # df/dy = xÂ² + 1\n",
    "print(\"df/dx(3,4) =\", df_dx.evaluate())\n",
    "print(\"df/dy(3,4) =\", df_dy.evaluate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df/dx(3,4) = 24.000400000048216\n",
      "df/dy(3,4) = 10.000000000047748\n"
     ]
    }
   ],
   "source": [
    "def gradients(func, vars_list, eps=0.0001):\n",
    "    partial_derivatives = []\n",
    "    base_func_eval = func.evaluate()\n",
    "    for var in vars_list:\n",
    "        original_value = var.value\n",
    "        var.value = var.value + eps\n",
    "        tweaked_func_eval = func.evaluate()\n",
    "        var.value = original_value\n",
    "        derivative = (tweaked_func_eval - base_func_eval) / eps\n",
    "        partial_derivatives.append(derivative)\n",
    "    return partial_derivatives\n",
    "\n",
    "df_dx, df_dy = gradients(f, [x, y])\n",
    "print(\"df/dx(3,4) =\", df_dx)\n",
    "print(\"df/dy(3,4) =\", df_dy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Symbolic differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'derive', 'evaluate']\n",
      "<function <lambda> at 0x1271c3d90>\n",
      "True\n",
      "df/dx(3,4) = 24.0\n",
      "df/dy(3,4) = 10.0\n"
     ]
    }
   ],
   "source": [
    "Const.derive = lambda self, var: Const(0)\n",
    "Var.derive = lambda self, var: Const(1) if self is var else Const(0) # if var is the variable derived against\n",
    "                                                                     # return 1, else 0\n",
    "Add.derive = lambda self, var: Add(self.a.derive(var), self.b.derive(var))\n",
    "                                                                    # deriving a + b is a' + b'.\n",
    "Mul.derive = lambda self, var: Add(Mul(self.a, self.b.derive(var)), Mul(self.a.derive(var), self.b))\n",
    "                                                                    # deriving a * b is a'b + ab'\n",
    "\n",
    "print(dir(Const))\n",
    "print(str(Const.derive))\n",
    "print(callable(Const.derive))\n",
    "\n",
    "x = Var(3.0, name=\"x\")\n",
    "y = Var(4.0, name=\"y\")\n",
    "f = Add(Mul(Mul(x, x), y), Add(y, Const(2))) # f(x,y) = xÂ²y + y + 2\n",
    "\n",
    "df_dx = f.derive(x)  # 2xy\n",
    "df_dy = f.derive(y)  # xÂ² + 1\n",
    "print(\"df/dx(3,4) =\", df_dx.evaluate())\n",
    "print(\"df/dy(3,4) =\", df_dy.evaluate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'function'>\n",
      "<class 'method-wrapper'>\n",
      "<class 'type'>\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "def my_func():\n",
    "    return 1\n",
    "\n",
    "print(type(my_func))\n",
    "print(type(my_func.__call__))\n",
    "print(type(type(my_func.__call__)))\n",
    "print(isinstance(my_func, object))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic differentiation (autodiff) â€“ forward mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DualNumber(object):\n",
    "    # the dual number was initially setted. as x.value = DualNumber(x.value, 1), then it can be used to compute\n",
    "    # without anyohter need of changing(since this is the only variable)\n",
    "    def __init__(self, value=0.0, eps=0.0):\n",
    "        self.value = value\n",
    "        self.eps = eps\n",
    "    def __add__(self, b):\n",
    "        return DualNumber(self.value + self.to_dual(b).value,\n",
    "                          self.eps + self.to_dual(b).eps)\n",
    "                        # All the operations are made up of dual numbers, thus updating the current dual number!\n",
    "    def __radd__(self, a):\n",
    "        return self.to_dual(a).__add__(self)\n",
    "    def __mul__(self, b):\n",
    "        return DualNumber(self.value * self.to_dual(b).value,\n",
    "                          self.eps * self.to_dual(b).value + self.value * self.to_dual(b).eps)\n",
    "    def __rmul__(self, a):\n",
    "        return self.to_dual(a).__mul__(self)\n",
    "    def __str__(self):\n",
    "        if self.eps:\n",
    "            return \"{:.1f} + {:.1f}Îµ\".format(self.value, self.eps)\n",
    "        else:\n",
    "            return \"{:.1f}\".format(self.value)\n",
    "    def __repr__(self):\n",
    "        return str(self)\n",
    "    @classmethod\n",
    "    def to_dual(cls, n):\n",
    "        if hasattr(n, \"value\"):\n",
    "            return n\n",
    "        else:\n",
    "            return cls(n)   # class constructor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$3 + (3 + 4 \\epsilon) = 6 + 4\\epsilon$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.0 + 4.0Îµ"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3 + DualNumber(3, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$(3 + 4Îµ)\\times(5 + 7Îµ) = 3 \\times 5 + 3 \\times 7Îµ + 4Îµ \\times 5 + 4Îµ \\times 7Îµ = 15 + 21Îµ + 20Îµ + 28Îµ^2 = 15 + 41Îµ + 28 \\times 0 = 15 + 41Îµ$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.0 + 41.0Îµ"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DualNumber(3, 4) * DualNumber(5, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42.0"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.value = DualNumber(3.0)\n",
    "y.value = DualNumber(4.0)\n",
    "\n",
    "f.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x.value = DualNumber(3.0, 1.0)  # 3 + Îµ\n",
    "y.value = DualNumber(4.0)       # 4\n",
    "\n",
    "df_dx = f.evaluate().eps\n",
    "\n",
    "x.value = DualNumber(3.0)       # 3\n",
    "y.value = DualNumber(4.0, 1.0)  # 4 + Îµ\n",
    "\n",
    "df_dy = f.evaluate().eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.0"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autodiff â€“ Reverse mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(x,y) = ((x) * (x)) * (y) + y + 2\n",
      "f(3,4) = 42\n",
      "df_dx = 24.0\n",
      "df_dy = 10.0\n"
     ]
    }
   ],
   "source": [
    "class Const(object):\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "    def evaluate(self):\n",
    "        return self.value\n",
    "    def backpropagate(self, gradient):\n",
    "        pass\n",
    "    def __str__(self):\n",
    "        return str(self.value)\n",
    "\n",
    "class Var(object):\n",
    "    def __init__(self, init_value, name):\n",
    "        self.value = init_value\n",
    "        self.name = name\n",
    "        self.gradient = 0\n",
    "    def evaluate(self):\n",
    "        return self.value\n",
    "    def backpropagate(self, gradient):\n",
    "        self.gradient += gradient\n",
    "    def __str__(self):\n",
    "        return self.name\n",
    "\n",
    "class BinaryOperator(object):\n",
    "    def __init__(self, a, b):\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "\n",
    "class Add(BinaryOperator):\n",
    "    def evaluate(self):\n",
    "        self.value = self.a.evaluate() + self.b.evaluate()\n",
    "        return self.value\n",
    "    def backpropagate(self, gradient):\n",
    "        self.a.backpropagate(gradient)\n",
    "        self.b.backpropagate(gradient)\n",
    "    def __str__(self):\n",
    "        return \"{} + {}\".format(self.a, self.b)\n",
    "\n",
    "class Mul(BinaryOperator):\n",
    "    def evaluate(self):\n",
    "        self.value = self.a.evaluate() * self.b.evaluate()\n",
    "        return self.value\n",
    "    def backpropagate(self, gradient):\n",
    "        self.a.backpropagate(gradient * self.b.value)\n",
    "        self.b.backpropagate(gradient * self.a.value)\n",
    "    def __str__(self):\n",
    "        return \"({}) * ({})\".format(self.a, self.b)\n",
    "\n",
    "x = Var(3, name=\"x\")\n",
    "y = Var(4, name=\"y\")\n",
    "f = Add(Mul(Mul(x, x), y), Add(y, Const(2))) # f(x,y) = xÂ²y + y + 2\n",
    "\n",
    "result = f.evaluate()\n",
    "f.backpropagate(1.0)\n",
    "\n",
    "print(\"f(x,y) =\", f)\n",
    "print(\"f(3,4) =\", result)\n",
    "print(\"df_dx =\", x.gradient)\n",
    "print(\"df_dy =\", y.gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autodiff â€“ reverse mode (using TensorFlow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42.0, [24.0, 10.0])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "x = tf.Variable(3., name=\"x\")\n",
    "y = tf.Variable(4., name=\"y\")\n",
    "f = x*x*y + y + 2\n",
    "\n",
    "gradients = tf.gradients(f, [x, y])\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    f_val, gradients_val = sess.run([f, gradients])\n",
    "\n",
    "f_val, gradients_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. to 11."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "See appendix A."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Logistic Regression with Mini-Batch Gradient Descent using TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's create the moons dataset using Scikit-Learn's `make_moons()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "\n",
    "m = 1000\n",
    "X_moons, y_moons = make_moons(m, noise=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a peek at the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAD/CAYAAADi+OGRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJztvX14VeWZ7/+9d7I32SGQQqBqRxNqC9oGBGu8bEdFTjl9\nAX9tLVatBoy1HsYwndI505f4ox5EBtvjdKZ69VKQ1iiQ2KOdC2UcwLaH+oIWrVhIEaeio4afk9SG\naEGSwA7Z9++PlSd77bWfZ61nrb32a57Pda0L9trr5dkra637ud+JmWEwGAwGgxeRQg/AYDAYDKWB\nERgGg8Fg0MIIDIPBYDBoYQSGwWAwGLQwAsNgMBgMWhiBYTAYDAYtjMAwGAwGgxZGYBgMBoNBCyMw\nDAaDwaBFZaEHECbTpk3jGTNmFHoYBoPBUFK89NJLR5h5utd2ZSUwZsyYgb179xZ6GAaDwVBSEFG3\nznbGJGUwGAwGLUIVGET0DSLaS0QniehBl+1aiOglIjpGRG8T0Z1EVGn7/ikiOkFEx0eXV8Mcp8Fg\nMBj8E7aG0QPgHwG0e2xXDeBbAKYBuAjAQgDfdmzzDWauGV3OCXmcBoPBYPBJqD4MZt4KAETUBOBM\nl+3W2z7+FxF1AvhvYY7FYDCUH8PDw3j77bdx4sSJQg+lJKmqqsKZZ56JaDQaaP9icXrPB3DQse4H\nRPRDAK8CWMXMT8l2JKLlAJYDQH19fS7HaDAYCszbb7+NSZMmYcaMGSCiQg+npGBm9Pf34+2338aH\nP/zhQMcouNObiG4E0ATgR7bV3wNwNoC/ArARwONE9BHZ/sy8kZmbmLlp+nTPqDCDwT+9vcBllwF/\n+lOhRzLuOXHiBOrq6oywCAARoa6uLivtrKACg4iuAPADAIuY+YhYz8wvMPP7zHySmTcBeA7A4kKN\n0zDOWbsWePZZ619DwTHCIjjZXruCCQwi+jyAnwL4AjMf8NicAZi7xJB/enuBBx4AkknrX6NlGMYx\nYYfVVhJRFYAKABVEVGUPl7Vt92kAnQCuZObfOb77ABF9TuxLRM2wfBxPhDlWg0GLtWstYQEAIyNG\nyzCgoqIC8+bNw+zZs3HVVVdhcHDQ9zFuuukmvPLKKwCAO+64I+27v/7rvw5lnLkgbA3j+wCGALQB\nWDr6/+8TUf1oPoXwSt8KoBbADluuxc7R76KwQnP7ABwB8HcArmDmQyGP1WBwR2gXiYT1OZEwWkaJ\n0XmgEzPumoHImghm3DUDnQc6sz5mPB7H/v378fLLLyMWi2HDhg2+j/Gzn/0MH//4xwFkCozf/va3\nWY8xV4QqMJj5NmYmx3IbMx8ezac4PLrdf2PmSlueRQ0zLxr9ro+ZL2TmScz8AWb+JDP/OsxxGkqI\nQjqc7dqFwGgZJUPngU4sf3w5uo92g8HoPtqN5Y8vD0VoCC699FK8/vrrAIB/+Zd/wezZszF79mzc\nddddAICBgQFcfvnlmDt3LmbPno2HH34YALBgwQLs3bsXbW1tGBoawrx589Dc3AwAqKmpAQB89atf\nxfbt28fOdcMNN+Bf//VfMTIygu985zu48MILcd555+G+++4L7fd4UfAoKYPBlVw4nJ1CSCWU9uxJ\naReCRAIo4hmgIcWqXaswOJxuLhocHsSqXatCOf6pU6ewc+dOzJkzBy+99BIeeOABvPDCC3j++efx\n05/+FPv27cMTTzyBD33oQ+jq6sLLL7+Mz3/+82nH+OEPfzimsXR2pguya665Bo888ggAIJFIYNeu\nXbj88stx//33o7a2Fi+++CJefPFF/PSnP8Wbb74Zym/ywggMQ/GSK4ezUwg5PwsBsnMnwJy57NsX\nzjgMOeXw0cO+1usiNIKmpibU19fj61//Op599ll8+ctfxsSJE1FTU4MlS5Zg9+7dmDNnDn7961/j\ne9/7Hnbv3o3a2lrt8yxatAhPPvkkTp48iZ07d2L+/PmIx+P41a9+hc2bN2PevHm46KKL0N/fj9de\ney2r36SLERjjmWLPL8iFw9kphLq6MoWSCaMtC+pr5Ym8qvW6CI1g//79+MlPfoJYLKbcdtasWfj9\n73+POXPm4Pvf/z5uv/127fNUVVVhwYIF+OUvf4mHH34Y11xzDQArAe8nP/nJ2BjefPNNfPazn83q\nN+liBMZ4phhfjEKIiRe53eHc3p69cHMKoebm9M9tbSaMtkxYt3AdqqPVaeuqo9VYt3Bd6Oe69NJL\n8dhjj2FwcBADAwN49NFHcemll6KnpwfV1dVYunQpvvOd7+D3v/99xr7RaBTDw8PS415zzTV44IEH\nsHv37jFz1uc+9zmsX79+bJ9Dhw5hYGAg9N8khZnLZrngggvYoElPD3NVlWVkiceZe3uzP978+dkf\np7WVORJhbmxkjsXSjUGRCHNLS/Dz2H+zaqmoYI5Grf/HYswrVvg/RxjXwSDllVde8bV9xx86uOHH\nDUy3ETf8uIE7/tCR9RgmTpwoXf/P//zP3NjYyI2NjfzjH/+YmZmfeOIJnjNnDs+dO5ebmpr4xRdf\nZGbmyy67bOz/3/3ud/ncc8/l6667LuP4iUSCp0yZwjfccMPYupGREb7lllt49uzZ3NjYyAsWLOC/\n/OUv2uOXXUMAe1njHVvwl3yYixEYPmhtTb2Qg7wYZceLRLI7jv2FTiR/odfWWv+2tPg//vXXq4+r\nWvwK0zCug0GJX4FhyCQbgWFMUuORsPMLwnJO281F0SiwYgXQ2goIG3E0Chw7Zv2/o8P7PE4fzfbt\nlhjwgx/fickKN5Q5RmCMR8LOL/ByTquc6/b1MiHW3m4tYt3wcOqFPzICzJvn/lK2+2h6ewFh543H\nrc89PUBVVWpdY2PmMfyE0ZqscEOZYwTGeCTM/AIdbUXlXLevlwmxRMISEireeQe45Rb3cYnZ/i23\nZL7MnS/4yy4LHkZrssIN4wEdu1WpLMaHYSNfzle7L0Qsdp9ITw/zhAnW+qqq1HicTvfGRn++BbuT\nWvYbnT6aior0/aqqMh3g2Tj/va6DIRSMDyN7jA9jPKGbO5GvkFkvbWXt2pSWkEikJ8uJ2f2JE0BT\nk/Wa7ekB5s+3fqf99VtXJz+/CIX91KespasL+OQnM2f7IyOZY3SOOxszkskKN4wHdKRKqSzjQsPQ\nicIJO2RWZ0yyiCu7dmFf5szJXC80Bdnv6+mxfoebliH+LzSVSCSYxjJvnt5vNuGzBcFoGNljNIzx\ngm4UTj6dr262e7t2YefAgcz1IyPAypXy37d2LTA0pB6DXXs4ONrp1+kPASwnuRANPT3AGWcARNa/\nQqPRLftRjEmPhrxARPiHf/iHsc8/+tGPcNttt4V+nmIse24ERimhIwjy7XxVRVx94hPAb34jf3ED\n8vXbtmX+PvF7grBiRbqAmDw5dR3a2lJCordX7TyXESR8ttjLsJQzIV/7CRMmYOvWrThy5Ij3xllQ\njGXPjcAoFXQFwdq1mfb6XGoZKtt9by9QWQlEFLdYJAJUVKSvO3ky8/fZo5ucEFmLivvvT9dS7CG2\njsqg2LzZ8oHovFT8aHDiZXXLLUYjKRQha4OVlZVYvnw5fvzjH2d819fXhyuvvBIXXnghLrzwQjz3\n3HNj6z/zmc+gsbERN910ExoaGsYEzhVXXIELLrgAjY2N2LhxIwAUb9lzHbuV7gLgGwD2AjgJ4EGP\nbf8ewJ8AHAPQDmCC7bupAB4FMACgG8B1Oucvax+GbhTOvHnZ2eaDImz6+/d7Z2vrLtFoZnST3+WM\nM9LHFI8zX3WVentVBrns94klEmHu6pLv19pqXQfxO/LhUypjfPswcuDPmzhxIh89epQbGhr4L3/5\nC//TP/0Tr169mpmZr732Wt69ezczM3d3d/O5557LzMx/+7d/y3fccQczM+/cuZMBcF9fHzMz9/f3\nMzPz4OAgNzY28pEjR8bO4zwvM/PWrVv5+uuvZ2bmkydP8plnnsmDg4N833338dq1a5mZ+cSJE3zB\nBRfwG2+8kTH+oikNAmAJgCsArHcTGAA+B+AdAI0ApgB4CsAPbd//HMDDAGoAXALgKIBGr/OXtcDQ\nFQS5cHjrOHivv94656xZKcEWiQR3Poe52OtSOYWuzIGuCtNV1bgCmM85R37dnMIlFsuuHtY4x7fA\nCLsEDqde3LfeeivffvvtaQJj+vTpPHfu3LHlQx/6EL///vs8d+7ctJf3lClTxgTG6tWr+bzzzuPz\nzjuPJ0+ezHv27Ek7j/O8Q0NDfNZZZ/GJEyf4scceG6tBdeWVV/LMmTPHzj1jxgz+5S9/mTH+ohEY\nYwe1Wqy6CYyHANxh+/xpAH8a/f9EAAkAs2zfb7YLFNVS1gJDFz8PiG6kjz1ySbZPT09wTUAm8MTx\nVUIy14tTy9CpcQVkXsfW1lQhQ6dQIjI5GgHwJTBkAjuESZR4cff393NDQwPfdtttYwKjrq6Oh4aG\nMvZRCYwnn3ySL774Yh4YGGBmqyjhk08+mXYe53mZmZctW8bbtm3ja6+9lrdt28bMzEuWLOEnnnjC\nc/ylGCXVCKDL9rkLwGlEVAdgFoBTnN7Du2t0H4Mbfh3eKtuurGSHPWPauc83v5npN3FSVQXMmZPy\nOYjyHM6oJPuY9u2zakkRZfo7YrGU07qnx/3cftmyJTNT3Vnjihm4/vr0/draUv8X100WJTYyYu1v\nMsFzS45b7E6dOhVXX3017r///rF1n/3sZ/GTn/xk7PP+/fsBABdffPFY97xf/epXeO+99wAAR48e\nxZQpU1BdXY0//vGPeP7558f2Lcqy5zpSxe8Cbw3jPwF83vY5CoABzABwKUa1Ddv3/wPAU4pjLYfl\nN9lbX1/vKV3LGj/Zxk7T1f79qZm9XaOwV3i1+xTETK2nR2/W7jRNycYlG5NbOXKxv+x3ey2y/BCn\nltHTw/zJT2ZuK8bm1Krs5iydMZlMcN/40jBy5M+zz/T/9Kc/cTweH9Mw+vr6+Oqrr+Y5c+bwxz72\nMf6bv/kbZmZ+5513+NOf/jQ3NjbyTTfdxKeffjqfOHGCT5w4wZ///Of53HPP5S996UtpGkauyp6X\nokmqC8DVts/TRgVGHYDzAQw6tv82gMe9zjvuTVJ+HhCn6aqxMdVvQrwg3V6q4mX3la/Iv7/6am8z\nldM8IBuT20t3xgxLyOmUFbEnA/b0eDvkp061xqMSduecoxY0bn8Lr2tgcKVUE/dOnDjBw8PDzMz8\n29/+lufOnVuwsZSiwHgIwDrb54XI9GHMtH2/BcaHER5ujYSEfV33Zad6oevM+J1Z4V7NjWT7y7Le\nVQKksdH6Xmf2P2WK+3hUwnDatMzrLQICvK6BwZNSFRiHDh3iefPm8XnnncdNTU38u9/9rmBjKRof\nBhFVElEVgAoAFURURUSVkk03A/g6EX2ciKYAuBXAgwDAzAMAtgK4nYgmEtElAL44KjTGL7rJR729\nVi0lkVMg209m2xUI+7oOp05l5mDYj+OFs+aUakxu+yeTma1b589P9dAQxGLWdQDkuSNOjh1L/w2N\njemv+g98QL7faaelf5blfDh/Q9CELJMMWDLMnDkT+/btQ1dXF1588UVceOGFhR5SMHSkiu4C4DYA\n7FhuA1AP4DiAetu2/xNWaO0xAA8gMw/jMVh5GIdh8jD0O7kJMwpgbSv2s4dy5jL6KBq1zuWlLTh/\nSzZj0j2W3TQnOvf5WUSuhSx0WfX3UWkXp50WXjvbcaShvPLKK5xMJgs9jJIlmUwWn0mqUEvZCgzd\n3ApZsT9hehGmJmcOQBCHsVhUL926Or1jChOR7HcIk49wJHv5Q+yl03WuZ5DfO2tW5jWLxaxEQOHn\ncP596urUxwurne048oO88cYb3NfXZ4RGAJLJJPf19UmT+XQFhsxcZCg2ZKUo7rlHvp0zDE+YXoR5\npaPDOpY4ho55RsXRo1bIK3NqXTwOnH460N+fuf3EicDrrwO33w7cd1/KROSkrS01XlGUMBp1N3OJ\n0umy6+IkaFjloUNW+XRn6PIvfpHaxvn3Oess+bUArOMsX26FJT/8sHXddNG9J8qMM888E2+//Tb6\n+voKPZSSpKqqCmeeeWbwA+hIlVJZylLD0E0+UpUS9zMjD8NUFY3KE/zspjG3mbEsCZBIngDnXGTZ\n1k727dPLPo9E5M5/eya7275285W4Dvv2ZV6r6mr9JD638iTjSMswhA+MSapM0M2tEC9k3Re706+h\nOmc06r8mVDxuHdueHS5ecBUV7pnobhFFXovTxCXLSg/a2U8suhnt1dWZvo1Zs9z/Hqp6VM6/sSzc\n2ERbGbLACIxyQTe3Ioh2ICtRESS81bnYW6IK4aGaleva/CdMsEJd3c5LlJnX4cy/yOZ3Ocfrlc9x\n1VWpa5mNT8f5d1GdM9cFJg1lixEY45UgM+iqKiujWcyIgzrBZYtXxVnnzNht/LLj2B3s4ljODG1n\nJBOQEiR+Ba19vLrZ3H6Ob9cy7BpSDoroGQwCIzDGK0G0A/ESPeMM96Q3t5e5eIkFMSnZZ8bZCiyh\n0dh/l6gQ6/TxqCKrvH6DGG+25i3VdbZfi0gkPQrL/juNz8IQEroCwzRQKjeGhlKvldZWa51bkyEg\nFW3T2wtceGH6K0wU2LvwQnlCnCCRAJ5+2j1JzY49Ec5egDCbqC3AihjaMprjKX5XImFFh8kiyJwR\nU/v3W82UVMyblxrv/PlWI6jG0bqYXtdZh1deySz6+Itf5LSInsGgixEY5Upvr5UBDVgvZV3slVrt\nWcpbtgDPPCN/mYte2fPnq0Nfp0xJvVDtWddO9u1LCTuVcHJDZH87GRnJXJ9MWkLOztKl6mMLYdHb\na2XSt7dbxxB9xP1cZxXRqCUIvDLfs8kQNxgCYgRGuSLLydBB5D0AmfkQTu1DLDt2pF6gdkQJ854e\nq9SGeKHq9Bn3q2lUVVl9xCdM0N/HKbh6e60Zvoy6upRmsXYt8Pzzwa6vF0JTs+d6COyl1Z2amcGQ\nD3TsVqWylL0PQ7fhkU6kk1v3OMC7fLcdt6quKr+GruPWXupExw/jHIPIC9GJNnP6TyKRzJwRr3wX\nP5neqn3d/DjO46saWpmOfgYfwDi9SxCvB91PPSkdx7FbSOjZZ8vX2+rvj43ZTTjNm6d+aXqFgYYR\n4qtzHrdz2R3n9tpcqnM5BaGfKCyxr9s+0WiqT/lFFzF/8IOZodHjsMaUITuMwChF3B50P7WDdF9S\nsZi/GTBgbe8cs1u4ZzZtMnXDVp09u72yyWXX9owzvLPJq6r0sukbG4PX67KP1+3v6OzHYdeAxmGN\nKUN2GIFRasgedLc4fFWWtux4Xi832UtQJzlMRxj46QLoRFfwOcfqlU3uxI/ZS6dzoFPwB9EyVH9T\nVU6LXQMy+RoGnxiBUWrIHnRZ/SX7S1FVg6inxzJVeL383Ewsui9aL2GQozaZrmNwm7XLrpVXBrXX\nohKizqxwIeCvusr9eE4NRezrtZ9MAzJahkEDIzBKCdlM3f7w22fMstl1V1emNqLzopswwdv/YB+j\n8yU2caL3fmFcGzdNSmf27ib8dGbkXj4BnYxs4fi/6ip3weSswyXOryPMdDUgg8FBQQQGrMZHj8Jq\nfNQNReMjABtgNVQSy0kA79u+fwrACdv3r+qcv2QFhmyWLHv43Wakdm3EPsusqkq9gFQvGa8XjLDx\n2zUalfbjdIqHcW38OHD9aDQ6JjUdn4CbJhiPM//613p/RyC9lEo8Lq9M63cxNaYMHhRKYPwcwMMA\nagBcAuAogEaN/R4E0G77/BSAm/yev2QFhs4sWbz4idxrM1VUpAuBSMTqM53NC8YeFut8iVVUpDuL\nVaG3TnRCP3PtwNUxqflx6juvhdgnSHc/sa9b6LNzqasz0VGGQORdYACYCCABYJZt3WYAP9TY730A\nl9nWjS+B4UT2IvMSFG7LhAmp4oJ+Y/SdjtZo1PslpqNleEWEzZ+fXuU2F6YVL20kqFM/n4tsrMZv\nYfBJIQTG+QAGHev+AcDjHvtdD+ANAGRb9xSAPgBHADwHYIHOGMpGYOSq5/YZZ6TMU15RVoIgxQS9\ntAyvl5uw2TsFZL5fhF4aiMr3ZB+jWw+MIIs9Qm7/flPN1hAKhRAYlwL4k2Pd/wDwlMd+uwDc5lh3\nEYBJACYAaBnVQD6i2H85gL0A9tbX1+fkYhaMXFRDFeYqtygrgVsYp9fidly3l5tbOHC+X4ReGojK\n9xRm/w3ZIkxPjY3uvhOjZRg0KRYN49tuGgaAegAjAM72OPYTAP7OawxFp2FkU6LB2c4z6GzUaxt7\nLwwnutFWbi9V2TVxe7l5mXiKyYGrEiiiRHmQ66f6fWLyMGuWPMRa5jsxWoZBE12BEWbxwUMAKolo\npm3dXAAHXfZZBuA5Zn7D49gMIITa0Xlm7Vrg2WeDlaF2q5oqqKuzymur0Cnel0hYhfRkY9yzR76P\nqDorigvKXn2qwniyKqyiVLco6W0ft/McxVRwT1ZZ117QUHX9RHVf1XXr7bWOIYoz/vrXqYq4hw5l\nVgQeGZGXbjfVbA1hoyNVdBcA/wdWpNREaERJAXgVwI2OdR8A8DkAVQAqATTDCtOd5XX+otIwsnFC\nurX+nDYttV2YJitVMyEn2drK3cw82WSFF4psSp+osAcE6JoFjQnKkAUoYB7GY6Mv+MMYzcOAZXo6\nDqDetu2nRreb5DjGdAAvwvJb/AXA8wA+o3P+gguMsFpq6lYr9SqE52fRCceUVWoN80WV66zwXBC2\nkHNOND7zGb2/XyxmBTUYoWEIQEEERqGXggsMtwQu3Rk8s3uUlP1lFLZT3OvlLxNQxa4B5JqwhZxd\nAHkVQ5Qt4/lvYQiMrsAwDZTCwt5Ss6Mj084saweqQtjGRetP53GEbfqCC9K/O+cceZe6lpbMV4us\no51X289nnsn0P4x3W7n4WzmXIL4Wpw9Hp0FTRYXlU6qosD57NaYyGLLACIywsDtzZU7IZBJ48EF/\nD7PoGS26rPX0WOt27kxvnyp49VW5o3v79sx1so52Xi9/e0/vWCw1rmJyRBczorXrpz4lvw+82rLK\nGBmx/gb2zoi6ExOnc91g8EJHDSmVpWAmKZXj056pLEw59hh94e9QdU0TxxShr/aidKqEuquvzk1M\nfi6cu+MNe5itzHQUVsKmn34jppSIgfVNUgV/yYe5FExgqByfsugW4cuwP6yyB9d+THuynXghTJki\nf1lMmJCbSKNSjGAqJpwBA14+rWyEh1dPDZElbkqJGEbRFRjGJBUGKvOO048h1re1pfwd7e2p/wv7\ns9OWbTd12f9V4dfUpEMQE5Yhxdq16WZKN59Wby8weTKwfz8wYYL/c7n9XURuUHNz+n0VJFfIMP7Q\nkSqlshQ0SspPlzV7Zq69jLno1+zVM0EcQzXjN4XoigP7bF7W1VClZYh7SZT+8KNdqMq4O7UKmRnL\nXpvKMK6AMUnliZ4ey78gXghhVDPV6Rvt9sIwheiKA68Xv8x/ELQDIJH6RS+KOVZXq0N1RSl149PI\nmo4/dHDDjxuYbiNu+HEDd/yhQ+u7QqIrMMjatjxoamrivXv35vekK1YA69db0UzJpBU9dO21wJtv\nAg8/DCxaZJkWghKNqsMrGxuBl19OX9fbC5x9NnDiRGpdPA688QZw+unBx2Hwh/3vQGS9lmXMm5ce\nZbZiBXD//ZZZKRYDZs4E/uM/gKlTgSNH3M959dXWPWdn/34r/Fon+kqM09wvgek80Inljy/H4PDg\n2LrqaDU2fmEjACi/a57TnPex2iGil5i5yWs748PIht5eywcBpB7IRMLKw9i927ILO+P0583zdw6V\nsLDXLLKHR7rVajLkD/vfIRpNhSA7F7uwcPquEgmrhlQyaQmLXbusfVpbrWM6+cUvUj4wcT8sXaoW\nFvbQaPsxzf0SmFW7VqUJBAAYHB7Eql2rXL8rFYzAyAanI1MgYuNlSVRCgFx/fWqdPfFKRleXtR/Z\n6i8mEqnj24scGud04ZG9+HUS6rzyML7yFevfPXvk9x0zsHKlFVTxzDPA8uWpooUyxH0RdLyGDA4f\nPaxc7/ZdqWAERlDsmd0qVDO13l5LCxEwu0c+XXedNVN0mjVGRoBvfhPYsCEVZbVzp/dMtgzoPNCJ\nGXfNQGRNBDPumoHOA53eO+WLoFqeTNjbee894De/sf6WU6bIt9m2LZXQ+fjj8m0qKlIVgPftM1pp\niNTX1kvXT41PRYTkr1vVPjIKfd8bgREU2UMWi6VrCqqZWlubv4zegweBV17JXJ9IAP/2bylBMk4e\ncmEn7j7aDQaj+2g3lj++HCu2r8CMu2aA1hAqb68EraHCCBM/Wp7dfGQ3X8pKtwDAZz/rPvM/edI7\n7Np5nxitNDTWLVyH6mh12rpYRQzHTh7DCGf+Xaqj1Vi3cJ3WsVX3fT7vbyMwgqKbeyF7iatmfioi\nkZQ5ylmS4+TJ9POPA1OCyha8Ye8GdB/tBoCxh7P7aDeWbl2KaXdOy9+D5ae+lKpnikrbGBmxTE3H\njmU3RrswCLMe1jineU4zNn5hIxpqG0AgNNQ2YFJsEoaTmSbECqrw5fAuBh+IERhB2bcvVdtJqPcy\nh7Zzptbb6/9hTybTnert7SmHppMCaBn5VpNVNl+GOuKvf6g/77MxT+xmTaeg37dPXnwSsCYcXlqE\nCtG8SdaoySDF7/3dPKcZb33rLWxZsgWAde/JSHLSV3RUMfhAjMDIBufsUCZEnDO1tWv1HnahSchM\nEyJbXGWmyqMpoRBqsh+br53B4UG0PNpSPELDWbDSKej/8z/DOU8slronnfdi0I6Q44Sg97d9PxU6\n97FdWIXhA8mWUAUGEU0lokeJaICIuonoOsV2NxDRCBEdty0L/B6noKhmh14PoaptpxPx4peZJpJJ\ny3dBjq61orJtHk0JhVCTZXZi0uzgO8IjxaFp6EQmvfEGUFWV/bkSiVSYt/P8Mu3GMEbQ+1u2nx0d\n34VTWGXrAwmDsDWMewAkAJwGq7XqeiJS6NXYw8w1tuWpgMcpDLLZoc5DKLQQt17cgp07U9ufcUZ6\naXEg03GeTAJPPx38NwUg12qyzBwgsxPf3HQzYhUSJ7GEooh9V0UmtbW559R4EYnI7y1nmLeXdmMA\nEPz+dvsCUbGwAAAgAElEQVS+obYhzXehMnmphE4FVYzd9/lO+qsM60BENBHAlQBmM/NxAM8S0TYA\nywC05fs4OUU1OxwYyHwI77knc3/dKKmVK4G77rLszn/+c2p9IgGcOpW5vT2ZL0/U19ZL1e4w1GRn\n1qwwBwCWndj+oHQe6MTPfv8z7WMXPPZdFTTx7/9uhc+qcmrszJsHnHcesHlzap1OmPf3vy+/f2+9\n1cru7u0FvvpVK2t8nGd7B72/Vfs11DbgrW+9NfbZ7R5X3aNJTiK52udEIiTC1DBmATjFzIds67oA\nqDSD84noCBEdIqJbiUgIL7/HyT+ymd+pU1ZuhczE4HQu6kZJPfKIJTTswkIgezEUIBRSZh7KVk0W\nM66lW5dqmwNW7VoljURRkU+7rxRZZFJPT2rS4ZZTI5YdOzKbaAEp/1dPT7pJK5Gwcna+9S35/Su0\nDOPbGCPo/a27n5vJS3WPFvLeDVNg1ABwhv8cAzBJsu0zAGYD+CAsbeJaAN8JcBwQ0XIi2ktEe/v6\n+gIO3Seymd/wsDqk1v4A+o2S+sUv9LaLxzOdmnlAZh7KRk3WcRbKZl5uGkPYAi1n+DUTqQIoxMRB\nNrFJJq3kPtn9+/TTwCc/aXwbNoLe37r7qe7b7qPd6D7aneGbK/S9G1rxQSI6H8BzzFxtW/dtAJcx\n8xc89v0qgO8w8wXZHCdvxQedKntvr1UkbmAgc9vGRiva5cQJ66V+9dXApk3Bzy2O8fOfpz/0sRhw\n001yE1gJMeOuGa7CArBsuJu+vCnt4VPt11DbgHUL12HVrlU4fPQw6mvrsW7huoIXe8vAb9FIr+3d\n7kkiS/tgTh3Dfm/aC2mWwT1VzOjc7wQCg8fu5Vzcu4UoPngIQCURzbStmwvApZjNGAyMidJsjpMf\nnCr72rXA0JC8wNz8+emzRll/bT+IY5RpZq6Ob0EW6aSKnFo8c/FYXHxydRJvfeut4hMWgL/yHL29\nVgVap3Zx6hTwiU+kHOaDg1awRG9vZnh2W1v6OU+dArZYeQNpOT9Gy8gpsvvWiRAWxXDvhiYwmHkA\nwFYAtxPRRCK6BMAXAWxxbktEi4jotNH/nwvgVgDb/B6nIDgjobq60rvnfepTqQdM5hwfGEjlaDht\nzIJoVF0rKJEAzjyzbDNzde2zTl9G85xmtMxtSVPhGYxNXZsKH0Krg5/yHMK06SxAODxsrRcdHZmt\nzytXpt+HzJa/rb09tW54WO4XMxFUgdFJ+HOarlQUPEhjlLDDalcAiAP4M4CHALQy80Eiqh/NtRBv\ng4UA/kBEAwB2wBIQd3gdJ+SxBsNpZ7a3ukwkgOefT9c83GaNKhv08LAVKSPD3j+hDDN1F89crL2t\n8yHa8dqOjGzvogih1UG3PIeYhACWGWn/fuCiiyzNQrRz7ehIv68eeUTuX3OLwBIkEpZvo8zuM7/o\nZnuL7WgNYdnWZWkJf6oSNXYNuKG2QXrcggdpjBKqwGDmd5n5CmaeyMz1zPzQ6PrDo7kWh0c/f5uZ\nTxvd7mxm/l/MPOx1nIKj6lfg7L3d3m5pHps3u88aVWWqZYjIlx070uP0yyyaZcdrO7S3dT5ExVA6\nIefIJiwvvAD8/vepe2lkRK59OHHzX9prls2fX3b3mR90s72dARuyUjX9Q/2utc1yEXUYJqY0iB90\nE6kSCetBtvs1RMmQ/fuByZOtF/6OHfqZvPbIl2efTZkdyiyaRfflLvwT9lnf1PhU6bbFMjvLGtWE\nReA3yS8eB1pa5N85e2WU2X3mB91sb6/sbjsqwRF21GHYmBatfjj/fP/tVkXkyu23A/fdB3zsY1bL\nzZtvtgSJvR2niEhRRcDs2WOFPZ44YZVRr6jI3LfE0YkaEVRHqz0f0GJpgRkK9vatYRCLWdqI3Vzl\njMxytowtk/vMD5E1EWVhy44lHWP3ltt2bhTDPWpatOYCu53Z7rCuqJC3zARS5R7ELE203GxvV9cS\nUvk+7P4Suw26jKJZdH0YFVShFBbCeVhss7Os8cr89otXOX7TiQ+Au4ZqN02pNFwvgvjZCtVIyQiM\noDhtySpfhOjx7XwwE4nMh188rKqImVdeUb8wyiSa5ZGDj3huUx2tlhZiExRTGGKoOBss6dQjU1FV\nJS+fbvexmU58AICPTv2o8rvB4UGs3LkS0+6cpixjroMfP1shGykZgREE58wLSGVayx5CmUCx97gQ\niIdVFjHT2qrWYuz7FhF+Z0GdBzo9HzqhNdTF61y36z7aXZztW8Nizx7/Pgs7iYQVPOEWmWU68aHz\nQCd+8+ZvXLfpH+pX3rc1sRq0NrUqS5MLhBaj88wUspGSERhBcKs0KutRocIeiWJ/WGXhsipzhGiI\nU2R5GEFmQV43vD1a5NhJ7/IqhWpjmRdUkwpddCobm058WLVrVSC/hOB44jju33c/kqwW7uK+1n1m\nChkNaASGF7ovb1Fp1E0LcOKWmOUMYyyxhzfILMjrhhfqv99CgyWTi6GD8360f9bttSL6qBw5klkc\nswxze7IhjJdwYkTtd6qL14352XSfmUIWJTQCw4nzgZG9vO3hsMIU1dNjRS+pfAw33OAvMavEwxhV\nkU7ZdiDrH+rXjqKyUza5GLKyNOLzvn16WoaIjHznHeCWWyzN+JlnrP+XYW5PNuT6JTx0amjs/7qa\nQyFzNYzAcOKsLKvqqmd3eLe1WbV9ZIXeBFu26L38y6SxTQVV+FoP6NXVCUpZ5GK4laWxf/bD5s2p\nEumbN5fFZCVMVDXKWptalVnZfrBrELqaQyFzNYzAsON8IG+5Rd1Vzx5q2NFhrXfDvr9K5S+jMEZV\nFJNbdJP9QciGYisJHRpuZWnEZ51+8XaSydQ+yWTq3ivhyUqYyF7OW5Zswb2X34t1C9chGvFhglYg\nNAg/mkOhCmoagWHH+UDKGiLZhYhA9yG1Z2o7H0ZVBdISfXBVL30vYSAeBF7NnpFQKkRYLYFQF69D\nvDKOZVuXlXbElFdZGhF2LQvvFoER11/vfR5npdqurnHv03B7ORPp9ZJ3Q+RvFHuWN2AERgrZAyl7\nef/7v/tLnrL7LnbsUKv8qgqkJRrGGIad9e5FdwcyUYkcjC1LtmDo1BD6h/pLP2JKpyxNZWXKoV1R\nkaqKLPxkup0eBUJrMT4NKat2rXJ1aOty7OSxsXuy2EvxG4Eh0HkgEwngrLMyewu4sW2b/ByyjFog\n5UQvgUgoN8KYLclKlusghFIh49VDRyfLe3g45dAWvjU7fhP9hNai69MYZxFWYQVSDCeHS+aeNAJD\noJPnYNcSdLWMilEnr0yDaW/PLAUiHvQyePC8ZkuyJKXOA52Yduc00BoCrSFs2LvBVxz8xOjEsfOU\nVfVaZ1i1vTSNKHNe4Qgo6OhID78VQRlie5U55ZxzMpNFdVvGlqg2EqTUhspJHaGIb3NqqdyTRmAI\ndPMcdCvWCs48U73fiRPAN78pd6Lv3l2SD54usiSlrz32Ndzw2A1pWbN+k6bsGkUh49VzjnOScc01\nchOq0DJkDnOVwKis9B+AUcLh4EFLbajqnv3NBX+DI989Al7N2tpxqdyTRmD4Rcc0EIlYGdw9PalS\n5qr9tm2TO9GZS+7B84PMXDScHMap5Cmt/VXhufYHb/HMxeUZMSV7mb/6qnzb7dvVDnPVxOeVV9TB\nHapJTAmHgwc1Xap6t9jX6woCP43D7OS7CGGoAoOIphLRo0Q0QETdRHSdYrsWInqJiI4R0dtEdCcR\nVdq+f4qITox26TtORIqnoQA4NZF58zK3EWUX7Cq6qveFrAihoMQePD9ko4JXR6ux/ILliFWk+5Fi\nFbExYdB5oBObujalaSgEQsvclqJzJPrGj5Z75pn+teJo1F/f+BIPBw9qutTZTzZpkeGncZigEEUI\nw9Yw7gGQAHAagGYA64lIUo0P1QC+BWAagItgtWz9tmObb4x26ath5nNCHmc49PZaGkRLS7pDMRIB\nmprUOR1OWlrkPb5L7MHzg18VvIIqxpznLXNb8MjBRzIiVBIjCTx3+DkA8lkjgwM9mEWHrr9NmFT9\nlkW39563N/6aPx/YuTNz+xKvahvUdOm1n2zSoiLIBKoQQR2hCQwimgjgSgC3MvNxZn4WwDYAy5zb\nMvN6Zt7NzAlm/i8AnQAuDmsseWPtWsvX0NGR/sAkk+nrvMJxhXOyxB88P/jJ6o5Gotj05U1Irk5i\n3cJ1uH/f/crqoBv2bkDngc7ycng78VtXTGzf0wOccYbluzjttFQPcBnivhNaslt4bYlXtfUbAi7M\nQLISNfb9/HTgkwkfL3NTIe7xMDWMWQBOMfMh27ouADINw8l8AAcd635AREeI6DkiWqDakYiWE9Fe\nItrb19fne9CBEWo4szxxz9ngaHBQXvpcbOvWB6NEHjw/iLBbr2iSungdHrjigTEz0sqdK11j3xmM\nVbtWlbfDOyhtbamQ7XfeAU6eVG+bSFhmVWfjL5nGW2KFMZ34CQF39u224zR56r64ZcJJx9xUiHs8\nTIFRA8BZc/oYgEluOxHRjQCaAPzItvp7AM4G8FcANgJ4nIg+ItufmTcycxMzN02fPj3o2P3j1y48\nMmKFysp8HkB6HwxhBnAmXpUZzXOaUROrkX7XUNsAXs048t0jYw+gTr8MwCpw2H20uzwd3kHp7bU0\nWR1E2f3588eNxqubMOemNThNnjovbgKNmZHswkDH3FSIIoRhCozjACY71tUCeF+1AxFdAeAHABYx\n8xGxnplfYOb3mfkkM28C8ByAYGEEuUDWQMkLt+ZITqFQwvHsftFVqzsPdOLGbTf6OjYjFdZYjGUW\n8kpbm/4Ex65dyDTeMvWr6eDHEa4T+ST8G04NQue5KEQpkTAFxiEAlUQ007ZuLjJNTQAAIvo8gJ8C\n+AIzH/A4NgM+031ziY52EY2m24njcbnD0EkJx7MHQVetDlqGoWzbtfph/36rEq0Xdqe5TLsQlKmW\noYMfR7jfAAvR78XtPLLKtfksJRKawGDmAQBbAdxORBOJ6BIAXwSwxbktEX0alqP7Smb+neO7DxDR\n54ioiogqiagZlo/jibDGmjW6ZRreeSdVG0r3ISvhePYg6KjVnQc6A/XAEJSFozsbli51/17UO7Nr\nuW73+DjWMtyCNZz3bZD7rn+oH50HOgva88KNsMNqVwCIA/gzgIcAtDLzQSKqH82nEOLxVljmqh22\nXAsx/Y4C+EcAfQCOAPg7AFc4nOmFReVrsJdtEGUanNU/3R6yEo9nD4KXWi2cf1401DYgFpHX9xLV\nQMclvb3ebYPt9c4EO3bI723RoGloqOwnMzKcJfhFAqnMHBTU+bxq16qirVxLLIqVlQFNTU28d+/e\n/J1wxQrgvvuAm28G7rkntb6lRW4CILK+UzW5WbECuP/+9JldLAbcdFP68UuczgOdWLVrFQ4fPYz6\n2nqsW7hO+SCowhf9UBevw5HvHvHesBwR96ibCXXaNMAZYSi7t3t7gQ9/OBVdVVUFvPkmcPrpuRl7\nEeG8ZxfPXIwdr+1wvYc7D3Ri6VYP7U4CgZBc7SOgJgSI6CVmbvLczgiMgPT2AmefbdWDiseBN96w\nHpzeXquirapHRl2d1UtZxvnnW/ZmJ/PmlU2klNAY7BEg1dFq6ewp6AMno6G2QUtAlRX2e9SNeNwy\nQX3zm8DDD1uahOzedgqfSCRzslSGyO5ZJ6p7OP6PcZwY8bj+DoTPLZ/oCgxTSyooKl9DW5tcWAgT\n1eCgZWKSlYIu8Xh2HXSzUzsPdOL6rRoNfzSxx7Mv27oMtIZKq6FSkNLha9fqNfdy9r2Q3du9vVZ1\nZWeCqqi4XMboJOCp7mG3DpMyisFP4YbRMIIgm7mJmdjs2UC/S66AMDExy81ZZQ6tUQe78WoeU/2z\nNUPpopoZFh0q8ydg3Y9f/aqlHdjNQyqN1Y1YLNPZXVVllcA5ciTTtDUOtIzImoh21WShyU6NT8V7\nJ95DkvVNS3XxOty96O6C3ItGw8glbiU8zjrLfd9EwvJTrF8/bkJn7aiqzFZQhWsWba4oiYZKXqHW\nqrydHTsye2TYEc26WltTtdBkkVGJBPDnP8v9IMlkWVYisKPrvCbQmCbbP9TvS1gAQE2spugnLkZg\nBEFVwuPpp1NmpdZW9cNqL8kwDkJnBW4q+giP+Kq9EyZu4Y/5Lh8txS3U2k2YeJmkRL8Mp6nJifhO\nNF5yRk+VkclUhm7dM7+9W5yUQvi3ERhBcPoaxAztssus78VDrGM/Hgehs4B3eKxQ5QuBqvDbtDun\nYenWpXktH52BV6i1TttfFYmEVRTT2UdecMMNwPXXpxotjdMe37IQ19am1rHPfrvrqSiFOmdGYDhx\ncy7KvpPN8NauVT+EkySltcqoLasKN+1BOPoK8cC4FX6T1a3KuwnLzfzpJky8qhGIZL3TT1dvt3mz\ntQg/p73x0jiY5NhxZlTfe/m9Y59V9dBkOHu4CIrd2S0wAsOJWx0n2XeyGd6ePWrt4n1JaS0x0yvj\nmZub9iCczjLVX7fFpS6xihjq4nWuyVBeprG8akJuFYzdhIlXNQKRrDd/vuXoluEmcMaJKVXVd96+\nzo/Prf1L7VpJf8WKiZKyo8qtUH1nj1cXiJh2VVVaJ7EYcO21VoSL7LxlgurBcsacy5L6wo6a8krk\n84qKKUScvBSdvB0RXfWxjwGHDqU034oK4O23gUWL/EdSCUrsXtVJGLVH6UUokuG4jlXEwMwYTios\nCC4UzX0jwURJBcHNuSj7TjXDu+Ya/XMmEkBnZ9nXj9KtjSMrpuan2ZIOol6PCjfTWFGZDrzyduzm\n0oMH082kwgxqP4az46MXJXSvyvpLLNu6DCu2r5BuA0Aa5ZQYSQQSFkV132SBERgCN3uw6rtnnpGb\nC15/XX6OWCyz5tQHPwicOlX29aOyrY0Tr4yP/b8mVoNoJJr2fXW0OsMRGSH17e3mh1i3cJ3U1lwX\nrysZ0wEAbz+G6PSou72TEmrupWrZKzo0qrYJQgVVpN2Lqnu9KCLwfFJZ6AEUDW72YGb5d5ddBrz8\ncvp6URpERiIB/OEPwHnnWZ+/+U0rvt2JOG+ZJUM1z2lWvmxV5gJZWYYkJ3HTJ25S1vIR+7jFwXv5\nIZym2mgkWrCkqkDo9Gxx3me6vb9LsL6Z6u8tOjQ2z2kOzTeV5CTuvfxe6Xd2kxeBMvphACjqe8xo\nGID1cG3erHYu+mmd6hX7ft111r/79wP/+q/ybUpo5hYGbu0oVaVEdry2Q9kHQGemWF9br5zhrdq1\nKsPsMJwcLt4EP1n0nq62sGtX6v/CPOXlfytBLdjNzCgERVhReqrjOE1eTj9ZKSSRGoEBWA/X0JDl\nIJTZg/3UeNqzx/1cr7xiPWhXXpn5nci8HQfJUHbc6ku5dR5TvfC9ZooEwkenfjRDSC3duhQ1d9Qo\nHexFm1gli97T1RYqJUYG+/2uug9LyH8BWGZGVcSdeMGH4Stz81Ws3LnScyJTtPfYKEZghN3hbt8+\nK9lJRTRqmaLeeCPzuxJ7CMPCTSioZmtT41OVWonXTJHB+M2bv5E+vAPDA8r9ijKxSnb/9vZatZ/E\n5KOnJ5V850RMYFSomi+VmBbcPKcZNzfd7Nrn3dnrwi9ufjndfvRFeY/ZCFVgENFUInqUiAaIqJuI\nrnPZ9u+J6E9EdIyI2oloQpDjZE3YHe56e62oJxWJhLxhjfiuhB7CsHBrR6mKrgIg1UpaHm0Zsw+7\n4VXGwe3FUlTI7t+2Nisgo60ttU10NEggFgMaG1O5F9Go+p53a75UgiX37738XmxZssW1WZfQauvi\ndRmBFW7UxetcW6SK1qtuFO09ZiNsDeMeAAkApwFoBrCeiBqdGxHR5wC0AVgIoAHA2QDW+D1O1uh0\nuPNbVtrNhzFvnjXbU5kKpkwpuYcwDNxCblXRVe8OvSs9lqhVxeCskv5EL/Bi6naWgez+bW9PTVg6\nOoCursxtDh7MvOe7uuR+kKjkpVlRodefvghR9cB2+tH6h/p9hc/2D/Vj2p3TlJFOXtpF0d5jDkJL\n3COiiQDeAzBbtFMlos0Aepi5zbHtQwDeYub/d/TzpwE8xMyn+zmOE9+Jezod7tzKSjuRlT13diVb\nscKqVCtD1vlsnOCnCx8QTic+N4o5yWoM2f1LlCrlAQDnnGPdf27+jFgMmDkT+I//SL/P3cqjr1hR\nUlFSXoR1P6nK5XuV9S80hUjcmwXglKP3dhcAmWbQOPqdfbvTiKjO53Gywyv6ya9/QxaZkkhkOiNV\nDAyUVORJmKhmfiqydVB6aR/HE8eLPz5edv86J4Cvvurt/E4kLNOT8z639653JvSVWJSUDHvQRFiT\nD1UjJVVOUFiFC/NFmAKjBsAxx7pjACTV9lAD4KhjO4xu6+c4IKLlRLSXiPb2+Z2de0U/+fVvyB7g\nZNIqe24/Z0+PvPT5OHV6B8FpqlL12VAhTE4q+of6C1ehVhfn/asKtrj6avl9bq+2LExPsnvQLUep\nRHGaoMLEHsTx3zf/dyzdulSaExSriOHuRXeHeu5cE6bAOA5gsmNdLQBJtb2MbWtH/33f53HAzBuZ\nuYmZm6ZPn+570Ep0/BtOduywirm1tKScirFYquy5QOXnGKdO7zD4QNUHlJVAZdTF67SLHZZCfDwA\nYPt2+XpVkAWgd5/7yUMqEXLZe0UEcazYvgK73twl3aaCKtD+pfai91k4CVNgHAJQSUQzbevmAjgo\n2fbg6Hf27d5h5n6fx8kdQWZVa9cCu3dbzkbVAyjrUzBO8y+yYcX2FVi2dVmak3Ik6a9/ssyhrppt\ndh/tLn4TlVuFAdVER+c+L8Ne87nKd7BHOm18aaNyuyQnS05YACEKDGYeALAVwO1ENJGILgHwRQBb\nJJtvBvB1Ivo4EU0BcCuABwMcJ3f4nVUJQcCcqT3YH8C2tvSOe87vDZ50HujEhr0bMl7uqm5+MkSU\nldN34mamKikTVWurXuhsGWoPOoSZ72Dvh2GveeZ2PxZ7voWKsMNqVwCIA/gzgIcAtDLzQSKqJ6Lj\nRFQPAMz8BIA7ATwJoBvAmwBWex0n5LG643dW5VaKwf4Abt+e6ZgcBw9omKzatSpru3OEIlKNQceZ\nXvQmKj/mVLf73G9IeQnhlvntl6HhobH/9w/148ZtN6LzQKerX63Y8y1UhCowmPldZr6CmScycz0z\nPzS6/jAz1zDzYdu2/8LMpzHzZGb+GjOf9DpOwfB6cGSF3uxmJvsDODCg/t6gRRjmhBEeGdMYbtx2\nI6bdOQ2RNRGs2rUKLXNbxsxUKoraRBWWk9qtmViJ4SwjA0Ca+R0EpyaRGElg5c6VWH6BvCXxwg8v\nLElzFGBKg+jh9eDIHtBTp4BPfEJdEM6YoQIzNT5V+V0kwC2dGEmkRUVt6tqEdQvXIbk6WZomqjDM\nTGGXzCkgquKWF9dfPJb5HTb9Q/24uP5itDa1jmkaouz5/73+/4Z+vnxhOu554dWF78tfBg4cAAYV\nERciwUmW1FdiHcuKhWl3TpNmzsYiMYAsAZAtInFPVl7dbfuywZ4UWILlzO3odHv06rIYBFUSXzFi\nOu6FhVcXvhdesISFvdKtPdFJzM50tRCDJ6qyIIlkIhRhAaTMXs5IKq/ty4IgIeVFjFtxS8DSQMIW\nFkAJ+LoCYASGG15d+NrbU9u2t6ceKJmQkZkJhoet4xjTlC/yEWEyNT51zOa9atcqTxNVqUa9SCmz\nRD234paAe/dFGX6SRHNZvqYQGIHhhtuDs3Zteo9kUQJEJWR27sxsz+rUQgxaqIoVhlVmIVYRw7GT\nx6Sl03V7kxcFQaOcyizU1utv5lc7XH7BcnQs6dAqTUOg4vJvZYkRGG6oHpynn7Y0CrswSSatdbfc\nojc7Mw7wwKgq2N696O6sG+AAVjtWZ6VSUTp92dZliFfGURevK+5KtkDwKKcyTNSz50c4e7P71Q5F\nQp79HlRNVkQL2HLBOL2DICrYOgVDJGKVKO+XlDK29w8Ypw5wvxVpszlHrk0BRe/QdAvWGEfIghac\nf7vOA51YulXRKEqB7O+vqkhLICRXa7TLLSDG6Z1L9uyRJ+klk1Z5Bq/ZWZnZiHVw69sdJiJzm1cz\nOpZ05KwaaNE7NO332DgOrnBr/ytontPs+z6R/f11/FuqtsKlghEYQVCp7Lpqe5nZiHXQeXDDRGga\n7w69i4baBnQs6Qgts1dQtJFRTj+aCK5oc20nU7K4vYS9IqQEQcyZTi3Wy1eSr0lTLjECoxCUoY3Y\nC90HNwxUD6Zbwl8QVLbvgs8iVWVqOjqKXsvwe+3cXsJufSgYnHF8u58jCE7fWl28DvHKOJZtXYYZ\nd83Ayp0r8zppygXGh2HICzrJU7p4+UJU54pQRNqXIAgqH4aOzTznlGinvCDXTvW3rovXYejUkGfC\nZXW0Gi1zW7Cpa1Ogcueqbnm6CZ9Acfg4jA/DUFSEFY6qo9artJYwhIVXZJSb6S1vmkeJdsrzY7YU\n11IV3NA/1K/1sh4cHsTGlzYGEhZu+Rh++m2UUg6PERiGvKAKhfU769Z5qeTyAYxQBItnLlaOWyWs\nhGDLq/26xIIrdM2W9klDGPgpi29HVVwQ0De1Fm0OjwIjMAx5w2/fbhk6L5Vs+327McIjWL93PWgN\nSbUElbCqoIr8269LLLhCde0iFEm7zl6z91hFLOe9shd+eCHuvfxe5feq31IXr8t60lRIjMAwlBRe\nZR6A7Pt96yLTElSmN9UsNqeRViUWXLF45mJpJNsIj6RdZ69rJqoPO4lGor7a+Lrx+ruvu36vug/u\nXnR31pOmQhKKwCCiqUT0KBENEFE3EV3nsm0LEb1ERMeI6G0iupOIKm3fP0VEJ0YbLh0nolfDGKOh\nPND1hTTPaca6hetQX1uvZXJQRdN4IYvpl5neVDH69ppVpRiXHxadBzqxqWuTsgig/ToHNTlOnjAZ\nl9ZfGniMdryEVlgm2GIjlCgpIvo5LOHzdQDzAGwH8NeyLnlE1ArgZQAvAJgO4N8A/IKZfzj6/VMA\nOs+OD8YAABSMSURBVJj5Z37HYaKkxgc6GeN+olSyRSfKRTaeWEUMzJxWhiSsiKp8ZNWHOSY3B7ZA\nXOd8/m1VlFs5+7xFSRHRRABXAriVmY8z87MAtgFYJtuemdcz825mTjDzfwHoBHBxtuMwjB+cvhAA\nGbN0P1Eq2VJfW+8aAWUfjzCPNdQ2YFJskrRmVbZ+jWJMEPMak45pTmgWYvaeaz+FilJzVIdJ1hoG\nEZ0P4Dlmrrat+wcAC5j5Cxr7Pwbgj8zcNvr5KQCNAAjAqwBWMfNTOmMxGsb4QxW7n8/ZZ2tTa0Yc\nv4jvf+TgIxn2dKFFLNu6TGqCyTYuP8ycl7DwGpOXhqHSvGruqMHA8EDo43WjtanV1eFdiuQzD6MG\nwDHHumMAJnntSEQ3AmgC8CPb6u8BOBvAXwHYCOBxIvqIyzGWE9FeItrb19fnd+yGEkcVZpsrR7eT\nungddry2QzqGDXs3SJ2vg8ODWLlzpZYDPwj5zKrXxWtMMt+UcIA77f9Cm6M1lHdhAQA/+/3Pxq2v\nyVNgjDqhWbE8C+A4gMmO3WoBvO9x3CsA/ADAImY+ItYz8wvM/D4zn2TmTQCeA7BYdRxm3sjMTczc\nNH36dK+fYygzVC+iER7JWWitnbsX3a0cg1sXt/6hfiyeuVg7mdFP0l+uBFE2eI1J5iTesmQLeDWn\nRROFnYMBQBqZ5cZwcrikynmEiafAYOYFzEyK5RIAhwBUEtFM225zAWQ4vAVE9HkAPwXwBWY+4DUE\nIOSqcYayQfUickYnCY2jLl4XWmhlZcQK7gtao2rHazu0Imn8+iRU4anHE8fzMjOWCTeZBhGriOF4\n4vjYdgAK4pu6uelmZRSbiqItPJljsjZJMfMAgK0AbieiiUR0CYAvAtgi256IPg3L0X0lM//O8d0H\niOhzRFRFRJVE1AxgPoAnsh2noTxxC7O1lzo/9b9OgVczjnz3CNq/1D72gsimgu2p5Cms3Lky8P5u\nLx37S7fl0Rapyavl0ZYMjcMtPLV/qD/nzm+VcAMyGw4xM/qH+qVCUHWcoJqF+DtPjE7M+G5T1yap\ntudGKZXzCJOwwmqnAmgH8BkA/QDamPmh0e/qAbwC4OPMfJiIngRwKQBb9yDsZuZFRDQdwA4A5wIY\nAfBHWNFXv9YZh3F6j0+yCSHNV8MlGRGKoIIqMsJqgxTDE05hP7+lobYhtHBbnetoP19QJ3gFVQQu\n5SEmCW7H1Tl+NBLFA1c8UPAw5TDRdXqbarUGA/TyAFRk8xIL83gNtQ04fPSwq+/EiU7eh5dA9pMX\nIc7n1uGOV7Oye504RhCzlNAyvK5PdbQa8cq4NGAhQhFs/vLmshIWgKlWazD4IhubdNgO9qDCR7zQ\n/eCV96HjP/HjVxDnU0Wx6US3tcxtCWRKrK+t17o+4rfITJ3lKCz8YASGwYDsbNJe5T/8oipTEqGI\na20sMfv3K7zchKVOuXa/mln30W6lUNQRlpu6NuHmppvHgg50iEaiWLdwHdYtXIdoJOq5vSiPbk+0\nLIfSHtliBIah5Amjz0Q2FW4Xz1w8ZrIJA1XntyQnUV9bj+UXLHd19PudgbsJS51y7WHijGqTMTg8\niB2v7cCDVzyoLaSJSPp/L4T2WAylVYoB48MwlDRhdrjrPNCJlkdbfJuEYhUxJEYS3htqUBevk9rO\n7RAIn/7wp/H6u68HrsskCNrRLmy/DWD9LgajobYBH536Uex6c5frtiIbXvf3ujm9dfYtp9pRTowP\nwzAu8NOlzYvmOc2uXflUs/awhIXAq3Iug7HrzV1YPHOxtEy2l6bjx8zit1x7NghndPfRbuw+vBsL\nP7xQua1dK9LVDg8fPRxYCxyveRdOjMAwlDRhl8FwSwTcsmSL75Ijfsum9w/1a7eSVTVycvsN9pwU\nnX4Mfsu1A8CEignS9X6uXWIkgd+8+RsAmYLamQ2vW4zQy+lNINTEaqTfTY1PxbQ7p4HWEGgNYdqd\n08ZleRAjMAwlTdhlMLwSAf30BW+obcDmL28OLbNchTNyKdv+6Z0HOtNejit3rsS6hevStJnFM5XV\nenBy5GTGulhFDMsvWO7LtyI0DnsYbF28TqoVNc9pVr7sgdTvd9NGGIwJFROkGel/OfGXNFNh/1A/\nvvbY18ad0DACw1DSqF4AQctgeDW+8SOIDh89jOY5zWj/UrvvcfhFFDQEsmve03mgE1977GsZL8cb\nt92Ydj13vLbD1/gmxSbh3svvxc1NN/vaz8nQqSHld25apfj9IihAxbtD72Zcu2gkKjXBjceaUsbp\nbSh5Og90YuXOlcoy4n6c32EmqdXF61ATqxl7kflJqAtKtqW3p905Tel0tzt+I2sivn6PvfmRW9Ke\nDioHtE5Zd6+/n/PYXuPNthR9sWCc3oZxg8oc4df5rZOkJmbvXr6JaCSK9xPvjx0rH8ICADbs3RDY\nTNJ5oNM1Qss+g/dr8hNNpkRdqWzoPtot/Y06pji3JEOZ2c7r/hlvNaWMwDCUBWE4v3UjrprnNGNK\n1RTlcRpqGzB5wuTQo6d0YHBgM4nuy9FLsDipjlZj8czF0iKKQRGC3J6Ds2rXKrTMbXE1xemYrey4\nbS+SAccT+qmSBkMRU19bLzVH+PU56K5/d+hd6bYEwlvfeguRNYWbi+UqdHTxzMXoPNCJG7fd6CoM\nCYSp8al4d+hdTI1PxcmRk1i/d32gMakQPpuhU0NjQqj7aDc2dW1yNUOq7pOG2gbpPqrtCVR2BQh1\nMBqGoSzINjIIUPe1kK33is4qxmZFXnj19djUtQkrd6701JwYjPdOvAeGVb78eOK41vmDhCD7zcHx\ne5+ott+yZMu4ExaAERiGMiGbyKAgeL14sik1okLnhapjJrGbcSb9YBIiayKgNeRpZhocHtQ2RfkJ\nPxacNfmsUOpxuWlKfu+TfN9XxY6JkjIYRlFF/qgiYXQiqsT3U+NTfdn9s8Gtz4WfKK9iR1VGRfz+\noD1SxiN57Ycx2kDpfgCfBXAEwC2igZJk2xtGt7UHVP8/zPyU32M5MQLDkA06YZnZ4NbjIWxESDGA\ntBfn8cTxvAmuXFIXr8Pdi+6W1hGTNaAKWl9svJDvsNp7ACQAnAagGcB6Imp02X4PM9fYlqeyOJbB\nEAph+EFUeIW61sXrPEtb+GFweBDXP3o9vvbY19LChMtBWFRHq3H3oruV5qIdr+0Irb6YIZ2sBQYR\nTQRwJaxWqseZ+VkA2wAsK+SxDAa/6Nirg5ZS93pZ9Q/1Y+jUUKhCI8nJtPavpYzqbyL6touyJYC6\nGq0pIJg9YYTVzgJwipkP2dZ1AVjgss/5RHQEwLsAtgD4ATOfCngsgyE0RPkIGU77v0jsE/u5ofOy\nGhwexODw4FiZ71ImQpFAjm8ZuiZBr8TA8ZZklwvCMEnVADjmWHcMwCTF9s8AmA3gg7C0iWsBfCfg\nsUBEy4loLxHt7evr8zl0g0GfbEqp+3lZeQmLaCTqOwQ13zCz1Zs7QCtVO35Mgn6zuA3+8bzriOgp\nImLF8iyA4wAmO3arBfC+7HjM/AYzv8nMSWY+AOB2AF8Z/drXsUaPt5GZm5i5afr06V4/x2AITDbZ\n5EHCbFUv2+HkcGiz91wRNB8lggjq4nWBQlj9ZnEb/OMpMJh5ATOTYrkEwCEAlUQ007bbXAAHNcfA\nwNiTke2xDIackU0pdad/pC5e51n2PJ9mqWw1ATtB81EaahuweclmHPnuEWljKC/c+oAYYREOWeu1\nzDwAYCuA24loIhFdAuCLsHwTGRDRIiI6bfT/5wK4FZZj2/exDIZ8km0Uld1Be/eiuzEpprS0hoqX\nI72CKnBz083SPhDRSNTXuWROaa+GS3XxOs+GTjrBBrmMcjNYhGUIXQEgDuDPAB4C0MrMBwGAiOqJ\n6DgRCfG/EMAfiGgAwA5YAuIOnWMZDIUkrKxf4ZzNV4ir13mSnMS9l9+b8dvav9SOB654QOsc1dFq\ndCzpkL70haBUaTGqulwCVRXhFdtXpAkRACYrO8eYTG+DIc+oEgQrqCInvbK98IpCUo3XTseSDs8X\ns6rXRtDzO6PJTHJecEw/DIOhSHCaU1Qv3yQnQ6ml5AcCoftot2tOiZcfQidiq/NAJ95PZMau6NS+\nUjmznT4ek5yXe4zAMBhyiMycojLNiJpHfqOpYhUxtDa1+hY29hm6rFmUQJjiVL6QJCeV+wpW7Vol\nrXI7ecJkT40gjBL1hnAwAsNgyCGy3ABGZn6CcM7qOIkBZPga7r38Xrz1rbfQsaTDM+KpobYBDbUN\nyhm6zMHcPKcZdy+6Wyk03Gb3nQc6lVqVl/8CkGs4bkLXkDuMD8NgyCFuva8bahtcq6kGLYa4YvsK\nbNi7QVl59+amm5XfA5bG4tQGJkYnIjGScC01Iqvq67eHtgpnZeDFMxebAoMhouvDMB33DIYc4tbh\nzetFuW7hOmk1Vi+b/72X34uL6y/Gql2rMs7NYM/udzLT0cDwgOs+gHx2H1b2taxki/iNpoR5/jAa\nhsGQQ2QzbD8zYa+eG17oRDiFgeo3uWlYOpFVhvxgNAyDoQgQL8SgL323Yog65MMJ7NawyW8PbUNx\nY5zeBkMOyVZDCHI+u8Paq093Nrgl6wnCyL4OWlLeED5GwzAYckQ25dDDOl80EpU6sd2IRqKYUDkB\nxxPHlduIjndevyNbDSvf19DgjvFhGAw5ItctX3XPVxevQ02sRunLqIxUonZCLd4dejfthd55oBMt\nj7ZIs89z9Ruc5PsajleMD8NgKDDZlEMP83zvDr2LI989AsCasa/cuXKsRIebptA8pxnLtsqbXeYr\nQS7f19DgjhEYBkOOUDl8c5VcpnM+v070fP+GYju/IR3j9DYYckS+y23n4nyFLhle6PMb0jECw2DI\nEWGVQy/k+fL9G4rt/IZ0jNPbYDAYxjmmvLnBYAiEyXswqAhFYBDRVCJ6lIgGiKibiK5z2XbDaAc+\nsZwkovdt3z9FRCds378axhgNBoM3qu52ukLDCJvyJiwN4x4ACQCnAWgGsJ6IGmUbMvPNzFwjFgA/\nB/ALx2bfsG1zTkhjNBgMHsiKBeo2JspW2BiKn6wFBhFNBHAlgFuZ+TgzPwtgGwB5ALd8303ZjsNg\nMGRPNnkP2QgbQ2kQhoYxC8ApZj5kW9cFQKphOLgSQB+AZxzrf0BER4joOSJa4HYAIlpORHuJaG9f\nX5+fcRsMBgeq/AadvAeTZFf+hCEwagAcc6w7BmCSxr4tADZzeqjW9wCcDeCvAGwE8DgRfUR1AGbe\nyMxNzNw0ffp0fyM3GAxpZJP3kI2wMZQGngJj1AnNiuVZAMcBTHbsVgsgs+N7+nHrASwAsNm+nplf\nYOb3mfkkM28C8ByAxT5+k8FgCEg2eQ8mya788SwNwswL3L4f9UNUEtFMZn5tdPVcAAc9Dr0MwHPM\n/IbXEACPJsUGgyE0gvbgyLYyraH4CSVxj4j+D6wX+00AzgewHcBfM7NSaIyGy/5vZm63rfsAgIsA\nPA3gFIBrYJmlznf4SKSYxD2DwWDwT74T91YAiAP4M4CHALQKYUFE9aP5FGOGTCL6FIAzkRlOGwXw\nj7Ac4UcA/B2AK3SEhcFgMBhySyjVapn5XQBXKL47DMsxbl+3B8BEybZ9AC4MY0wGg8FgCBdTGsRg\nMBgMWhiBYTAYDAYtjMAwGAwGgxZlVd6ciPoAyBsX55dpsJz2pUIpjbeUxgqY8eaSUhorUNzjbWBm\nz8znshIYxQIR7dUJUSsWSmm8pTRWwIw3l5TSWIHSG68MY5IyGAwGgxZGYBgMBoNBCyMwcsPGQg/A\nJ6U03lIaK2DGm0tKaaxA6Y03A+PDMBgMBoMWRsMwGAwGgxZGYBgMBoNBCyMwQoCIvjHa9e8kET2o\nsf3fE9GfiOgYEbUT0YQ8DNN+/qlE9CgRDRBRNxFd57LtDUQ0MlpAUiwLimh8JXEtC3EdJWPQvk8L\nfV1Hx6A13iK5thOI6P7Re+B9ItpPRItcti/49Q2CERjh0AOrym6714ZE9DkAbQAWAmiA1V1wTU5H\nl8k9ABIATgPQDGA9Ebm11N3DzDW25aliGF8JXst8X0cnWvdpkVxXwMdzhcJf20oA/x+Ay2A1kPs+\ngEeIaIZzwyK6vr4xAiMEmHkrMz8GoF9j8xYA9zPzQWZ+D8DtAG7I5fjsjDa8uhLArcx8nJmfBbAN\nVkOrguNzfOZa+sDHfVrQ6yrw+VwVFGYeYObbmPktZk4y878DeBPABZLNi+L6BsEIjPzTCKDL9rkL\nwGlEVJen888CcMrRY6RrdFwqzieiI0R0iIhuJaJQyuKHML5Su5b5vI7ZUOjrGoSiurZEdBqs+0PW\nRK4Ury8AIzAKQQ2Ao7bPx0b/nZTH8x9zrDvmcv5nAMwG8EFYs+lrAXwnZ6PzN75Supb5vo7ZUOjr\n6peiurZEFAXQCWATM/9RskmpXd8xjMDwgIieIiJWLM8GOORxAJNtn2tH/30/+9Fqjdd5fjEG6fmZ\n+Q1mfnNUzT4AS33+ShhjVeBnfDm9lhpoj7UA1zEbCn1dfVFM15aIIgC2wPJrfUOxWUldXztGYHjA\nzAuYmRTLJQEOeRDAXNvnuQDeYeZQ7LQa4z0EoJKIZjrGoOy/7jwFAApjrAr8jC+n11KDbK5lrq9j\nNhT6umZLQa4tERGA+2EFQFzJzMOKTUv2+hqBEQJEVElEVQAqAFQQUZWLDXUzgK8T0ceJaAqAWwE8\nmKehgpkHAGwFcDsRTSSiSwB8EdasKAMiWjRqjwURnQtrvNuKZHwlcy3zfR1l+LhPC3pdBbrjLYZr\nO8p6AB8D8AVmHnLZriiubyCY2SxZLgBugzWrsS+3jX5XD0sFrbdt/z8BvAPLdvkAgAl5Hu9UAI8B\nGABwGMB1tu/SxgvgR6NjHQDwBix1P1qI8ZXStSyG66h7nxbjdfUz3iK5tg2j4zsxOjaxNBfr9Q2y\nmFpSBoPBYNDCmKQMBoPBoIURGAaDwWDQwggMg8FgMGhhBIbBYDAYtDACw2AwGAxaGIFhMBgMBi2M\nwDAYDAaDFkZgGAwGg0ELIzAMBoPBoMX/Dzg2mdlQd6EWAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1271499b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(X_moons[y_moons == 1, 0], X_moons[y_moons == 1, 1], 'go', label=\"Positive\")\n",
    "plt.plot(X_moons[y_moons == 0, 0], X_moons[y_moons == 0, 1], 'r^', label=\"Negative\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must not forget to add an extra bias feature ($x_0 = 1$) to every instance. For this, we just need to add a column full of 1s on the left of the input matrix $\\mathbf{X}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_moons_with_bias = np.c_[np.ones((m, 1)), X_moons]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        , -0.05146968,  0.44419863],\n",
       "       [ 1.        ,  1.03201691, -0.41974116],\n",
       "       [ 1.        ,  0.86789186, -0.25482711],\n",
       "       [ 1.        ,  0.288851  , -0.44866862],\n",
       "       [ 1.        , -0.83343911,  0.53505665]])"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_moons_with_bias[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good. Now let's reshape `y_train` to make it a column vector (i.e. a 2D array with a single column):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_moons_column_vector = y_moons.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's split the data into a training set and a test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_ratio = 0.2\n",
    "test_size = int(m * test_ratio)\n",
    "X_train = X_moons_with_bias[:-test_size]\n",
    "X_test = X_moons_with_bias[-test_size:]\n",
    "y_train = y_moons_column_vector[:-test_size]\n",
    "y_test = y_moons_column_vector[-test_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now let's create a small function to generate training batches. In this implementation we will just pick random instances from the training set for each batch. This means that a single batch may contain the same instance multiple times, and also a single epoch may not cover all the training instances (in fact it will generally cover only about two thirds of the instances). However, in practice this is not an issue and it simplifies the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_batch(X_train, y_train, batch_size):\n",
    "    rnd_indices = np.random.randint(0, len(X_train), batch_size)\n",
    "    X_batch = X_train[rnd_indices]\n",
    "    y_batch = y_train[rnd_indices]\n",
    "    return X_batch, y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a small batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  1.93189866,  0.13158788],\n",
       "       [ 1.        ,  1.07172763,  0.13482039],\n",
       "       [ 1.        , -1.01148674, -0.04686381],\n",
       "       [ 1.        ,  0.02201868,  0.19079139],\n",
       "       [ 1.        , -0.98941204,  0.02473116]])"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_batch, y_batch = random_batch(X_train, y_train, 5)\n",
    "X_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0]])"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now that the data is ready to be fed to the model, we need to build that model. Let's start with a simple implementation, then we will add all the bells and whistles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's reset the default graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The _moons_ dataset has two input features, since each instance is a point on a plane (i.e., 2-Dimensional):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_inputs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's build the Logistic Regression model. As we saw in chapter 4, this model first computes a weighted sum of the inputs (just like the Linear Regression model), and then it applies the sigmoid function to the result, which gives us the estimated probability for the positive class:\n",
    "\n",
    "$\\hat{p} = h_\\mathbf{\\theta}(\\mathbf{x}) = \\sigma(\\mathbf{\\theta}^T \\cdot \\mathbf{x})$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that $\\mathbf{\\theta}$ is the parameter vector, containing the bias term $\\theta_0$ and the weights $\\theta_1, \\theta_2, \\dots, \\theta_n$. The input vector $\\mathbf{x}$ contains a constant term $x_0 = 1$, as well as all the input features $x_1, x_2, \\dots, x_n$.\n",
    "\n",
    "Since we want to be able to make predictions for multiple instances at a time, we will use an input matrix $\\mathbf{X}$ rather than a single input vector. The $i^{th}$ row will contain the transpose of the $i^{th}$ input vector $(\\mathbf{x}^{(i)})^T$. It is then possible to estimate the probability that each instance belongs to the positive class using the following equation:\n",
    "\n",
    "$ \\hat{\\mathbf{p}} = \\sigma(\\mathbf{X} \\cdot \\mathbf{\\theta})$\n",
    "\n",
    "That's all we need to build the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs + 1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n_inputs + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "logits = tf.matmul(X, theta, name=\"logits\")\n",
    "y_proba = 1 / (1 + tf.exp(-logits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, TensorFlow has a nice function `tf.sigmoid()` that we can use to simplify the last line of the previous code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_proba = tf.sigmoid(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw in chapter 4, the log loss is a good cost function to use for Logistic Regression:\n",
    "\n",
    "$J(\\mathbf{\\theta}) = -\\dfrac{1}{m} \\sum\\limits_{i=1}^{m}{\\left[ y^{(i)} log\\left(\\hat{p}^{(i)}\\right) + (1 - y^{(i)}) log\\left(1 - \\hat{p}^{(i)}\\right)\\right]}$\n",
    "\n",
    "One option is to implement it ourselves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epsilon = 1e-7  # to avoid an overflow when computing the log\n",
    "loss = -tf.reduce_mean(y * tf.log(y_proba + epsilon) + (1 - y) * tf.log(1 - y_proba + epsilon))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we might as well use TensorFlow's `tf.losses.log_loss()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = tf.losses.log_loss(y, y_proba)  # uses epsilon = 1e-7 by default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest is pretty standard: let's create the optimizer and tell it to minimize the cost function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All we need now (in this minimal version) is the variable initializer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we are ready to train the model and use it for predictions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's really nothing special about this code, it's virtually the same as the one we used earlier for Linear Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tLoss: 0.792602\n",
      "Epoch: 100 \tLoss: 0.343463\n",
      "Epoch: 200 \tLoss: 0.30754\n",
      "Epoch: 300 \tLoss: 0.292889\n",
      "Epoch: 400 \tLoss: 0.285336\n",
      "Epoch: 500 \tLoss: 0.280478\n",
      "Epoch: 600 \tLoss: 0.278083\n",
      "Epoch: 700 \tLoss: 0.276154\n",
      "Epoch: 800 \tLoss: 0.27552\n",
      "Epoch: 900 \tLoss: 0.274912\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1000\n",
    "batch_size = 50\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = random_batch(X_train, y_train, batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        loss_val = loss.eval({X: X_test, y: y_test})\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch:\", epoch, \"\\tLoss:\", loss_val)\n",
    "\n",
    "    y_proba_val = y_proba.eval(feed_dict={X: X_test, y: y_test})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: we don't use the epoch number when generating batches, so we could just have a single `for` loop rather than 2 nested `for` loops, but it's convenient to think of training time in terms of number of epochs (i.e., roughly the number of times the algorithm went through the training set)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each instance in the test set, `y_proba_val` contains the estimated probability that it belongs to the positive class, according to the model. For example, here are the first 5 estimated probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.54895616],\n",
       "       [ 0.70724374],\n",
       "       [ 0.51900256],\n",
       "       [ 0.9911136 ],\n",
       "       [ 0.50859052]], dtype=float32)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_proba_val[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To classify each instance, we can go for maximum likelihood: classify as positive any instance whose estimated probability is greater or equal to 0.5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True]], dtype=bool)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = (y_proba_val >= 0.5)\n",
    "y_pred[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the use case, you may want to choose a different threshold than 0.5: make it higher if you want high precision (but lower recall), and make it lower if you want high recall (but lower precision). See chapter 3 for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the model's precision and recall:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86274509803921573"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "precision_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88888888888888884"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot these predictions to see what they look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEFCAYAAAAIZiutAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nykEQBEHwQpSDIAiC4MX/B4BXNRO7H//+AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f96d0e11f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred_idx = y_pred.reshape(-1) # a 1D array rather than a column vector\n",
    "plt.plot(X_test[y_pred_idx, 1], X_test[y_pred_idx, 2], 'go', label=\"Positive\")\n",
    "plt.plot(X_test[~y_pred_idx, 1], X_test[~y_pred_idx, 2], 'r^', label=\"Negative\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that looks pretty bad, doesn't it? But let's not forget that the Logistic Regression model has a linear decision boundary, so this is actually close to the best we can do with this model (unless we add more features, as we will show in a second)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's start over, but this time we will add all the bells and whistles, as listed in the exercise:\n",
    "* Define the graph within a `logistic_regression()` function that can be reused easily.\n",
    "* Save checkpoints using a `Saver` at regular intervals during training, and save the final model at the end of training.\n",
    "* Restore the last checkpoint upon startup if training was interrupted.\n",
    "* Define the graph using nice scopes so the graph looks good in TensorBoard.\n",
    "* Add summaries to visualize the learning curves in TensorBoard.\n",
    "* Try tweaking some hyperparameters such as the learning rate or the mini-batch size and look at the shape of the learning curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start, we will add 4 more features to the inputs: ${x_1}^2$, ${x_2}^2$, ${x_1}^3$ and ${x_2}^3$. This was not part of the exercise, but it will demonstrate how adding features can improve the model. We will do this manually, but you could also add them using `sklearn.preprocessing.PolynomialFeatures`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_enhanced = np.c_[X_train,\n",
    "                         np.square(X_train[:, 1]),\n",
    "                         np.square(X_train[:, 2]),\n",
    "                         X_train[:, 1] ** 3,\n",
    "                         X_train[:, 2] ** 3]\n",
    "X_test_enhanced = np.c_[X_test,\n",
    "                        np.square(X_test[:, 1]),\n",
    "                        np.square(X_test[:, 2]),\n",
    "                        X_test[:, 1] ** 3,\n",
    "                        X_test[:, 2] ** 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what the \"enhanced\" training set looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.00000000e+00,  -5.14696757e-02,   4.44198631e-01,\n",
       "          2.64912752e-03,   1.97312424e-01,  -1.36349734e-04,\n",
       "          8.76459084e-02],\n",
       "       [  1.00000000e+00,   1.03201691e+00,  -4.19741157e-01,\n",
       "          1.06505890e+00,   1.76182639e-01,   1.09915879e+00,\n",
       "         -7.39511049e-02],\n",
       "       [  1.00000000e+00,   8.67891864e-01,  -2.54827114e-01,\n",
       "          7.53236288e-01,   6.49368582e-02,   6.53727646e-01,\n",
       "         -1.65476722e-02],\n",
       "       [  1.00000000e+00,   2.88850997e-01,  -4.48668621e-01,\n",
       "          8.34348982e-02,   2.01303531e-01,   2.41002535e-02,\n",
       "         -9.03185778e-02],\n",
       "       [  1.00000000e+00,  -8.33439108e-01,   5.35056649e-01,\n",
       "          6.94620746e-01,   2.86285618e-01,  -5.78924095e-01,\n",
       "          1.53179024e-01]])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_enhanced[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, next let's reset the default graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define the `logistic_regression()` function to create the graph. We will leave out the definition of the inputs `X` and the targets `y`. We could include them here, but leaving them out will make it easier to use this function in a wide range of use cases (e.g. perhaps we will want to add some preprocessing steps for the inputs before we feed them to the Logistic Regression model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_regression(X, y, initializer=None, seed=42, learning_rate=0.01):\n",
    "    n_inputs_including_bias = int(X.get_shape()[1])\n",
    "    with tf.name_scope(\"logistic_regression\"):\n",
    "        with tf.name_scope(\"model\"):\n",
    "            if initializer is None:\n",
    "                initializer = tf.random_uniform([n_inputs_including_bias, 1], -1.0, 1.0, seed=seed)\n",
    "            theta = tf.Variable(initializer, name=\"theta\")\n",
    "            logits = tf.matmul(X, theta, name=\"logits\")\n",
    "            y_proba = tf.sigmoid(logits)\n",
    "        with tf.name_scope(\"train\"):\n",
    "            loss = tf.losses.log_loss(y, y_proba, scope=\"loss\")\n",
    "            optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "            training_op = optimizer.minimize(loss)\n",
    "            loss_summary = tf.summary.scalar('log_loss', loss)\n",
    "        with tf.name_scope(\"init\"):\n",
    "            init = tf.global_variables_initializer()\n",
    "        with tf.name_scope(\"save\"):\n",
    "            saver = tf.train.Saver()\n",
    "    return y_proba, loss, training_op, loss_summary, init, saver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a little function to get the name of the log directory to save the summaries for Tensorboard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def log_dir(prefix=\"\"):\n",
    "    now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "    root_logdir = \"tf_logs\"\n",
    "    if prefix:\n",
    "        prefix += \"-\"\n",
    "    name = prefix + \"run-\" + now\n",
    "    return \"{}/{}/\".format(root_logdir, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's create the graph, using the `logistic_regression()` function. We will also create the `FileWriter` to save the summaries to the log directory for Tensorboard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_inputs = 2 + 4\n",
    "logdir = log_dir(\"logreg\")\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs + 1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "\n",
    "y_proba, loss, training_op, loss_summary, init, saver = logistic_regression(X, y)\n",
    "\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At last we can train the model! We will start by checking whether a previous training session was interrupted, and if so we will load the checkpoint and continue training from the epoch number we saved. In this example we just save the epoch number to a separate file, but in chapter 11 we will see how to store the training step directly as part of the model, using a non-trainable variable called `global_step` that we pass to the optimizer's `minimize()` method.\n",
    "\n",
    "You can try interrupting training to verify that it does indeed restore the last checkpoint when you start it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tLoss: 0.629985\n",
      "Epoch: 500 \tLoss: 0.161224\n",
      "Epoch: 1000 \tLoss: 0.119032\n",
      "Epoch: 1500 \tLoss: 0.0973292\n",
      "Epoch: 2000 \tLoss: 0.0836979\n",
      "Epoch: 2500 \tLoss: 0.0743758\n",
      "Epoch: 3000 \tLoss: 0.0675021\n",
      "Epoch: 3500 \tLoss: 0.0622069\n",
      "Epoch: 4000 \tLoss: 0.0580268\n",
      "Epoch: 4500 \tLoss: 0.054563\n",
      "Epoch: 5000 \tLoss: 0.0517083\n",
      "Epoch: 5500 \tLoss: 0.0492377\n",
      "Epoch: 6000 \tLoss: 0.0471673\n",
      "Epoch: 6500 \tLoss: 0.0453766\n",
      "Epoch: 7000 \tLoss: 0.0438187\n",
      "Epoch: 7500 \tLoss: 0.0423742\n",
      "Epoch: 8000 \tLoss: 0.0410892\n",
      "Epoch: 8500 \tLoss: 0.0399709\n",
      "Epoch: 9000 \tLoss: 0.0389202\n",
      "Epoch: 9500 \tLoss: 0.0380107\n",
      "Epoch: 10000 \tLoss: 0.0371557\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10001\n",
    "batch_size = 50\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "checkpoint_path = \"/tmp/my_logreg_model.ckpt\"\n",
    "checkpoint_epoch_path = checkpoint_path + \".epoch\"\n",
    "final_model_path = \"./my_logreg_model\"\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    if os.path.isfile(checkpoint_epoch_path):\n",
    "        # if the checkpoint file exists, restore the model and load the epoch number\n",
    "        with open(checkpoint_epoch_path, \"rb\") as f:\n",
    "            start_epoch = int(f.read())\n",
    "        print(\"Training was interrupted. Continuing at epoch\", start_epoch)\n",
    "        saver.restore(sess, checkpoint_path)\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "        sess.run(init)\n",
    "\n",
    "    for epoch in range(start_epoch, n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = random_batch(X_train_enhanced, y_train, batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        loss_val, summary_str = sess.run([loss, loss_summary], feed_dict={X: X_test_enhanced, y: y_test})\n",
    "        file_writer.add_summary(summary_str, epoch)\n",
    "        if epoch % 500 == 0:\n",
    "            print(\"Epoch:\", epoch, \"\\tLoss:\", loss_val)\n",
    "            saver.save(sess, checkpoint_path)\n",
    "            with open(checkpoint_epoch_path, \"wb\") as f:\n",
    "                f.write(b\"%d\" % (epoch + 1))\n",
    "\n",
    "    saver.save(sess, final_model_path)\n",
    "    y_proba_val = y_proba.eval(feed_dict={X: X_test_enhanced, y: y_test})\n",
    "    os.remove(checkpoint_epoch_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we can make predictions by just classifying as positive all the instances whose estimated probability is greater or equal to 0.5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = (y_proba_val >= 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97979797979797978"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97979797979797978"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEFCAYAAAAIZiutAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f96d29e95f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred_idx = y_pred.reshape(-1) # a 1D array rather than a column vector\n",
    "plt.plot(X_test[y_pred_idx, 1], X_test[y_pred_idx, 2], 'go', label=\"Positive\")\n",
    "plt.plot(X_test[~y_pred_idx, 1], X_test[~y_pred_idx, 2], 'r^', label=\"Negative\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that's much, much better! Apparently the new features really helped a lot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try starting the tensorboard server, find the latest run and look at the learning curve (i.e., how the loss evaluated on the test set evolves as a function of the epoch number):\n",
    "\n",
    "```\n",
    "$ tensorboard --logdir=tf_logs\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can play around with the hyperparameters (e.g. the `batch_size` or the `learning_rate`) and run training again and again, comparing the learning curves. You can even automate this process by implementing grid search or randomized search. Below is a simple implementation of a randomized search on both the batch size and the learning rate. For the sake of simplicity, the checkpoint mechanism was removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "  logdir: tf_logs/logreg-run-20170606195328/\n",
      "  batch size: 19\n",
      "  learning_rate: 0.00443037524522\n",
      "  training: .....................\n",
      "  precision: 0.979797979798\n",
      "  recall: 0.979797979798\n",
      "Iteration 1\n",
      "  logdir: tf_logs/logreg-run-20170606195605/\n",
      "  batch size: 80\n",
      "  learning_rate: 0.00178264971514\n",
      "  training: .....................\n",
      "  precision: 0.969696969697\n",
      "  recall: 0.969696969697\n",
      "Iteration 2\n",
      "  logdir: tf_logs/logreg-run-20170606195646/\n",
      "  batch size: 73\n",
      "  learning_rate: 0.00203228544324\n",
      "  training: .....................\n",
      "  precision: 0.969696969697\n",
      "  recall: 0.969696969697\n",
      "Iteration 3\n",
      "  logdir: tf_logs/logreg-run-20170606195730/\n",
      "  batch size: 6\n",
      "  learning_rate: 0.00449152382514\n",
      "  training: .....................\n",
      "  precision: 0.980198019802\n",
      "  recall: 1.0\n",
      "Iteration 4\n",
      "  logdir: tf_logs/logreg-run-20170606200523/\n",
      "  batch size: 24\n",
      "  learning_rate: 0.0796323472178\n",
      "  training: .....................\n",
      "  precision: 0.980198019802\n",
      "  recall: 1.0\n",
      "Iteration 5\n",
      "  logdir: tf_logs/logreg-run-20170606200726/\n",
      "  batch size: 75\n",
      "  learning_rate: 0.000463425058329\n",
      "  training: .....................\n",
      "  precision: 0.912621359223\n",
      "  recall: 0.949494949495\n",
      "Iteration 6\n",
      "  logdir: tf_logs/logreg-run-20170606200810/\n",
      "  batch size: 86\n",
      "  learning_rate: 0.0477068184194\n",
      "  training: .....................\n",
      "  precision: 0.98\n",
      "  recall: 0.989898989899\n",
      "Iteration 7\n",
      "  logdir: tf_logs/logreg-run-20170606200851/\n",
      "  batch size: 87\n",
      "  learning_rate: 0.000169404470952\n",
      "  training: .....................\n",
      "  precision: 0.888888888889\n",
      "  recall: 0.808080808081\n",
      "Iteration 8\n",
      "  logdir: tf_logs/logreg-run-20170606200932/\n",
      "  batch size: 61\n",
      "  learning_rate: 0.0417146119941\n",
      "  training: .....................\n",
      "  precision: 0.980198019802\n",
      "  recall: 1.0\n",
      "Iteration 9\n",
      "  logdir: tf_logs/logreg-run-20170606201026/\n",
      "  batch size: 92\n",
      "  learning_rate: 0.000107429229684\n",
      "  training: .....................\n",
      "  precision: 0.882352941176\n",
      "  recall: 0.757575757576\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import reciprocal\n",
    "\n",
    "n_search_iterations = 10\n",
    "\n",
    "for search_iteration in range(n_search_iterations):\n",
    "    batch_size = np.random.randint(1, 100)\n",
    "    learning_rate = reciprocal(0.0001, 0.1).rvs(random_state=search_iteration)\n",
    "\n",
    "    n_inputs = 2 + 4\n",
    "    logdir = log_dir(\"logreg\")\n",
    "    \n",
    "    print(\"Iteration\", search_iteration)\n",
    "    print(\"  logdir:\", logdir)\n",
    "    print(\"  batch size:\", batch_size)\n",
    "    print(\"  learning_rate:\", learning_rate)\n",
    "    print(\"  training: \", end=\"\")\n",
    "\n",
    "    reset_graph()\n",
    "\n",
    "    X = tf.placeholder(tf.float32, shape=(None, n_inputs + 1), name=\"X\")\n",
    "    y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "\n",
    "    y_proba, loss, training_op, loss_summary, init, saver = logistic_regression(\n",
    "        X, y, learning_rate=learning_rate)\n",
    "\n",
    "    file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "\n",
    "    n_epochs = 10001\n",
    "    n_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "    final_model_path = \"./my_logreg_model_%d\" % search_iteration\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            for batch_index in range(n_batches):\n",
    "                X_batch, y_batch = random_batch(X_train_enhanced, y_train, batch_size)\n",
    "                sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            loss_val, summary_str = sess.run([loss, loss_summary], feed_dict={X: X_test_enhanced, y: y_test})\n",
    "            file_writer.add_summary(summary_str, epoch)\n",
    "            if epoch % 500 == 0:\n",
    "                print(\".\", end=\"\")\n",
    "\n",
    "        saver.save(sess, final_model_path)\n",
    "\n",
    "        print()\n",
    "        y_proba_val = y_proba.eval(feed_dict={X: X_test_enhanced, y: y_test})\n",
    "        y_pred = (y_proba_val >= 0.5)\n",
    "        \n",
    "        print(\"  precision:\", precision_score(y_test, y_pred))\n",
    "        print(\"  recall:\", recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `reciprocal()` function from SciPy's `stats` module returns a random distribution that is commonly used when you have no idea of the optimal scale of a hyperparameter. See the exercise solutions for chapter 2 for more details. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "nav_menu": {
   "height": "603px",
   "width": "616px"
  },
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
