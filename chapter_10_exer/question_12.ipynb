{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T00:28:02.461006Z",
     "start_time": "2018-01-15T00:28:01.634212Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T00:28:07.198788Z",
     "start_time": "2018-01-15T00:28:07.193469Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T00:28:08.158006Z",
     "start_time": "2018-01-15T00:28:08.120471Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 1, 0, 0],\n",
       "       [0, 0, 1, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       ..., \n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = mnist.train.images\n",
    "X_test = mnist.test.images\n",
    "y_train = mnist.train.labels.astype(\"int\")\n",
    "y_test = mnist.test.labels.astype(\"int\")\n",
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now Start to construct model v1\n",
    "\n",
    "comment: too slow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define some common attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T01:10:13.151867Z",
     "start_time": "2018-01-15T01:10:13.143057Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 300\n",
    "batch_size = 100\n",
    "\n",
    "n_input = 28*28\n",
    "n_layer1 = 400\n",
    "n_layer2 = 200\n",
    "n_output = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The planning phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T01:10:13.609742Z",
     "start_time": "2018-01-15T01:10:13.604020Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T01:10:13.841618Z",
     "start_time": "2018-01-15T01:10:13.833336Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float64, [None, n_input], \"X\")\n",
    "y = tf.placeholder(tf.float64, [None, n_output], \"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T01:10:14.131209Z",
     "start_time": "2018-01-15T01:10:14.084476Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"ann\"):\n",
    "    hidden1 = tf.layers.dense(X, n_layer1, name=\"hidden1\", activation=tf.nn.relu)\n",
    "    hidden2 = tf.layers.dense(hidden1, n_layer2, name=\"hidden2\", activation=tf.nn.relu)\n",
    "    output = tf.layers.dense(hidden2, n_output, name=\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T01:10:14.395297Z",
     "start_time": "2018-01-15T01:10:14.365132Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=output, name=\"xentropy\")\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T01:10:14.922629Z",
     "start_time": "2018-01-15T01:10:14.758208Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "with tf.name_scope(\"train\"):\n",
    "    adam_optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    training_op = adam_optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T01:10:15.559745Z",
     "start_time": "2018-01-15T01:10:15.181187Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    # a metrix with entries 1 if logits predict the same as class y\n",
    "    arg_y = tf.argmax(y, axis=1)\n",
    "    arg_y_pred = tf.argmax(output, axis=1)\n",
    "    acc_all, acc = tf.metrics.accuracy(arg_y, arg_y_pred, name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T01:10:15.735540Z",
     "start_time": "2018-01-15T01:10:15.637005Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "time_now = datetime.now().strftime(\"%Y-%m-%d=%H:%M:%S\")\n",
    "\n",
    "with tf.name_scope(\"log\"):\n",
    "    file_writer = tf.summary.FileWriter(\"logs/{}\".format(time_now), tf.get_default_graph())\n",
    "    acc_log = tf.summary.scalar(\"accuracy\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T01:10:16.034330Z",
     "start_time": "2018-01-15T01:10:15.970490Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saving_path = \"temp/save/{}\".format(time_now)\n",
    "\n",
    "if not os.path.isdir(saving_path):\n",
    "    os.makedirs(saving_path)\n",
    "\n",
    "with tf.name_scope(\"saver\"):\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T01:10:16.587493Z",
     "start_time": "2018-01-15T01:10:16.580559Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"initializer\"):\n",
    "    initializers = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T01:22:42.670738Z",
     "start_time": "2018-01-15T01:10:17.441835Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train acc: 0.98 Val acc: 0.956471\n",
      "0 loss= 0.0608635468541\n",
      "20 Train acc: 0.957115 Val acc: 0.96549\n",
      "20 loss= 0.0480413646043\n",
      "40 Train acc: 0.965825 Val acc: 0.968562\n",
      "40 loss= 0.000158997125522\n",
      "60 Train acc: 0.968766 Val acc: 0.969608\n",
      "60 loss= 0.00306137385281\n",
      "80 Train acc: 0.969707 Val acc: 0.97051\n",
      "80 loss= 0.0212749368892\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-141-ddf5f55108f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m             \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/ml_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/ml_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/ml_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/ml_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/ml_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    initializers.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(X_train.shape[0] // batch_size + 1):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            \n",
    "            \n",
    "        if not (epoch%20):\n",
    "    ##### !!!!!!! Dont evaluate acc each batch!!!! BUg!!!!!!!!!!!!!\n",
    "            acc_train = acc.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "            acc_val, str_acc = sess.run([acc, acc_log], feed_dict={X: mnist.validation.images,\n",
    "                                                                                 y: mnist.validation.labels})\n",
    "            file_writer.add_summary(str_acc, epoch)\n",
    "            saver.save(sess, \"{}/{}.ckpt\".format(saving_path, epoch//20))\n",
    "            print(epoch, \"Train acc:\", acc_train, \"Val acc:\", acc_val)\n",
    "            print(epoch, \"loss=\", loss.eval(feed_dict={X:X_batch, y:y_batch}))\n",
    "\n",
    "    # Can use this with global to restore from the most recent run\n",
    "    save_path = saver.save(sess, \"{}/final.ckpt\".format(saving_path))\n",
    "    file_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now Start to construct model v2\n",
    "\n",
    "comment: bad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define some common attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T00:37:39.447336Z",
     "start_time": "2018-01-15T00:37:39.440993Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 300\n",
    "batch_size = 100\n",
    "\n",
    "n_input = 28*28\n",
    "n_layer1 = 300\n",
    "n_layer2 = 100\n",
    "n_layer3 = 33\n",
    "n_layer4 = 15\n",
    "n_output = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The planning phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T00:37:40.253985Z",
     "start_time": "2018-01-15T00:37:40.249318Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T00:37:40.695992Z",
     "start_time": "2018-01-15T00:37:40.688732Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float64, [None, n_input], \"X\")\n",
    "y = tf.placeholder(tf.float64, [None, n_output], \"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T00:37:41.226492Z",
     "start_time": "2018-01-15T00:37:41.161005Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"ann\"):\n",
    "    hidden1 = tf.layers.dense(X, n_layer1, name=\"hidden1\", activation=tf.nn.relu)\n",
    "    hidden2 = tf.layers.dense(hidden1, n_layer2, name=\"hidden2\", activation=tf.nn.relu)\n",
    "    hidden3 = tf.layers.dense(hidden2, n_layer3, name=\"hidden3\", activation=tf.nn.relu)\n",
    "    hidden4 = tf.layers.dense(hidden3, n_layer4, name=\"hidden4\", activation=tf.nn.relu)\n",
    "    output = tf.layers.dense(hidden4, n_output, name=\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T00:37:41.877019Z",
     "start_time": "2018-01-15T00:37:41.849378Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=output, name=\"xentropy\")\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T00:37:42.601085Z",
     "start_time": "2018-01-15T00:37:42.412078Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "with tf.name_scope(\"train\"):\n",
    "    adam_optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    training_op = adam_optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T00:37:42.918627Z",
     "start_time": "2018-01-15T00:37:42.891996Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    # a metrix with entries 1 if logits predict the same as class y\n",
    "    arg_y = tf.argmax(y, axis=1)\n",
    "    arg_y_pred = tf.argmax(output, axis=1)\n",
    "    acc_all, acc = tf.metrics.accuracy(arg_y, arg_y_pred, name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T00:37:43.296082Z",
     "start_time": "2018-01-15T00:37:43.205020Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "time_now = datetime.now().strftime(\"%Y-%m-%d=%H:%M:%S\")\n",
    "\n",
    "with tf.name_scope(\"log\"):\n",
    "    file_writer = tf.summary.FileWriter(\"logs/{}\".format(time_now), tf.get_default_graph())\n",
    "    acc_log = tf.summary.scalar(\"accuracy\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T00:37:43.654232Z",
     "start_time": "2018-01-15T00:37:43.515655Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saving_path = \"temp/save/{}\".format(time_now)\n",
    "\n",
    "if not os.path.isdir(saving_path):\n",
    "    os.makedirs(saving_path)\n",
    "\n",
    "with tf.name_scope(\"saver\"):\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T00:37:43.926092Z",
     "start_time": "2018-01-15T00:37:43.916965Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"initializer\"):\n",
    "    initializers = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T01:06:59.224540Z",
     "start_time": "2018-01-15T00:37:44.662881Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train acc: 0.96 Val acc: 0.960588\n",
      "0 loss= 0.106491546\n",
      "20 Train acc: 0.961346 Val acc: 0.968333\n",
      "20 loss= 0.000938012674273\n",
      "40 Train acc: 0.968641 Val acc: 0.969739\n",
      "40 loss= 0.000336676375869\n",
      "60 Train acc: 0.969935 Val acc: 0.970441\n",
      "60 loss= 0.00376661113818\n",
      "80 Train acc: 0.970488 Val acc: 0.970078\n",
      "80 loss= 0.0501652779507\n",
      "100 Train acc: 0.970039 Val acc: 0.968235\n",
      "100 loss= 0.163446326232\n",
      "120 Train acc: 0.967948 Val acc: 0.951905\n",
      "120 loss= 0.34471344596\n",
      "140 Train acc: 0.950754 Val acc: 0.906985\n",
      "140 loss= 0.904730425202\n",
      "160 Train acc: 0.905501 Val acc: 0.839368\n",
      "160 loss= 1.46132619583\n",
      "180 Train acc: 0.837891 Val acc: 0.776412\n",
      "180 loss= 1.83425543922\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-130-ddf5f55108f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m             \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/ml_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/ml_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/ml_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/ml_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/ml_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    initializers.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(X_train.shape[0] // batch_size + 1):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            \n",
    "            \n",
    "        if not (epoch%20):\n",
    "    ##### !!!!!!! Dont evaluate acc each batch!!!! BUg!!!!!!!!!!!!!\n",
    "            acc_train = acc.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "            acc_val, str_acc = sess.run([acc, acc_log], feed_dict={X: mnist.validation.images,\n",
    "                                                                                 y: mnist.validation.labels})\n",
    "            file_writer.add_summary(str_acc, epoch)\n",
    "            recent_path = \"{}/{}.ckpt\".format(saving_path, epoch//20)\n",
    "            global recent_path\n",
    "            saver.save(sess, recent_path)\n",
    "            print(epoch, \"Train acc:\", acc_train, \"Val acc:\", acc_val)\n",
    "            print(epoch, \"loss=\", loss.eval(feed_dict={X:X_batch, y:y_batch}))\n",
    "\n",
    "    # Can use this with global to restore from the most recent run\n",
    "    save_path = saver.save(sess, \"{}/final.ckpt\".format(saving_path))\n",
    "    file_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FUcking overfitting lahsdlfakn dkf1asfd @#$ Q@$T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now Start to construct model v3\n",
    "\n",
    "comment: use 2 layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define some common attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T13:56:03.610621Z",
     "start_time": "2018-01-15T13:56:03.605194Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 300\n",
    "batch_size = 100\n",
    "\n",
    "n_input = 28*28\n",
    "n_layer1 = 300\n",
    "n_layer2 = 100\n",
    "n_output = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The planning phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T13:56:04.009072Z",
     "start_time": "2018-01-15T13:56:04.003441Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T13:56:07.261884Z",
     "start_time": "2018-01-15T13:56:07.234381Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float64, [None, n_input], \"X\")\n",
    "y = tf.placeholder(tf.float64, [None, n_output], \"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T13:56:08.039601Z",
     "start_time": "2018-01-15T13:56:07.930891Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"ann\"):\n",
    "    hidden1 = tf.layers.dense(X, n_layer1, name=\"hidden1\", activation=tf.nn.relu)\n",
    "    hidden2 = tf.layers.dense(hidden1, n_layer2, name=\"hidden2\", activation=tf.nn.relu)\n",
    "    output = tf.layers.dense(hidden2, n_output, name=\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T13:56:08.906058Z",
     "start_time": "2018-01-15T13:56:08.861672Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=output, name=\"xentropy\")\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T13:56:17.328210Z",
     "start_time": "2018-01-15T13:56:17.152981Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "with tf.name_scope(\"train\"):\n",
    "    adam_optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    training_op = adam_optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T13:56:18.060940Z",
     "start_time": "2018-01-15T13:56:18.020623Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    # a metrix with entries 1 if logits predict the same as class y\n",
    "    arg_y = tf.argmax(y, axis=1)\n",
    "    arg_y_pred = tf.argmax(output, axis=1)\n",
    "    acc_all, acc = tf.metrics.accuracy(arg_y, arg_y_pred, name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T13:56:18.639540Z",
     "start_time": "2018-01-15T13:56:18.530618Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "time_now = datetime.now().strftime(\"%Y-%m-%d=%H:%M:%S\")\n",
    "\n",
    "with tf.name_scope(\"log\"):\n",
    "    file_writer = tf.summary.FileWriter(\"logs/{}\".format(time_now), tf.get_default_graph())\n",
    "    acc_log = tf.summary.scalar(\"accuracy\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T13:56:19.164902Z",
     "start_time": "2018-01-15T13:56:19.095296Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saving_path = \"temp/save/{}\".format(time_now)\n",
    "\n",
    "if not os.path.isdir(saving_path):\n",
    "    os.makedirs(saving_path)\n",
    "\n",
    "with tf.name_scope(\"saver\"):\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T13:56:19.677007Z",
     "start_time": "2018-01-15T13:56:19.667997Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"initializer\"):\n",
    "    initializers = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T14:12:27.826385Z",
     "start_time": "2018-01-15T13:56:43.047694Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train acc: 0.95 Val acc: 0.959412\n",
      "0 loss= 0.166441789127\n",
      "20 Train acc: 0.960192 Val acc: 0.967451\n",
      "20 loss= 0.0101078848715\n",
      "40 Train acc: 0.967767 Val acc: 0.968889\n",
      "40 loss= 0.00213528188394\n",
      "60 Train acc: 0.969091 Val acc: 0.969804\n",
      "60 loss= 6.04461299629e-05\n",
      "80 Train acc: 0.969805 Val acc: 0.969922\n",
      "80 loss= 0.0876250391634\n",
      "100 Train acc: 0.970039 Val acc: 0.971405\n",
      "100 loss= 4.520523842e-08\n",
      "120 Train acc: 0.971466 Val acc: 0.971569\n",
      "120 loss= 0.0265208022615\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-156-13a4e18ce241>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m             \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/ml_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/ml_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/ml_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/ml_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/ml_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    initializers.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(X_train.shape[0] // batch_size + 1):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            \n",
    "            \n",
    "        if not (epoch%20):\n",
    "    ##### !!!!!!! Dont evaluate acc each batch!!!! BUg!!!!!!!!!!!!!\n",
    "            acc_train = acc.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "            acc_val, str_acc = sess.run([acc, acc_log], feed_dict={X: mnist.validation.images,\n",
    "                                                                     y: mnist.validation.labels})\n",
    "            file_writer.add_summary(str_acc, epoch)\n",
    "            save_path = saver.save(sess, \"{}/{}.ckpt\".format(saving_path, epoch//20))\n",
    "            print(epoch, \"Train acc:\", acc_train, \"Val acc:\", acc_val)\n",
    "            print(epoch, \"loss=\", loss.eval(feed_dict={X:X_batch, y:y_batch}))\n",
    "\n",
    "    # Can use this with global to restore from the most recent run\n",
    "    save_path = saver.save(sess, \"{}/final.ckpt\".format(saving_path))\n",
    "    file_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a test section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T14:12:58.552199Z",
     "start_time": "2018-01-15T14:12:57.916367Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from temp/save/2018-01-16=00:56:18/6.ckpt\n",
      "The accuracy for test set is: 0.9715\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    initializers.run()\n",
    "    saver.restore(sess, save_path)\n",
    "    acc_test = acc.eval(feed_dict={X:X_test, y:y_test})\n",
    "    print(\"The accuracy for test set is: {:<.4f}\".format(acc_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now Start to construct model v4\n",
    "\n",
    "comment: Adam is bad, change to normal GD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define some common attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T14:15:58.786305Z",
     "start_time": "2018-01-15T14:15:58.779258Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 300\n",
    "batch_size = 100\n",
    "\n",
    "n_input = 28*28\n",
    "n_layer1 = 300\n",
    "n_layer2 = 100\n",
    "n_output = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The planning phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T14:52:29.818788Z",
     "start_time": "2018-01-15T14:52:29.815243Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T14:52:30.026044Z",
     "start_time": "2018-01-15T14:52:30.017474Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float64, [None, n_input], \"X\")\n",
    "y = tf.placeholder(tf.float64, [None, n_output], \"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T14:52:30.298484Z",
     "start_time": "2018-01-15T14:52:30.223769Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"ann\"):\n",
    "    hidden1 = tf.layers.dense(X, n_layer1, name=\"hidden1\", activation=tf.nn.relu)\n",
    "    hidden2 = tf.layers.dense(hidden1, n_layer2, name=\"hidden2\", activation=tf.nn.relu)\n",
    "    output = tf.layers.dense(hidden2, n_output, name=\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T14:52:30.409261Z",
     "start_time": "2018-01-15T14:52:30.372685Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=output, name=\"xentropy\")\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T14:52:30.470248Z",
     "start_time": "2018-01-15T14:52:30.412034Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.05\n",
    "with tf.name_scope(\"train\"):\n",
    "    gradient_optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = gradient_optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T14:52:30.690413Z",
     "start_time": "2018-01-15T14:52:30.654277Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    # a metrix with entries 1 if logits predict the same as class y\n",
    "    arg_y = tf.argmax(y, axis=1)\n",
    "    arg_y_pred = tf.argmax(output, axis=1)\n",
    "    acc_all, acc = tf.metrics.accuracy(arg_y, arg_y_pred, name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T14:52:30.782252Z",
     "start_time": "2018-01-15T14:52:30.735858Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "time_now = datetime.now().strftime(\"%Y-%m-%d=%H:%M:%S\")\n",
    "\n",
    "with tf.name_scope(\"log\"):\n",
    "    file_writer = tf.summary.FileWriter(\"logs/{}\".format(time_now), tf.get_default_graph())\n",
    "    acc_log = tf.summary.scalar(\"accuracy\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T14:52:30.935746Z",
     "start_time": "2018-01-15T14:52:30.909134Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saving_path = \"temp/save/{}\".format(time_now)\n",
    "\n",
    "if not os.path.isdir(saving_path):\n",
    "    os.makedirs(saving_path)\n",
    "\n",
    "with tf.name_scope(\"saver\"):\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T14:52:32.234150Z",
     "start_time": "2018-01-15T14:52:32.227080Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"initializer\"):\n",
    "    initializers = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T15:26:48.627029Z",
     "start_time": "2018-01-15T14:54:57.741859Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train acc: 0.93 Val acc: 0.92902\n",
      "0 loss= 0.268428471313\n",
      "5 Train acc: 0.930192 Val acc: 0.948529\n",
      "5 loss= 0.103769925192\n",
      "10 Train acc: 0.949029 Val acc: 0.95732\n",
      "10 loss= 0.0343264129421\n",
      "15 Train acc: 0.957597 Val acc: 0.962598\n",
      "15 loss= 0.0264392795545\n",
      "20 Train acc: 0.96278 Val acc: 0.966471\n",
      "20 loss= 0.00751361838445\n",
      "25 Train acc: 0.966602 Val acc: 0.969118\n",
      "25 loss= 0.0036491574307\n",
      "30 Train acc: 0.969218 Val acc: 0.97112\n",
      "30 loss= 0.00539456683715\n",
      "35 Train acc: 0.971201 Val acc: 0.972623\n",
      "35 loss= 0.00471257621901\n",
      "40 Train acc: 0.97269 Val acc: 0.973813\n",
      "40 loss= 0.00275044985103\n",
      "45 Train acc: 0.97387 Val acc: 0.974765\n",
      "45 loss= 0.00213583883691\n",
      "50 Train acc: 0.974814 Val acc: 0.975615\n",
      "50 loss= 0.00129352378819\n",
      "55 Train acc: 0.975658 Val acc: 0.976274\n",
      "55 loss= 0.00221665690016\n",
      "60 Train acc: 0.976313 Val acc: 0.976863\n",
      "60 loss= 0.00254346590089\n",
      "65 Train acc: 0.976898 Val acc: 0.977339\n",
      "65 loss= 0.000419140842244\n",
      "70 Train acc: 0.977371 Val acc: 0.977712\n",
      "70 loss= 0.00058465872688\n",
      "75 Train acc: 0.977742 Val acc: 0.9781\n",
      "75 loss= 0.000837970685476\n",
      "80 Train acc: 0.978127 Val acc: 0.978408\n",
      "80 loss= 0.00125395861539\n",
      "85 Train acc: 0.978433 Val acc: 0.978715\n",
      "85 loss= 0.000485175844805\n",
      "90 Train acc: 0.978738 Val acc: 0.978968\n",
      "90 loss= 0.000999375018922\n",
      "95 Train acc: 0.97899 Val acc: 0.979206\n",
      "95 loss= 0.00233230909813\n",
      "100 Train acc: 0.979226 Val acc: 0.979412\n",
      "100 loss= 0.00046690991365\n",
      "105 Train acc: 0.979431 Val acc: 0.979608\n",
      "105 loss= 0.000392501448671\n",
      "110 Train acc: 0.979626 Val acc: 0.979795\n",
      "110 loss= 0.000841702108087\n",
      "115 Train acc: 0.979813 Val acc: 0.979943\n",
      "115 loss= 0.000312035604344\n",
      "120 Train acc: 0.979959 Val acc: 0.980094\n",
      "120 loss= 0.00106502533121\n",
      "125 Train acc: 0.98011 Val acc: 0.980241\n",
      "125 loss= 0.000480340007549\n",
      "130 Train acc: 0.980256 Val acc: 0.980356\n",
      "130 loss= 0.00101647315706\n",
      "135 Train acc: 0.98037 Val acc: 0.980469\n",
      "135 loss= 0.00058883032004\n",
      "140 Train acc: 0.980483 Val acc: 0.980575\n",
      "140 loss= 0.000351147404775\n",
      "145 Train acc: 0.980588 Val acc: 0.98068\n",
      "145 loss= 0.000585940977592\n",
      "150 Train acc: 0.980692 Val acc: 0.980772\n",
      "150 loss= 0.000376214107519\n",
      "155 Train acc: 0.980784 Val acc: 0.980864\n",
      "155 loss= 0.000602250297823\n",
      "160 Train acc: 0.980876 Val acc: 0.980945\n",
      "160 loss= 0.000515127907773\n",
      "165 Train acc: 0.980956 Val acc: 0.981027\n",
      "165 loss= 0.000394265628831\n",
      "170 Train acc: 0.981037 Val acc: 0.981098\n",
      "170 loss= 0.000664133790089\n",
      "175 Train acc: 0.981109 Val acc: 0.981171\n",
      "175 loss= 0.000331831810282\n",
      "180 Train acc: 0.981181 Val acc: 0.98124\n",
      "180 loss= 0.000312863699386\n",
      "185 Train acc: 0.98125 Val acc: 0.9813\n",
      "185 loss= 0.0004405044316\n",
      "190 Train acc: 0.98131 Val acc: 0.981363\n",
      "190 loss= 0.000142299107171\n",
      "195 Train acc: 0.981372 Val acc: 0.981417\n",
      "195 loss= 0.000301269883716\n",
      "200 Train acc: 0.981426 Val acc: 0.981468\n",
      "200 loss= 0.000150730506014\n",
      "205 Train acc: 0.981477 Val acc: 0.981517\n",
      "205 loss= 0.00022182024824\n",
      "210 Train acc: 0.981526 Val acc: 0.981569\n",
      "210 loss= 0.000682942932327\n",
      "215 Train acc: 0.981577 Val acc: 0.981618\n",
      "215 loss= 0.000440931570742\n",
      "220 Train acc: 0.981626 Val acc: 0.981664\n",
      "220 loss= 0.000206746538482\n",
      "225 Train acc: 0.981672 Val acc: 0.981705\n",
      "225 loss= 0.000301431159968\n",
      "230 Train acc: 0.981713 Val acc: 0.981748\n",
      "230 loss= 0.000258659126801\n",
      "235 Train acc: 0.981756 Val acc: 0.981781\n",
      "235 loss= 0.000249577477301\n",
      "240 Train acc: 0.981788 Val acc: 0.981817\n",
      "240 loss= 0.000290899461225\n",
      "245 Train acc: 0.981824 Val acc: 0.981847\n",
      "245 loss= 0.000275077171205\n",
      "250 Train acc: 0.981854 Val acc: 0.981884\n",
      "250 loss= 0.000262046449402\n",
      "255 Train acc: 0.981891 Val acc: 0.981919\n",
      "255 loss= 0.000197590105566\n",
      "260 Train acc: 0.981926 Val acc: 0.98195\n",
      "260 loss= 0.00053801258853\n",
      "265 Train acc: 0.981956 Val acc: 0.981979\n",
      "265 loss= 0.000185541450875\n",
      "270 Train acc: 0.981986 Val acc: 0.982007\n",
      "270 loss= 0.000246694529795\n",
      "275 Train acc: 0.982014 Val acc: 0.982031\n",
      "275 loss= 5.68878041027e-05\n",
      "280 Train acc: 0.982037 Val acc: 0.982054\n",
      "280 loss= 0.00023439453143\n",
      "285 Train acc: 0.98206 Val acc: 0.982076\n",
      "285 loss= 0.000308871974469\n",
      "290 Train acc: 0.982082 Val acc: 0.9821\n",
      "290 loss= 0.000186670372015\n",
      "295 Train acc: 0.982106 Val acc: 0.982124\n",
      "295 loss= 0.00026176269822\n"
     ]
    }
   ],
   "source": [
    "max_rep = 8\n",
    "max_val_acc = 0\n",
    "best_save_path = None\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    initializers.run()\n",
    "    rep = 0\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(X_train.shape[0] // batch_size + 1):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "\n",
    "            \n",
    "        if not (epoch%5):\n",
    "    ##### !!!!!!! Dont evaluate acc each batch!!!! BUg!!!!!!!!!!!!!\n",
    "            acc_train = acc.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "            acc_val, str_acc = sess.run([acc, acc_log], feed_dict={X: mnist.validation.images,\n",
    "                                                                     y: mnist.validation.labels})\n",
    "\n",
    "                \n",
    "            file_writer.add_summary(str_acc, epoch)\n",
    "            save_path = saver.save(sess, \"{}/{}.ckpt\".format(saving_path, epoch//20))\n",
    "            \n",
    "            \n",
    "            if acc_val > max_val_acc:\n",
    "                best_save_path = save_path\n",
    "                min_val_acc = acc_val\n",
    "                rep = 0\n",
    "            else:\n",
    "                print(\"incrementing rep\")\n",
    "                rep += 1\n",
    "            \n",
    "            print(epoch, \"Train acc:\", acc_train, \"Val acc:\", acc_val)\n",
    "            print(epoch, \"loss=\", loss.eval(feed_dict={X:X_batch, y:y_batch}))\n",
    "            \n",
    "            if rep > max_rep:\n",
    "                print(\"Overfitting: early terminating\")\n",
    "                break\n",
    "\n",
    "    # Can use this with global to restore from the most recent run\n",
    "    save_path = saver.save(sess, \"{}/final.ckpt\".format(saving_path))\n",
    "    file_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a test section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T15:30:59.218432Z",
     "start_time": "2018-01-15T15:30:58.736201Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from temp/save/2018-01-16=01:52:30/14.ckpt\n",
      "The accuracy for test set is: 0.9812\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    initializers.run()\n",
    "    if best_save_path:\n",
    "        saver.restore(sess, best_save_path)\n",
    "    else:\n",
    "        saver.restore(sess, save_path)\n",
    "    acc_test = acc.eval(feed_dict={X:X_test, y:y_test})\n",
    "    print(\"The accuracy for test set is: {:<.4f}\".format(acc_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, the accuracy is pretty good this time around"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
