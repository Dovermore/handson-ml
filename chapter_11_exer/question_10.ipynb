{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Question 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Basic Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T06:20:09.512265Z",
     "start_time": "2018-02-01T06:19:54.326038Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/ml_env/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from time import time, clock\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# a.Building neuro net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## The neuro networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Setting Constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T06:20:09.556016Z",
     "start_time": "2018-02-01T06:20:09.524197Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "n_input_AB = 28 * 28\n",
    "n_hidden_AB = 100\n",
    "n_hidden = 20\n",
    "n_outputs_AB = 100\n",
    "n_outputs = 2\n",
    "n_layer_AB = 3\n",
    "kernel_regularizer = None\n",
    "\n",
    "learning_rate = 0.005\n",
    "\n",
    "base_dir = os.path.join(\".\", \"temp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Set The graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T06:20:09.609409Z",
     "start_time": "2018-02-01T06:20:09.574556Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making new graph\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    del graph\n",
    "    print(\"Deleting old graph\")\n",
    "except:\n",
    "    pass\n",
    "print(\"Making new graph\")\n",
    "graph = tf.Graph()\n",
    "\n",
    "time_str = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "log_dir = os.path.join(base_dir, \"logs\", time_str)\n",
    "save_dir = os.path.join(base_dir, \"saves\", time_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Network A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T06:20:09.848055Z",
     "start_time": "2018-02-01T06:20:09.628810Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    \n",
    "    with tf.name_scope(\"inputs_A\"):\n",
    "        X_A = tf.placeholder(\n",
    "                            tf.float32,\n",
    "                            shape=[None, n_input_AB],\n",
    "                            name=\"X_A\"\n",
    "                            )\n",
    "        tf.add_to_collection(\"feed_vars_A\", X_A)\n",
    "\n",
    "        training_A = tf.placeholder_with_default(\n",
    "                                                False,\n",
    "                                                [],\n",
    "                                                name=\"training_A\"\n",
    "                                            )\n",
    "        tf.add_to_collection(\"feed_vars_A\", training_A)\n",
    "\n",
    "    \n",
    "    inputs_A = X_A\n",
    "    \n",
    "    with tf.name_scope(\"dnn_A\"):\n",
    "        for i in range(1, n_layer_AB+1):\n",
    "\n",
    "            inputs_A = tf.layers.dense(\n",
    "                                    inputs_A,\n",
    "                                    n_hidden_AB,\n",
    "                                    activation=tf.nn.selu,\n",
    "                                    use_bias=True,\n",
    "                                    kernel_initializer=kernel_regularizer,\n",
    "                                    bias_initializer=tf.zeros_initializer(),\n",
    "                                    kernel_regularizer=None,\n",
    "                                    bias_regularizer=None,\n",
    "                                    activity_regularizer=None,\n",
    "                                    kernel_constraint=None,\n",
    "                                    bias_constraint=None,\n",
    "                                    trainable=True,\n",
    "                                    name=\"hidden{}_A\".format(i),\n",
    "                                    reuse=None\n",
    "                                )\n",
    "            tf.add_to_collection(\"hidden_A\", inputs_A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Network B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T06:20:10.038518Z",
     "start_time": "2018-02-01T06:20:09.866131Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    with tf.name_scope(\"inputs_B\"):\n",
    "        X_B = tf.placeholder(\n",
    "                            tf.float32,\n",
    "                            shape=[None, n_input_AB],\n",
    "                            name=\"X_B\"\n",
    "                            )\n",
    "        tf.add_to_collection(\"feed_vars_B\", X_B)\n",
    "\n",
    "        training_B = tf.placeholder_with_default(\n",
    "                                                False,\n",
    "                                                [],\n",
    "                                                name=\"training_B\"\n",
    "                                            )\n",
    "        tf.add_to_collection(\"feed_vars_B\", training_B)\n",
    "\n",
    "    \n",
    "    inputs_B = X_B\n",
    "    \n",
    "    with tf.name_scope(\"dnn_B\"):\n",
    "        for i in range(1, n_layer_AB+1):\n",
    "\n",
    "            inputs_B = tf.layers.dense(\n",
    "                                    inputs_B,\n",
    "                                    n_hidden_AB,\n",
    "                                    activation=tf.nn.selu,\n",
    "                                    use_bias=True,\n",
    "                                    kernel_initializer=kernel_regularizer,\n",
    "                                    bias_initializer=tf.zeros_initializer(),\n",
    "                                    kernel_regularizer=None,\n",
    "                                    bias_regularizer=None,\n",
    "                                    activity_regularizer=None,\n",
    "                                    kernel_constraint=None,\n",
    "                                    bias_constraint=None,\n",
    "                                    trainable=True,\n",
    "                                    name=\"hidden{}_B\".format(i),\n",
    "                                    reuse=None\n",
    "                                )\n",
    "            tf.add_to_collection(\"hidden_B\", inputs_B)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Output layer and Other ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T06:20:12.063725Z",
     "start_time": "2018-02-01T06:20:10.049941Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    with tf.name_scope(\"inputs\"):\n",
    "        y = tf.placeholder(tf.int32, [None, n_outputs], name='y')\n",
    "        tf.add_to_collection(\"feed_vars\", y)\n",
    "\n",
    "    \n",
    "    with tf.name_scope(\"logits\"):\n",
    "        inputs = tf.concat(\n",
    "                        [inputs_A, inputs_B],\n",
    "                        axis=1,\n",
    "                        name=\"inputs\"\n",
    "                    )\n",
    "        tf.add_to_collection(\"logits\", inputs)\n",
    "\n",
    "        hidden1 = tf.layers.dense(\n",
    "                            inputs,\n",
    "                            n_hidden,\n",
    "                            activation=tf.nn.selu,\n",
    "                            use_bias=True,\n",
    "                            kernel_initializer=None,\n",
    "                            bias_initializer=tf.zeros_initializer(),\n",
    "                            kernel_regularizer=kernel_regularizer,\n",
    "                            bias_regularizer=None,\n",
    "                            activity_regularizer=None,\n",
    "                            kernel_constraint=None,\n",
    "                            bias_constraint=None,\n",
    "                            trainable=True,\n",
    "                            name=\"hidden1\",\n",
    "                            reuse=None\n",
    "                        )\n",
    "        \n",
    "        \n",
    "        logits = tf.layers.dense(\n",
    "                            hidden1,\n",
    "                            n_outputs,\n",
    "                            activation=None,\n",
    "                            use_bias=True,\n",
    "                            kernel_initializer=None,\n",
    "                            bias_initializer=tf.zeros_initializer(),\n",
    "                            kernel_regularizer=kernel_regularizer,\n",
    "                            bias_regularizer=None,\n",
    "                            activity_regularizer=None,\n",
    "                            kernel_constraint=None,\n",
    "                            bias_constraint=None,\n",
    "                            trainable=True,\n",
    "                            name=\"logits\",\n",
    "                            reuse=None\n",
    "                        )\n",
    "        tf.add_to_collection(\"logits\", logits)\n",
    "    \n",
    "    \n",
    "    with tf.name_scope(\"outputs\"):\n",
    "        y_prob = tf.nn.softmax(\n",
    "                            logits,\n",
    "                            axis=-1,\n",
    "                            name=\"y_prob\",\n",
    "                        )\n",
    "        tf.add_to_collection(\"prediction\", y_prob)\n",
    "\n",
    "        y_pred = tf.argmax(\n",
    "                        # The logtis is enough for determining the best one\n",
    "                        # don't need add additional calculation of y_prob\n",
    "                        logits,\n",
    "                        axis=1,\n",
    "                        name=\"y_pred\",\n",
    "                    )\n",
    "        tf.add_to_collection(\"prediction\", y_pred)\n",
    "\n",
    "        \n",
    "    with tf.name_scope(\"metrics\"):\n",
    "        true_labels = tf.argmax(\n",
    "                                y,\n",
    "                                axis=1,\n",
    "                                name=\"true_labels\",\n",
    "                               )\n",
    "        tf.add_to_collection(\"metrics\", true_labels)\n",
    "        \n",
    "        \n",
    "#         xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "#                                                          labels=true_labels,\n",
    "#                                                          logits=logits,\n",
    "#                                                          name=\"xentropy\"\n",
    "#                                                          )\n",
    "\n",
    "        xentropy = tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "                                                        labels=y,\n",
    "                                                        logits=logits,\n",
    "                                                        name=\"xentropy\"\n",
    "                                                    )\n",
    "        base_loss = tf.reduce_mean(xentropy, name=\"base_loss\")\n",
    "        reg_loss = tf.get_collection(\n",
    "            tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "        loss = tf.add_n([base_loss]+reg_loss, name=\"loss\")\n",
    "        tf.add_to_collection(\"metrics\", base_loss)\n",
    "        tf.add_to_collection(\"metrics\", loss)\n",
    "\n",
    "\n",
    "        correct = tf.nn.in_top_k(logits, true_labels, 1, name=\"correct\")\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), \n",
    "                                  name=\"accuracy\")\n",
    "        tf.add_to_collection(\"metrics\", correct)\n",
    "        tf.add_to_collection(\"metrics\", accuracy)\n",
    "\n",
    "    \n",
    "    \n",
    "    with tf.name_scope(\"training\"):\n",
    "        optimizer = tf.train.AdamOptimizer(\n",
    "                                learning_rate=learning_rate,\n",
    "                            )\n",
    "        training_op = optimizer.minimize(loss)\n",
    "        tf.add_to_collection(\"training\", training_op)\n",
    "        \n",
    "    \n",
    "    with tf.name_scope(\"saver\"):\n",
    "        saver = tf.train.Saver(name=\"saver\")\n",
    "\n",
    "\n",
    "    with tf.name_scope(\"logging\"):\n",
    "        file_writer = tf.summary.FileWriter(\n",
    "                                            log_dir,\n",
    "                                            graph=None,\n",
    "                                            max_queue=10,\n",
    "                                            flush_secs=120,\n",
    "                                            graph_def=None,\n",
    "                                            filename_suffix=None\n",
    "                                        )\n",
    "        \n",
    "        training_loss_log = tf.summary.scalar(\n",
    "                                            \"loss_log\",\n",
    "                                            loss,\n",
    "                                            collections=[tf.GraphKeys.SUMMARIES, \n",
    "                                                         \"logging\"],\n",
    "                                            family=\"training\"\n",
    "                                        )\n",
    "        \n",
    "        training_acc_log = tf.summary.scalar(\n",
    "                                            \"acc_log\",\n",
    "                                            accuracy,\n",
    "                                            collections=[tf.GraphKeys.SUMMARIES, \n",
    "                                                         \"logging\"],\n",
    "                                            family=\"training\"\n",
    "                                        )\n",
    "        \n",
    "        valid_loss_log = tf.summary.scalar(\n",
    "                                            \"loss_log\",\n",
    "                                            loss,\n",
    "                                            collections=[tf.GraphKeys.SUMMARIES, \n",
    "                                                         \"logging\"],\n",
    "                                            family=\"validation\"\n",
    "                                        )\n",
    "        \n",
    "        valid_acc_log = tf.summary.scalar(\n",
    "                                            \"acc_log\",\n",
    "                                            accuracy,\n",
    "                                            collections=[tf.GraphKeys.SUMMARIES, \n",
    "                                                         \"logging\"],\n",
    "                                            family=\"validation\"\n",
    "                                        )\n",
    "        \n",
    "    with tf.name_scope(\"initializer\"):\n",
    "        initializers = tf.group(tf.global_variables_initializer(),\n",
    "                                        tf.local_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# b.Data Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We need to create our training data in order to proceed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 1. Extract the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T12:43:12.040530Z",
     "start_time": "2018-02-01T12:43:11.158607Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./temp/datasets/train-images-idx3-ubyte.gz\n",
      "Extracting ./temp/datasets/train-labels-idx1-ubyte.gz\n",
      "Extracting ./temp/datasets/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./temp/datasets/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"./temp/datasets/\")\n",
    "X_train = mnist.train.images\n",
    "y_train = mnist.train.labels\n",
    "X_test = mnist.test.images\n",
    "y_test = mnist.test.labels\n",
    "X_valid = mnist.validation.images\n",
    "y_valid = mnist.validation.labels\n",
    "\n",
    "rnd_select_index=np.random.permutation(55000)\n",
    "\n",
    "X_train_1, X_train_2 = X_train[:55000], X_train[rnd_select_index[:5000]]\n",
    "y_train_1, y_train_2 = y_train[:55000], y_train[rnd_select_index[:5000]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Creating batching function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T06:20:14.167523Z",
     "start_time": "2018-02-01T06:20:13.921280Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def make_batch(X, y, batch_size, epoch_num=1, batch_num=1, state=42):\n",
    "    shape_X = [batch_size, 28 * 28]\n",
    "    shape_y = [batch_size, 2]\n",
    "    \n",
    "    data_size = X.shape[0]\n",
    "    np.random.seed((epoch_num * batch_size - batch_num * state)%(2**32-1))\n",
    "    c_same = 0\n",
    "    c_diff = 0\n",
    "    \n",
    "    X_batch_1=np.empty(shape_X, dtype=float, order='C')\n",
    "    X_batch_2=np.empty(shape_X, dtype=float, order='C')\n",
    "    y_batch=np.empty(shape_y, dtype=float, order='C')\n",
    "    while c_same + c_diff < batch_size:\n",
    "        if c_same >= batch_size // 2:\n",
    "            \n",
    "            rnd1 = np.random.randint(data_size)\n",
    "            rnd2 = np.random.randint(data_size)\n",
    "            \n",
    "            while y[rnd1] == y[rnd2]:\n",
    "                rnd2 = np.random.randint(data_size)\n",
    "            \n",
    "            X_batch_1[c_same+c_diff]=X[rnd1]\n",
    "            X_batch_2[c_same+c_diff]=X[rnd2]\n",
    "            y_batch[c_same+c_diff]=[0, 1]\n",
    "            \n",
    "            c_diff += 1\n",
    "            \n",
    "        elif c_diff >= batch_size // 2:\n",
    "\n",
    "            rnd1 = np.random.randint(data_size)\n",
    "            rnd2 = np.random.randint(data_size)\n",
    "            \n",
    "            while y[rnd1] != y[rnd2]:\n",
    "                rnd2 = np.random.randint(data_size)\n",
    "            \n",
    "            X_batch_1[c_same+c_diff]=X[rnd1]\n",
    "            X_batch_2[c_same+c_diff]=X[rnd2]\n",
    "            y_batch[c_same+c_diff]=[1, 0]\n",
    "            \n",
    "            c_same += 1\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            rnd1 = np.random.randint(data_size)\n",
    "            rnd2 = np.random.randint(data_size)\n",
    "            \n",
    "            is_same = (y[rnd1] == y[rnd2])\n",
    "\n",
    "            \n",
    "            X_batch_1[c_same+c_diff]=X[rnd1]\n",
    "            X_batch_2[c_same+c_diff]=X[rnd2]\n",
    "            y_batch[c_same+c_diff]=[0, 1] if (is_same == False) else [1, 0]\n",
    "\n",
    "            c_diff += (is_same == False)\n",
    "            c_same += (is_same == True)\n",
    "    rnd_ind = np.random.permutation(batch_size)\n",
    "    return X_batch_1[rnd_ind], X_batch_2[rnd_ind], y_batch[rnd_ind]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "heading_collapsed": true
   },
   "source": [
    "# c. Training DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T06:20:49.411185Z",
     "start_time": "2018-02-01T06:20:49.400555Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "n_epoch = 100\n",
    "batch_size = 1000\n",
    "log_freq = 3\n",
    "training_num = X_train_1.shape[0]\n",
    "\n",
    "if not os.path.isdir(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T06:31:29.757508Z",
     "start_time": "2018-02-01T06:20:50.296013Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3  Training accuracy: 0.8091, \n",
      "                  Training loss: 0.420722\n",
      "                                            Validation accuracy: 0.8000\n",
      "                                                Validation loss: 0.436135\n",
      "epoch: 6  Training accuracy: 0.8952, \n",
      "                  Training loss: 0.254889\n",
      "                                            Validation accuracy: 0.8894\n",
      "                                                Validation loss: 0.266316\n",
      "epoch: 9  Training accuracy: 0.9336, \n",
      "                  Training loss: 0.171562\n",
      "                                            Validation accuracy: 0.9298\n",
      "                                                Validation loss: 0.168983\n",
      "epoch: 12 Training accuracy: 0.9564, \n",
      "                  Training loss: 0.116516\n",
      "                                            Validation accuracy: 0.9514\n",
      "                                                Validation loss: 0.122248\n",
      "epoch: 15 Training accuracy: 0.9596, \n",
      "                  Training loss: 0.109162\n",
      "                                            Validation accuracy: 0.9590\n",
      "                                                Validation loss: 0.119346\n",
      "epoch: 18 Training accuracy: 0.9737, \n",
      "                  Training loss: 0.077796\n",
      "                                            Validation accuracy: 0.9710\n",
      "                                                Validation loss: 0.087903\n",
      "epoch: 21 Training accuracy: 0.9773, \n",
      "                  Training loss: 0.065308\n",
      "                                            Validation accuracy: 0.9682\n",
      "                                                Validation loss: 0.082371\n",
      "epoch: 24 Training accuracy: 0.9801, \n",
      "                  Training loss: 0.058036\n",
      "                                            Validation accuracy: 0.9730\n",
      "                                                Validation loss: 0.079007\n",
      "epoch: 27 Training accuracy: 0.9803, \n",
      "                  Training loss: 0.057257\n",
      "                                            Validation accuracy: 0.9716\n",
      "                                                Validation loss: 0.079668\n",
      "epoch: 30 Training accuracy: 0.9858, \n",
      "                  Training loss: 0.040800\n",
      "                                            Validation accuracy: 0.9804\n",
      "                                                Validation loss: 0.069845\n",
      "epoch: 33 Training accuracy: 0.9878, \n",
      "                  Training loss: 0.033986\n",
      "                                            Validation accuracy: 0.9808\n",
      "                                                Validation loss: 0.060388\n",
      "epoch: 36 Training accuracy: 0.9881, \n",
      "                  Training loss: 0.034289\n",
      "                                            Validation accuracy: 0.9798\n",
      "                                                Validation loss: 0.062493\n",
      "epoch: 39 Training accuracy: 0.9865, \n",
      "                  Training loss: 0.041259\n",
      "                                            Validation accuracy: 0.9804\n",
      "                                                Validation loss: 0.066905\n",
      "epoch: 42 Training accuracy: 0.9882, \n",
      "                  Training loss: 0.036718\n",
      "                                            Validation accuracy: 0.9788\n",
      "                                                Validation loss: 0.067605\n",
      "epoch: 45 Training accuracy: 0.9890, \n",
      "                  Training loss: 0.033980\n",
      "                                            Validation accuracy: 0.9808\n",
      "                                                Validation loss: 0.077468\n",
      "epoch: 48 Training accuracy: 0.9897, \n",
      "                  Training loss: 0.030613\n",
      "                                            Validation accuracy: 0.9788\n",
      "                                                Validation loss: 0.074672\n",
      "epoch: 51 Training accuracy: 0.9915, \n",
      "                  Training loss: 0.024994\n",
      "                                            Validation accuracy: 0.9824\n",
      "                                                Validation loss: 0.063365\n",
      "epoch: 54 Training accuracy: 0.9923, \n",
      "                  Training loss: 0.021955\n",
      "                                            Validation accuracy: 0.9796\n",
      "                                                Validation loss: 0.063394\n",
      "epoch: 57 Training accuracy: 0.9931, \n",
      "                  Training loss: 0.022444\n",
      "                                            Validation accuracy: 0.9794\n",
      "                                                Validation loss: 0.069351\n",
      "epoch: 60 Training accuracy: 0.9945, \n",
      "                  Training loss: 0.016134\n",
      "                                            Validation accuracy: 0.9820\n",
      "                                                Validation loss: 0.076630\n",
      "epoch: 63 Training accuracy: 0.9911, \n",
      "                  Training loss: 0.027512\n",
      "                                            Validation accuracy: 0.9802\n",
      "                                                Validation loss: 0.087376\n",
      "epoch: 66 Training accuracy: 0.9962, \n",
      "                  Training loss: 0.011885\n",
      "                                            Validation accuracy: 0.9810\n",
      "                                                Validation loss: 0.072206\n",
      "Early Stopping!\n"
     ]
    }
   ],
   "source": [
    "num_batch = np.ceil(training_num/batch_size).astype(int)\n",
    "with graph.as_default():\n",
    "    with tf.Session(graph=graph) as sess:\n",
    "        c_drop = 0\n",
    "        c_max = 5\n",
    "        \n",
    "        max_acc = 0\n",
    "        \n",
    "        initializers.run()\n",
    "        \n",
    "        file_writer.add_graph(tf.get_default_graph())\n",
    "        \n",
    "        for epoch in range(1, n_epoch+1):\n",
    "            for batch in range(1, num_batch+1):\n",
    "                X_batch_1, X_batch_2, y_batch = make_batch(X_train_1, \n",
    "                                                           y_train_1, \n",
    "                                                           batch_size, \n",
    "                                                           epoch, \n",
    "                                                           batch\n",
    "                                                          )\n",
    "                training_op.run(feed_dict={X_A:X_batch_1,\n",
    "                                           X_B:X_batch_2,\n",
    "                                           y:y_batch}\n",
    "                               )\n",
    "            \n",
    "            if not epoch % log_freq:\n",
    "                save_path = \\\n",
    "                    saver.save(sess, \n",
    "                               os.path.join(save_dir, \n",
    "                                            \"{}.ckpt\".format(epoch/log_freq)\n",
    "                                           )\n",
    "                              )\n",
    "                X_batch_1, X_batch_2, y_batch = make_batch(X_train_1, \n",
    "                                                           y_train_1, \n",
    "                                                           X_train_1.shape[0]//3, \n",
    "                                                           epoch, \n",
    "                                                           batch\n",
    "                                                          )\n",
    "\n",
    "                training_loss, training_acc, \\\n",
    "                log_training_loss, log_training_acc = \\\n",
    "                                        sess.run([loss, \n",
    "                                                  accuracy, \n",
    "                                                  training_loss_log, \n",
    "                                                  training_acc_log], \n",
    "                                                 feed_dict={X_A:X_batch_1,\n",
    "                                                            X_B:X_batch_2,\n",
    "                                                            y:y_batch})\n",
    "                    \n",
    "#                 training_loss, training_acc,\\\n",
    "#                 log_training_loss, log_training_acc,\\\n",
    "#                 train_inputs_A, train_inputs_B, train_input, train_pred, \\\n",
    "#                 train_prob, train_label, train_correct, train_logits,\\\n",
    "#                 train_entropy= \\\n",
    "#                                         sess.run([loss, \n",
    "#                                                   accuracy, \n",
    "#                                                   training_loss_log, \n",
    "#                                                   training_acc_log,\n",
    "#                                                   inputs_A,\n",
    "#                                                   inputs_B,\n",
    "#                                                   inputs,\n",
    "#                                                   y_pred,\n",
    "#                                                   y_prob,\n",
    "#                                                   true_labels,\n",
    "#                                                   correct,\n",
    "#                                                   logits,\n",
    "#                                                   xentropy\n",
    "#                                                  ], \n",
    "#                                                  feed_dict={X_A:X_batch_1,\n",
    "#                                                             X_B:X_batch_2,\n",
    "#                                                             y:y_batch})\n",
    "\n",
    "#                 print(\"pred\", train_pred[:5])\n",
    "#                 print(\"prob\", train_prob[:5])\n",
    "#                 print(\"label\",train_label[:5])\n",
    "#                 print(\"correct\", train_correct[:5])\n",
    "#                 print(np.unique(train_correct, return_counts=True))\n",
    "                \n",
    "                \n",
    "                X_val_1, X_val_2, y_val = make_batch(X_valid, \n",
    "                                                     y_valid, \n",
    "                                                     X_valid.shape[0]\n",
    "                                                    )\n",
    "                valid_loss, valid_acc, log_valid_loss, log_valid_acc = \\\n",
    "                                        sess.run([loss, \n",
    "                                                  accuracy, \n",
    "                                                  valid_loss_log, \n",
    "                                                  valid_acc_log], \n",
    "                                                 feed_dict={X_A:X_val_1,\n",
    "                                                            X_B:X_val_2,\n",
    "                                                            y:y_val})\n",
    "                \n",
    "                file_writer.add_summary(log_training_loss, epoch * num_batch)\n",
    "                file_writer.add_summary(log_training_acc, epoch * num_batch)\n",
    "                file_writer.add_summary(log_valid_loss, epoch * num_batch)\n",
    "                file_writer.add_summary(log_valid_acc, epoch * num_batch)\n",
    "                \n",
    "                \n",
    "                \n",
    "                print(\"epoch: {:<3}Training accuracy: {:<6.4f}, \\n\\\n",
    "                  Training loss: {:<6.6f}\\n\\\n",
    "                                            Validation accuracy: {:<6.4f}\\n\\\n",
    "                                                Validation loss: {:<6.6f}\"\n",
    "                          .format(epoch, training_acc, training_loss, \n",
    "                                  valid_acc, valid_loss))\n",
    "                if valid_acc >= max_acc:\n",
    "                    max_acc = valid_acc\n",
    "                    c_drop = 0\n",
    "                    final_save_path = save_path\n",
    "                else:\n",
    "                    c_drop += 1\n",
    "                \n",
    "                if c_drop >= c_max:\n",
    "                    print(\"Early Stopping!\")\n",
    "                    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Let's do a test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T06:32:20.926806Z",
     "start_time": "2018-02-01T06:32:20.232412Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T06:32:23.909956Z",
     "start_time": "2018-02-01T06:32:23.456800Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_batch_1, X_batch_2, y_batch = make_batch(X_test, \n",
    "                                           y_test, \n",
    "                                           X_test.shape[0]//2\n",
    "                                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T06:35:07.621095Z",
     "start_time": "2018-02-01T06:35:06.904572Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAC7CAYAAAB1qmWGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n2BXqYGOFNAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10f10db00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "same image\n"
     ]
    }
   ],
   "source": [
    "ind = np.random.randint(500)\n",
    "img1 = X_batch_1[ind].reshape(28,28)\n",
    "img2 = X_batch_2[ind].reshape(28,28)\n",
    "result = np.argmax(y_batch[ind])\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.imshow(img1, cmap=\"Greys\")\n",
    "plt.subplot(122)\n",
    "plt.imshow(img2, cmap=\"Greys\")\n",
    "plt.show()\n",
    "\n",
    "if result:\n",
    "    print(\"different image\")\n",
    "    \n",
    "if not result:\n",
    "    print(\"same image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T06:35:05.141597Z",
     "start_time": "2018-02-01T06:35:05.134126Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "final_save_path = './temp/saves/20180201172009/18.0.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T06:35:11.574673Z",
     "start_time": "2018-02-01T06:35:11.365979Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./temp/saves/20180201172009/18.0.ckpt\n",
      "same image\n"
     ]
    }
   ],
   "source": [
    "with graph.as_default():\n",
    "    with tf.Session() as sess:\n",
    "#         sess.run(initializers)\n",
    "        saver.restore(sess, final_save_path)\n",
    "        \n",
    "        pred = sess.run(y_pred, feed_dict={\n",
    "                X_A: img1.reshape(1,-1), X_B: img2.reshape(1,-1)\n",
    "            })\n",
    "if pred:\n",
    "    print(\"different image\")\n",
    "\n",
    "if not pred:\n",
    "    print(\"same image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T06:34:54.589204Z",
     "start_time": "2018-02-01T06:34:53.122155Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./temp/saves/20180201172009/18.0.ckpt\n",
      "0.9788\n"
     ]
    }
   ],
   "source": [
    "with graph.as_default():\n",
    "    with tf.Session() as sess:\n",
    "#         sess.run(initializers)\n",
    "        saver.restore(sess, final_save_path)\n",
    "        X_batch_1, X_batch_2, y_batch = make_batch(X_test, \n",
    "                                                   y_test, \n",
    "                                                   X_test.shape[0]//2\n",
    "                                                  )\n",
    "        acc = sess.run(accuracy, feed_dict={\n",
    "                X_A: X_batch_1, X_B: X_batch_2, y:y_batch\n",
    "            })\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# d.Transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T13:47:54.572200Z",
     "start_time": "2018-02-01T13:47:53.747995Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    del graph\n",
    "except:\n",
    "    pass\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    saver = tf.train.import_meta_graph(final_save_path+\".meta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T13:47:55.628292Z",
     "start_time": "2018-02-01T13:47:55.271752Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    X_A = tf.get_collection(\"feed_vars_A\",scope=\".*X_A:0\")[0]\n",
    "    A_vars = graph.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, \n",
    "                                  scope=\"hidden[1-3]_A\")\n",
    "    y = tf.placeholder(tf.int32, name=\"new_y\")\n",
    "    inputs_A = tf.get_collection(\"hidden_A\")[2]\n",
    "    frozen_outputs = tf.stop_gradient(inputs_A)\n",
    "    logits = tf.layers.dense(\n",
    "                    inputs_A,\n",
    "                    10,\n",
    "                    activation=None,\n",
    "                    use_bias=True,\n",
    "                    kernel_initializer=None,\n",
    "                    bias_initializer=tf.zeros_initializer(),\n",
    "                    kernel_regularizer=kernel_regularizer,\n",
    "                    bias_regularizer=None,\n",
    "                    activity_regularizer=None,\n",
    "                    kernel_constraint=None,\n",
    "                    bias_constraint=None,\n",
    "                    trainable=True,\n",
    "                    name=\"logits\",\n",
    "                    reuse=None\n",
    "                )\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                                                    labels=y,\n",
    "                                                    logits=logits,\n",
    "                                                    name=\"xentropy\"\n",
    "                                                )\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, name=\"new_optimizer\")\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    \n",
    "    correct = tf.cast(tf.nn.in_top_k(logits, y, 1, name=\"new_correct\"), tf.float32)\n",
    "    accuracy = tf.reduce_mean(correct, name=\"new_acc\")\n",
    "#     print(A_vars)\n",
    "#     for var in A_vars:\n",
    "#         print(\"======================\")\n",
    "#         print(var.op)\n",
    "#         print(var.op.name)\n",
    "#         print(\"======================\")\n",
    "    res_saver = tf.train.Saver(var_list={var.op.name: var for var in A_vars})\n",
    "    \n",
    "    initializer = tf.global_variables_initializer()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T13:47:56.339089Z",
     "start_time": "2018-02-01T13:47:56.332828Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "n_epoch = 100\n",
    "batch_size = 100\n",
    "log_freq = 3\n",
    "training_num = X_train_1.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T13:47:57.003991Z",
     "start_time": "2018-02-01T13:47:56.994267Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 784)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T13:49:35.833928Z",
     "start_time": "2018-02-01T13:49:17.695950Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./temp/transfer/20180202001149/4.0.ckpt\n",
      "epoch: 3  Training loss: 0.648311\n",
      "                                            Validation accuracy: 0.7924\n",
      "\n",
      "epoch: 6  Training loss: 0.285135\n",
      "                                            Validation accuracy: 0.9120\n",
      "\n",
      "epoch: 9  Training loss: 0.148013\n",
      "                                            Validation accuracy: 0.9472\n",
      "\n",
      "epoch: 12 Training loss: 0.136692\n",
      "                                            Validation accuracy: 0.9486\n",
      "\n",
      "epoch: 15 Training loss: 0.118657\n",
      "                                            Validation accuracy: 0.9506\n",
      "\n",
      "epoch: 18 Training loss: 0.071435\n",
      "                                            Validation accuracy: 0.9560\n",
      "\n",
      "epoch: 21 Training loss: 0.045563\n",
      "                                            Validation accuracy: 0.9606\n",
      "\n",
      "epoch: 24 Training loss: 0.034160\n",
      "                                            Validation accuracy: 0.9614\n",
      "\n",
      "epoch: 27 Training loss: 0.024357\n",
      "                                            Validation accuracy: 0.9624\n",
      "\n",
      "epoch: 30 Training loss: 0.016362\n",
      "                                            Validation accuracy: 0.9640\n",
      "\n",
      "epoch: 33 Training loss: 0.011802\n",
      "                                            Validation accuracy: 0.9670\n",
      "\n",
      "epoch: 36 Training loss: 0.008900\n",
      "                                            Validation accuracy: 0.9680\n",
      "\n",
      "epoch: 39 Training loss: 0.006089\n",
      "                                            Validation accuracy: 0.9682\n",
      "\n",
      "epoch: 42 Training loss: 0.004145\n",
      "                                            Validation accuracy: 0.9686\n",
      "\n",
      "epoch: 45 Training loss: 0.003143\n",
      "                                            Validation accuracy: 0.9680\n",
      "\n",
      "epoch: 48 Training loss: 0.002544\n",
      "                                            Validation accuracy: 0.9680\n",
      "\n",
      "epoch: 51 Training loss: 0.002107\n",
      "                                            Validation accuracy: 0.9678\n",
      "\n",
      "epoch: 54 Training loss: 0.001744\n",
      "                                            Validation accuracy: 0.9680\n",
      "\n",
      "epoch: 57 Training loss: 0.001452\n",
      "                                            Validation accuracy: 0.9688\n",
      "\n",
      "epoch: 60 Training loss: 0.001237\n",
      "                                            Validation accuracy: 0.9690\n",
      "\n",
      "epoch: 63 Training loss: 0.001090\n",
      "                                            Validation accuracy: 0.9694\n",
      "\n",
      "epoch: 66 Training loss: 0.000986\n",
      "                                            Validation accuracy: 0.9694\n",
      "\n",
      "epoch: 69 Training loss: 0.000904\n",
      "                                            Validation accuracy: 0.9690\n",
      "\n",
      "epoch: 72 Training loss: 0.000836\n",
      "                                            Validation accuracy: 0.9690\n",
      "\n",
      "epoch: 75 Training loss: 0.000777\n",
      "                                            Validation accuracy: 0.9690\n",
      "\n",
      "epoch: 78 Training loss: 0.000727\n",
      "                                            Validation accuracy: 0.9686\n",
      "\n",
      "epoch: 81 Training loss: 0.000682\n",
      "                                            Validation accuracy: 0.9682\n",
      "\n",
      "Early Stopping!\n"
     ]
    }
   ],
   "source": [
    "time_now = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "new_save_dir = os.path.join(base_dir, \"transfer\", time_now)\n",
    "\n",
    "if not os.path.isdir(new_save_dir):\n",
    "    os.makedirs(new_save_dir)\n",
    "\n",
    "with graph.as_default():\n",
    "    with tf.Session(graph=graph) as sess:\n",
    "        c_drop = 0\n",
    "        c_max = 5\n",
    "        \n",
    "        max_acc = 0\n",
    "        \n",
    "        initializer.run()\n",
    "        res_saver.restore(sess, final_save_path)\n",
    "        \n",
    "        for epoch in range(1, n_epoch+1):\n",
    "            training_op.run(feed_dict={X_A:X_train_2,\n",
    "                                       y:y_train_2}\n",
    "                           )\n",
    "            \n",
    "            if not epoch % log_freq:\n",
    "                save_path = \\\n",
    "                    saver.save(sess, \n",
    "                               os.path.join(new_save_dir, \n",
    "                                            \"{}.ckpt\".format(epoch/log_freq)\n",
    "                                           )\n",
    "                              )\n",
    "\n",
    "                training_loss = sess.run(loss, \n",
    "                                        feed_dict={X_A:X_train_2,\n",
    "                                                   y:y_train_2})\n",
    "\n",
    "                valid_acc = sess.run(accuracy,\n",
    "                                     feed_dict={X_A:X_valid,\n",
    "                                               y:y_valid})\n",
    "                \n",
    "                \n",
    "                \n",
    "                print(\"epoch: {:<3}Training loss: {:<6.6f}\\n\\\n",
    "                                            Validation accuracy: {:<6.4f}\\n\"\n",
    "                          .format(epoch,training_loss, \n",
    "                                  valid_acc))\n",
    "                if valid_acc >= max_acc:\n",
    "                    max_acc = valid_acc\n",
    "                    c_drop = 0\n",
    "                    final_save_path = save_path\n",
    "                else:\n",
    "                    c_drop += 1\n",
    "                \n",
    "                if c_drop >= c_max:\n",
    "                    print(\"Early Stopping!\")\n",
    "                    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### We got 96 near 97% accuracy!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
