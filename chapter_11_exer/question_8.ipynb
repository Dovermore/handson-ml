{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T13:18:54.647602Z",
     "start_time": "2018-01-18T13:18:49.409896Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/ml_env/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from datetime import datetime\n",
    "import os\n",
    "from time import clock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T13:18:55.085694Z",
     "start_time": "2018-01-18T13:18:54.690016Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T14:13:17.319022Z",
     "start_time": "2018-01-18T14:13:16.536092Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "X_train = mnist.train.images\n",
    "y_train = mnist.train.labels\n",
    "X_test = mnist.test.images\n",
    "y_test = mnist.test.labels\n",
    "X_valid = mnist.validation.images\n",
    "y_valid = mnist.validation.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T14:14:42.257003Z",
     "start_time": "2018-01-18T14:14:42.142258Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ind_04_train = (np.argmax(y_train, axis=1) <= 4)\n",
    "ind_04_test = (np.argmax(y_test, axis=1) <= 4)\n",
    "ind_04_valid = (np.argmax(y_valid, axis=1) <= 4)\n",
    "X_train_04 = X_train[ind_04_train]\n",
    "y_train_04 = np.delete(y_train[ind_04_train], [5,6,7,8,9], axis=1)\n",
    "X_test_04 = X_test[ind_04_test]\n",
    "y_test_04 = np.delete(y_test[ind_04_test], [5,6,7,8,9], axis=1)\n",
    "X_valid_04 = X_valid[ind_04_valid]\n",
    "y_valid_04 = np.delete(y_valid[ind_04_valid], [5,6,7,8,9], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T14:14:43.664845Z",
     "start_time": "2018-01-18T14:14:43.655286Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55000\n",
      "10000\n",
      "5000\n",
      "(28038, 5)\n"
     ]
    }
   ],
   "source": [
    "print(len(mnist.train.images))\n",
    "print(len(mnist.test.images))\n",
    "print(len(mnist.validation.images))\n",
    "print(y_train_04.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load a image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T14:15:48.357047Z",
     "start_time": "2018-01-18T14:15:48.230756Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a1a3acc50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "some_index = 50000\n",
    "some_digit = mnist.train.images[some_index]\n",
    "\n",
    "plt.imshow(some_digit.reshape(28, 28), cmap=\"gray\")\n",
    "plt.show()\n",
    "print(np.argmax(mnist.train.labels[some_index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now build the computational graph for 0-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\n",
    "## First iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T14:33:16.796480Z",
     "start_time": "2018-01-18T14:33:16.790781Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iter_ind=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define some variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T14:33:17.063994Z",
     "start_time": "2018-01-18T14:33:17.054427Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_size = mnist.train.num_examples\n",
    "n_epoch = 50\n",
    "batch_size = 200\n",
    "\n",
    "n_feature = mnist.train.images.shape[1]\n",
    "n_neuron = {\"n_hidden{}\".format(i) : 100 for i in range(6)}\n",
    "n_label = mnist.train.labels.shape[1]\n",
    "n_label_04 = 5\n",
    "\n",
    "d_dtype=tf.float64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### first define some common functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T14:33:17.342050Z",
     "start_time": "2018-01-18T14:33:17.333967Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T14:33:17.772510Z",
     "start_time": "2018-01-18T14:33:17.767626Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_dnn_layer(activation=None, \n",
    "                 kernel_initializer=tf.contrib.layers.variance_scaling_initializer()):\n",
    "    return partial(tf.layers.dense, activation=activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T14:33:17.993383Z",
     "start_time": "2018-01-18T14:33:17.986808Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(X, y, batch_size, epoch, batch):\n",
    "    num_inst = X.shape[0]\n",
    "    np.random.seed(epoch * batch * np.random.randint(100))\n",
    "    shuffle_index = np.random.randint(num_inst, size=batch_size)\n",
    "    return X[shuffle_index], y[shuffle_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T14:33:18.246072Z",
     "start_time": "2018-01-18T14:33:18.239220Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T14:33:18.704417Z",
     "start_time": "2018-01-18T14:33:18.697745Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_04 = tf.placeholder(dtype=d_dtype, shape=(None, n_feature), name=\"X_04\")\n",
    "y_04 = tf.placeholder(dtype=d_dtype, shape=(None, n_label_04), name=\"y_04\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T14:33:19.069045Z",
     "start_time": "2018-01-18T14:33:18.982240Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "selu_layer = my_dnn_layer(activation=tf.nn.selu)\n",
    "linear_layer = my_dnn_layer(activation=None)\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = selu_layer(X_04, n_neuron[\"n_hidden1\"], name=\"hidden1\")\n",
    "    hidden2 = selu_layer(hidden1, n_neuron[\"n_hidden2\"], name=\"hidden2\")\n",
    "    hidden3 = selu_layer(hidden2, n_neuron[\"n_hidden3\"], name=\"hidden3\")\n",
    "    hidden4 = selu_layer(hidden3, n_neuron[\"n_hidden4\"], name=\"hidden4\")\n",
    "    hidden5 = selu_layer(hidden4, n_neuron[\"n_hidden5\"], name=\"hidden5\")\n",
    "    logits = linear_layer(hidden5, n_label_04, name=\"logits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T14:33:19.607164Z",
     "start_time": "2018-01-18T14:33:19.345418Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.softmax_cross_entropy_with_logits(labels=y_04, logits=logits, name=\"xentropy\")\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T14:33:19.866325Z",
     "start_time": "2018-01-18T14:33:19.824221Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    true_labels = tf.argmax(y_04, axis=1)\n",
    "    y_pred = tf.argmax(logits, axis=1)\n",
    "    all_acc, acc = tf.metrics.accuracy(labels=true_labels, predictions=y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T14:33:20.265293Z",
     "start_time": "2018-01-18T14:33:20.084043Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"train\"):\n",
    "    adam_optimizer = tf.train.AdamOptimizer()\n",
    "    train_op = adam_optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T14:33:31.818495Z",
     "start_time": "2018-01-18T14:33:31.710795Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "time_now = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "base_dir = os.path.join(\"temp\", str(iter_ind))\n",
    "save_dir = os.path.join(base_dir, time_now, \"saves\")\n",
    "log_dir = os.path.join(base_dir, time_now, \"logs\")\n",
    "\n",
    "with tf.name_scope(\"savNlog\"):\n",
    "    saver = tf.train.Saver()\n",
    "    file_writer = tf.summary.FileWriter(log_dir) # Need to add the graph\n",
    "    train_acc_log = tf.summary.scalar(\"train_accuracy\", acc)\n",
    "    validation_acc_log = tf.summary.scalar(\"validation_accuracy\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T13:19:00.379999Z",
     "start_time": "2018-01-18T13:19:00.001821Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# with tf.name_scope(\"make_batch\"):\n",
    "    X_batch_tens, y_batch_tens = tf.train.batch(\n",
    "        [mnist.train.images, mnist.train.labels], batch_size=batch_size, capacity=capacity,\n",
    "        num_threads=1, allow_smaller_final_batch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T14:34:31.564922Z",
     "start_time": "2018-01-18T14:34:31.559173Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"initializer\"):\n",
    "    inits = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T14:37:12.030105Z",
     "start_time": "2018-01-18T14:34:31.948935Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5  Training accuracy: 0.9950, \n",
      "Validation accuracy: 0.9880\n",
      "epoch: 10 Training accuracy: 0.9888, \n",
      "Validation accuracy: 0.9889\n",
      "epoch: 15 Training accuracy: 0.9893, \n",
      "Validation accuracy: 0.9906\n",
      "epoch: 20 Training accuracy: 0.9908, \n",
      "Validation accuracy: 0.9915\n",
      "epoch: 25 Training accuracy: 0.9916, \n",
      "Validation accuracy: 0.9916\n",
      "epoch: 30 Training accuracy: 0.9917, \n",
      "Validation accuracy: 0.9917\n",
      "epoch: 35 Training accuracy: 0.9918, \n",
      "Validation accuracy: 0.9918\n",
      "epoch: 40 Training accuracy: 0.9919, \n",
      "Validation accuracy: 0.9919\n",
      "epoch: 45 Training accuracy: 0.9920, \n",
      "Validation accuracy: 0.9920\n",
      "epoch: 50 Training accuracy: 0.9921, \n",
      "Validation accuracy: 0.9921\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(inits)\n",
    "    file_writer.add_graph(tf.get_default_graph())\n",
    "    tf.train.start_queue_runners(sess=sess)\n",
    "    for epoch in range(1, n_epoch+1):\n",
    "        for batch in range(1, np.ceil(train_size/batch_size).astype(int)+1):\n",
    "            X_batch, y_batch =  get_batch(X_train_04, y_train_04, batch_size, epoch, batch)\n",
    "            sess.run(train_op, feed_dict={X_04: X_batch, y_04: y_batch})\n",
    "        if not epoch % 5:\n",
    "            train_acc, log_train = sess.run([acc, train_acc_log], \n",
    "                                            feed_dict={X_04: X_batch, y_04: y_batch})\n",
    "            val_acc, log_val = sess.run([acc, validation_acc_log],\n",
    "                                        feed_dict={X_04: X_valid_04, y_04: y_valid_04})\n",
    "            file_writer.add_summary(log_train, epoch)\n",
    "            file_writer.add_summary(log_val, epoch)\n",
    "            print(\"epoch: {:<3}Training accuracy: {:<6.4f}, \\nValidation accuracy: {:<6.4f}\"\n",
    "                  .format(epoch, train_acc, val_acc))\n",
    "            if not epoch % 10:\n",
    "                save_file_path = saver.save(sess, os.path.join(save_dir, \"{}.ckpt\".format(epoch//10)))\n",
    "                \n",
    "    file_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the result on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T14:37:12.678862Z",
     "start_time": "2018-01-18T14:37:12.406137Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from temp/1/20180119013331/saves/5.ckpt\n",
      "Accuracy for test set is: 0.9957\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    inits.run()\n",
    "    saver.restore(sess, save_file_path)\n",
    "    acc_test = sess.run(acc, feed_dict={X_04: X_test_04, y_04: y_test_04})\n",
    "    print(\"Accuracy for test set is: {:>4.4f}\".format(acc_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\n",
    "## Second iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T14:37:13.021453Z",
     "start_time": "2018-01-18T14:37:13.017125Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iter_ind = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Strong>Change</Strong>\n",
    "<ol>\n",
    "    <li>changed neuron per layer to 150\n",
    "    </li>\n",
    "    <li>changed activation function to elu for comparison\n",
    "    </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define some variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T14:40:43.390644Z",
     "start_time": "2018-01-18T14:40:43.377638Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_size = mnist.train.num_examples\n",
    "n_epoch = 50\n",
    "batch_size = 200\n",
    "\n",
    "n_feature = mnist.train.images.shape[1]\n",
    "n_neuron = {\"n_hidden{}\".format(i) : 150 for i in range(6)}\n",
    "n_label = mnist.train.labels.shape[1]\n",
    "n_label_04 = 5\n",
    "\n",
    "d_dtype=tf.float64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define some common functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T14:40:43.732313Z",
     "start_time": "2018-01-18T14:40:43.725749Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def my_dnn_layer(activation=None, \n",
    "                 kernel_initializer=tf.contrib.layers.variance_scaling_initializer()):\n",
    "    return partial(tf.layers.dense, activation=activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T14:40:44.141131Z",
     "start_time": "2018-01-18T14:40:44.133009Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(X, y, batch_size, epoch, batch):\n",
    "    num_inst = X.shape[0]\n",
    "    np.random.seed(epoch * batch * np.random.randint(100))\n",
    "    shuffle_index = np.random.randint(num_inst, size=batch_size)\n",
    "    return X[shuffle_index], y[shuffle_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T14:40:45.095412Z",
     "start_time": "2018-01-18T14:40:45.091750Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T14:40:45.416421Z",
     "start_time": "2018-01-18T14:40:45.407148Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_04 = tf.placeholder(dtype=d_dtype, shape=(None, n_feature), name=\"X_04\")\n",
    "y_04 = tf.placeholder(dtype=d_dtype, shape=(None, n_label_04), name=\"y_04\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T14:40:45.802519Z",
     "start_time": "2018-01-18T14:40:45.720694Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "elu_layer = my_dnn_layer(activation=tf.nn.elu)\n",
    "linear_layer = my_dnn_layer(activation=None)\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = elu_layer(X_04, n_neuron[\"n_hidden1\"], name=\"hidden1\")\n",
    "    hidden2 = elu_layer(hidden1, n_neuron[\"n_hidden2\"], name=\"hidden2\")\n",
    "    hidden3 = elu_layer(hidden2, n_neuron[\"n_hidden3\"], name=\"hidden3\")\n",
    "    hidden4 = elu_layer(hidden3, n_neuron[\"n_hidden4\"], name=\"hidden4\")\n",
    "    hidden5 = elu_layer(hidden4, n_neuron[\"n_hidden5\"], name=\"hidden5\")\n",
    "    logits = linear_layer(hidden5, n_label_04, name=\"logits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T14:40:46.068366Z",
     "start_time": "2018-01-18T14:40:46.038493Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.softmax_cross_entropy_with_logits(labels=y_04, logits=logits, name=\"xentropy\")\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T14:40:46.289461Z",
     "start_time": "2018-01-18T14:40:46.266335Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    true_labels = tf.argmax(y_04, axis=1)\n",
    "    y_pred = tf.argmax(logits, axis=1)\n",
    "    all_acc, acc = tf.metrics.accuracy(labels=true_labels, predictions=y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T14:40:46.685628Z",
     "start_time": "2018-01-18T14:40:46.464915Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"train\"):\n",
    "    adam_optimizer = tf.train.AdamOptimizer()\n",
    "    train_op = adam_optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T14:41:00.737273Z",
     "start_time": "2018-01-18T14:41:00.478633Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "time_now = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "base_dir = os.path.join(\"temp\", str(iter_ind))\n",
    "save_dir = os.path.join(base_dir, time_now, \"saves\")\n",
    "log_dir = os.path.join(base_dir, time_now, \"logs\")\n",
    "\n",
    "with tf.name_scope(\"savNlog\"):\n",
    "    saver = tf.train.Saver()\n",
    "    file_writer = tf.summary.FileWriter(log_dir) # Need to add the graph\n",
    "    train_acc_log = tf.summary.scalar(\"train_accuracy\", acc)\n",
    "    validation_acc_log = tf.summary.scalar(\"validation_accuracy\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T14:41:00.999097Z",
     "start_time": "2018-01-18T14:41:00.992364Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"initializer\"):\n",
    "    inits = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T14:45:11.907733Z",
     "start_time": "2018-01-18T14:41:01.648679Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5  Training accuracy: 1.0000, \n",
      "Validation accuracy: 0.9840\n",
      "epoch: 10 Training accuracy: 0.9851, \n",
      "Validation accuracy: 0.9868\n",
      "epoch: 15 Training accuracy: 0.9872, \n",
      "Validation accuracy: 0.9888\n",
      "epoch: 20 Training accuracy: 0.9890, \n",
      "Validation accuracy: 0.9898\n",
      "epoch: 25 Training accuracy: 0.9900, \n",
      "Validation accuracy: 0.9906\n",
      "epoch: 30 Training accuracy: 0.9907, \n",
      "Validation accuracy: 0.9911\n",
      "epoch: 35 Training accuracy: 0.9912, \n",
      "Validation accuracy: 0.9915\n",
      "epoch: 40 Training accuracy: 0.9916, \n",
      "Validation accuracy: 0.9918\n",
      "epoch: 45 Training accuracy: 0.9919, \n",
      "Validation accuracy: 0.9920\n",
      "epoch: 50 Training accuracy: 0.9921, \n",
      "Validation accuracy: 0.9922\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(inits)\n",
    "    file_writer.add_graph(tf.get_default_graph())\n",
    "    tf.train.start_queue_runners(sess=sess)\n",
    "    for epoch in range(1, n_epoch+1):\n",
    "        for batch in range(1, np.ceil(train_size/batch_size).astype(int)+1):\n",
    "            X_batch, y_batch =  get_batch(X_train_04, y_train_04, batch_size, epoch, batch)\n",
    "            sess.run(train_op, feed_dict={X_04: X_batch, y_04: y_batch})\n",
    "        if not epoch % 5:\n",
    "            train_acc, log_train = sess.run([acc, train_acc_log], \n",
    "                                            feed_dict={X_04: X_batch, y_04: y_batch})\n",
    "            val_acc, log_val = sess.run([acc, validation_acc_log],\n",
    "                                        feed_dict={X_04: X_valid_04, y_04: y_valid_04})\n",
    "            file_writer.add_summary(log_train, epoch)\n",
    "            file_writer.add_summary(log_val, epoch)\n",
    "            print(\"epoch: {:<3}Training accuracy: {:<6.4f}, \\nValidation accuracy: {:<6.4f}\"\n",
    "                  .format(epoch, train_acc, val_acc))\n",
    "            if not epoch % 10:\n",
    "                save_file_path = saver.save(sess, os.path.join(save_dir, \"{}.ckpt\".format(epoch//10)))\n",
    "                \n",
    "    file_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the result on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T14:45:12.624793Z",
     "start_time": "2018-01-18T14:45:12.276399Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from temp/2/20180119014100/saves/5.ckpt\n",
      "Accuracy for test set is: 0.9953\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    inits.run()\n",
    "    saver.restore(sess, save_file_path)\n",
    "    acc_test = sess.run(acc, feed_dict={X_04: X_test_04, y_04: y_test_04})\n",
    "    print(\"Accuracy for test set is: {:>4.4f}\".format(acc_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\n",
    "## Third iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:27:22.613119Z",
     "start_time": "2018-01-18T15:27:22.609012Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iter_ind = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Strong>Change</Strong>\n",
    "<ol>\n",
    "    <li>Layer number back to 100 for faster training, same for selu\n",
    "    </li>\n",
    "    <li>Adding early stopping\n",
    "    </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define some variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:27:23.348460Z",
     "start_time": "2018-01-18T15:27:23.337886Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_size = mnist.train.num_examples\n",
    "n_epoch = 50\n",
    "batch_size = 200\n",
    "max_drop = 5\n",
    "\n",
    "n_feature = mnist.train.images.shape[1]\n",
    "n_neuron = {\"n_hidden{}\".format(i) : 100 for i in range(6)}\n",
    "n_label = mnist.train.labels.shape[1]\n",
    "n_label_04 = 5\n",
    "\n",
    "d_dtype=tf.float64\n",
    "\n",
    "log_freq = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define some common functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:27:24.064403Z",
     "start_time": "2018-01-18T15:27:24.059622Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def my_dnn_layer(activation=None, \n",
    "                 kernel_initializer=tf.contrib.layers.variance_scaling_initializer()):\n",
    "    return partial(tf.layers.dense, activation=activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:27:24.445745Z",
     "start_time": "2018-01-18T15:27:24.439675Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(X, y, batch_size, epoch, batch):\n",
    "    num_inst = X.shape[0]\n",
    "    np.random.seed(epoch * batch * np.random.randint(100))\n",
    "    shuffle_index = np.random.randint(num_inst, size=batch_size)\n",
    "    return X[shuffle_index], y[shuffle_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:27:25.322846Z",
     "start_time": "2018-01-18T15:27:25.319169Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:27:25.680235Z",
     "start_time": "2018-01-18T15:27:25.665447Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_04 = tf.placeholder(dtype=d_dtype, shape=(None, n_feature), name=\"X_04\")\n",
    "y_04 = tf.placeholder(dtype=d_dtype, shape=(None, n_label_04), name=\"y_04\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:27:26.063400Z",
     "start_time": "2018-01-18T15:27:25.978162Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "selu_layer = my_dnn_layer(activation=tf.nn.selu)\n",
    "linear_layer = my_dnn_layer(activation=None)\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = selu_layer(X_04, n_neuron[\"n_hidden1\"], name=\"hidden1\")\n",
    "    hidden2 = selu_layer(hidden1, n_neuron[\"n_hidden2\"], name=\"hidden2\")\n",
    "    hidden3 = selu_layer(hidden2, n_neuron[\"n_hidden3\"], name=\"hidden3\")\n",
    "    hidden4 = selu_layer(hidden3, n_neuron[\"n_hidden4\"], name=\"hidden4\")\n",
    "    hidden5 = selu_layer(hidden4, n_neuron[\"n_hidden5\"], name=\"hidden5\")\n",
    "    logits = linear_layer(hidden5, n_label_04, name=\"logits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:27:26.351684Z",
     "start_time": "2018-01-18T15:27:26.309998Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.softmax_cross_entropy_with_logits(labels=y_04, logits=logits, name=\"xentropy\")\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:27:26.627796Z",
     "start_time": "2018-01-18T15:27:26.403645Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"train\"):\n",
    "    adam_optimizer = tf.train.AdamOptimizer()\n",
    "    train_op = adam_optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:27:27.059868Z",
     "start_time": "2018-01-18T15:27:27.033761Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    true_labels = tf.argmax(y_04, axis=1)\n",
    "    y_pred = tf.argmax(logits, axis=1)\n",
    "    all_acc, acc = tf.metrics.accuracy(labels=true_labels, predictions=y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:27:27.267655Z",
     "start_time": "2018-01-18T15:27:27.110576Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "time_now = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "base_dir = os.path.join(\"temp\", str(iter_ind))\n",
    "save_dir = os.path.join(base_dir, time_now, \"saves\")\n",
    "log_dir = os.path.join(base_dir, time_now, \"logs\")\n",
    "\n",
    "with tf.name_scope(\"savNlog\"):\n",
    "    saver = tf.train.Saver()\n",
    "    file_writer = tf.summary.FileWriter(log_dir) # Need to add the graph\n",
    "    train_acc_log = tf.summary.scalar(\"train_accuracy\", acc)\n",
    "    validation_acc_log = tf.summary.scalar(\"validation_accuracy\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:27:27.397440Z",
     "start_time": "2018-01-18T15:27:27.390490Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"initializer\"):\n",
    "    inits = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:30:06.097853Z",
     "start_time": "2018-01-18T15:27:27.762542Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5  Training accuracy: 1.0000, \n",
      "              Training loss: 0.002594\n",
      "                  Validation accuracy: 0.9880\n",
      "                      Validation loss: 0.047893\n",
      "0 0\n",
      "epoch: 10 Training accuracy: 0.9888, \n",
      "              Training loss: 0.000118\n",
      "                  Validation accuracy: 0.9898\n",
      "                      Validation loss: 0.054172\n",
      "0 1\n",
      "epoch: 15 Training accuracy: 0.9902, \n",
      "              Training loss: 0.001589\n",
      "                  Validation accuracy: 0.9911\n",
      "                      Validation loss: 0.044657\n",
      "0 0\n",
      "epoch: 20 Training accuracy: 0.9913, \n",
      "              Training loss: 0.000295\n",
      "                  Validation accuracy: 0.9912\n",
      "                      Validation loss: 0.054318\n",
      "0 1\n",
      "epoch: 25 Training accuracy: 0.9914, \n",
      "              Training loss: 0.000024\n",
      "                  Validation accuracy: 0.9910\n",
      "                      Validation loss: 0.055939\n",
      "1 2\n",
      "epoch: 30 Training accuracy: 0.9911, \n",
      "              Training loss: 0.005227\n",
      "                  Validation accuracy: 0.9907\n",
      "                      Validation loss: 0.083740\n",
      "2 3\n",
      "epoch: 35 Training accuracy: 0.9908, \n",
      "              Training loss: 0.000007\n",
      "                  Validation accuracy: 0.9910\n",
      "                      Validation loss: 0.042891\n",
      "3 0\n",
      "epoch: 40 Training accuracy: 0.9911, \n",
      "              Training loss: 0.000000\n",
      "                  Validation accuracy: 0.9912\n",
      "                      Validation loss: 0.043432\n",
      "4 1\n",
      "epoch: 45 Training accuracy: 0.9913, \n",
      "              Training loss: 0.000004\n",
      "                  Validation accuracy: 0.9914\n",
      "                      Validation loss: 0.044455\n",
      "0 2\n",
      "epoch: 50 Training accuracy: 0.9915, \n",
      "              Training loss: 0.000000\n",
      "                  Validation accuracy: 0.9916\n",
      "                      Validation loss: 0.045661\n",
      "0 3\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(inits)\n",
    "    \n",
    "    file_writer.add_graph(tf.get_default_graph())\n",
    "    \n",
    "    max_val_loss = np.inf\n",
    "    count_drop_loss = 0\n",
    "    best_model_loss = None\n",
    "    \n",
    "    max_val_acc = 0\n",
    "    count_drop_acc = 0\n",
    "    best_model_acc = None\n",
    "    \n",
    "    for epoch in range(1, n_epoch+1):\n",
    "        for batch in range(1, np.ceil(train_size/batch_size).astype(int)+1):\n",
    "            \n",
    "            X_batch, y_batch =  get_batch(X_train_04, y_train_04, batch_size, epoch, batch)\n",
    "            \n",
    "            sess.run(train_op, feed_dict={X_04: X_batch, y_04: y_batch})\n",
    "        if not epoch % log_freq:\n",
    "            train_acc, log_train, train_loss = sess.run([acc, train_acc_log, loss], \n",
    "                                            feed_dict={X_04: X_batch, y_04: y_batch})\n",
    "            val_acc, log_val, val_loss = sess.run([acc, validation_acc_log, loss],\n",
    "                                        feed_dict={X_04: X_valid_04, y_04: y_valid_04})\n",
    "            file_writer.add_summary(log_train, epoch)\n",
    "            file_writer.add_summary(log_val, epoch)\n",
    "            \n",
    "            print(\"epoch: {:<3}Training accuracy: {:<6.4f}, \\n              Training loss: {:<6.6f}\\n\\\n",
    "                  Validation accuracy: {:<6.4f}\\n                      Validation loss: {:<6.6f}\"\n",
    "                  .format(epoch, train_acc, train_loss, val_acc, val_loss))\n",
    "\n",
    "            save_file_path = saver.save(sess, os.path.join(save_dir, \"{}.ckpt\".format(epoch//log_freq)))\n",
    "            \n",
    "            if val_loss < max_val_loss:\n",
    "                max_val_loss = val_loss\n",
    "                best_model_loss = save_file_path\n",
    "                count_drop_loss = 0\n",
    "            else:\n",
    "                count_drop_loss += 1\n",
    "\n",
    "\n",
    "            if val_acc > max_val_acc:\n",
    "                max_val_acc = val_acc\n",
    "                best_model_acc = save_file_path\n",
    "                count_drop_acc = 0\n",
    "            else:\n",
    "                count_drop_acc += 1\n",
    "            if count_drop_acc > max_drop and count_drop_loss > max_drop:\n",
    "                break\n",
    "            print(count_drop_acc, count_drop_loss)\n",
    "                \n",
    "    file_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the result on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:30:06.909601Z",
     "start_time": "2018-01-18T15:30:06.589187Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from temp/3/20180119022727/saves/10.ckpt\n",
      "Accuracy for test set is: 0.9940\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    inits.run()\n",
    "    saver.restore(sess, best_model_acc)\n",
    "    acc_test = sess.run(acc, feed_dict={X_04: X_test_04, y_04: y_test_04})\n",
    "    print(\"Accuracy for test set is: {:>4.4f}\".format(acc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:30:07.888343Z",
     "start_time": "2018-01-18T15:30:07.658117Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from temp/3/20180119022727/saves/7.ckpt\n",
      "Accuracy for test set is: 0.9942\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    inits.run()\n",
    "    saver.restore(sess, best_model_loss)\n",
    "    acc_test = sess.run(acc, feed_dict={X_04: X_test_04, y_04: y_test_04})\n",
    "    print(\"Accuracy for test set is: {:>4.4f}\".format(acc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:30:07.888343Z",
     "start_time": "2018-01-18T15:30:07.658117Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from temp/3/20180119022727/saves/10.ckpt\n",
      "Accuracy for test set is: 0.9940\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    inits.run()\n",
    "    saver.restore(sess, save_file_path)\n",
    "    acc_test = sess.run(acc, feed_dict={X_04: X_test_04, y_04: y_test_04})\n",
    "    print(\"Accuracy for test set is: {:>4.4f}\".format(acc_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\n",
    "## Fourth iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:45:37.127395Z",
     "start_time": "2018-01-18T15:45:37.123729Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iter_ind = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Strong>Change</Strong>\n",
    "<ol>\n",
    "    <li>Adding BN\n",
    "    </li>\n",
    "<!--     <li>Adding early stopping\n",
    "    </li> -->\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define some variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:52:41.883654Z",
     "start_time": "2018-01-18T15:52:41.873550Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_size = mnist.train.num_examples\n",
    "n_epoch = 100\n",
    "batch_size = 200\n",
    "max_drop = 5\n",
    "\n",
    "n_feature = mnist.train.images.shape[1]\n",
    "n_neuron = {\"n_hidden{}\".format(i) : 100 for i in range(6)}\n",
    "n_label = mnist.train.labels.shape[1]\n",
    "n_label_04 = 5\n",
    "\n",
    "d_dtype=tf.float64\n",
    "\n",
    "log_freq = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define some common functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:52:42.457709Z",
     "start_time": "2018-01-18T15:52:42.453116Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def my_dnn_layer(**key_args):\n",
    "    return partial(tf.layers.dense, **key_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:52:42.788473Z",
     "start_time": "2018-01-18T15:52:42.783378Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_batch_norm(**key_args):\n",
    "    return partial(tf.layers.batch_normalization, **key_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:52:42.980388Z",
     "start_time": "2018-01-18T15:52:42.974047Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(X, y, batch_size, epoch, batch):\n",
    "    num_inst = X.shape[0]\n",
    "    np.random.seed(epoch * batch * np.random.randint(100))\n",
    "    shuffle_index = np.random.randint(num_inst, size=batch_size)\n",
    "    return X[shuffle_index], y[shuffle_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:52:43.583972Z",
     "start_time": "2018-01-18T15:52:43.579232Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:52:43.600903Z",
     "start_time": "2018-01-18T15:52:43.592565Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_04 = tf.placeholder(dtype=d_dtype, shape=(None, n_feature), name=\"X_04\")\n",
    "y_04 = tf.placeholder(dtype=d_dtype, shape=(None, n_label_04), name=\"y_04\")\n",
    "training = tf.placeholder_with_default(False, None, name=\"training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:52:44.524207Z",
     "start_time": "2018-01-18T15:52:44.084202Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "selu_layer = my_dnn_layer(activation=tf.nn.selu)\n",
    "linear_layer = my_dnn_layer(activation=None)\n",
    "\n",
    "batch_layer = my_batch_norm(training=training)\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = selu_layer(X_04, n_neuron[\"n_hidden1\"], name=\"hidden1\")\n",
    "    bn1 = batch_layer(hidden1, name=\"bn1\")\n",
    "    hidden2 = selu_layer(bn1, n_neuron[\"n_hidden2\"], name=\"hidden2\")\n",
    "    bn2 = batch_layer(hidden2, name=\"bn2\")\n",
    "    hidden3 = selu_layer(bn2, n_neuron[\"n_hidden3\"], name=\"hidden3\")\n",
    "    bn3 = batch_layer(hidden3, name=\"bn3\")\n",
    "    hidden4 = selu_layer(bn3, n_neuron[\"n_hidden4\"], name=\"hidden4\")\n",
    "    bn4 = batch_layer(hidden4, name=\"bn4\")\n",
    "    hidden5 = selu_layer(bn4, n_neuron[\"n_hidden5\"], name=\"hidden5\")\n",
    "    bn5 = batch_layer(hidden5, name=\"bn5\")\n",
    "    logits = linear_layer(bn5, n_label_04, name=\"logits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:52:44.683806Z",
     "start_time": "2018-01-18T15:52:44.653379Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.softmax_cross_entropy_with_logits(labels=y_04, logits=logits, name=\"xentropy\")\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:52:45.885184Z",
     "start_time": "2018-01-18T15:52:45.165646Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"train\"):\n",
    "    batch_updates = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    adam_optimizer = tf.train.AdamOptimizer()\n",
    "    train_op = adam_optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:52:46.151431Z",
     "start_time": "2018-01-18T15:52:46.128725Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    true_labels = tf.argmax(y_04, axis=1)\n",
    "    y_pred = tf.argmax(logits, axis=1)\n",
    "    all_acc, acc = tf.metrics.accuracy(labels=true_labels, predictions=y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:52:46.602934Z",
     "start_time": "2018-01-18T15:52:46.413196Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "time_now = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "base_dir = os.path.join(\"temp\", str(iter_ind))\n",
    "save_dir = os.path.join(base_dir, time_now, \"saves\")\n",
    "log_dir = os.path.join(base_dir, time_now, \"logs\")\n",
    "\n",
    "with tf.name_scope(\"savNlog\"):\n",
    "    saver = tf.train.Saver(max_to_keep=20)\n",
    "    file_writer = tf.summary.FileWriter(log_dir) # Need to add the graph\n",
    "    train_acc_log = tf.summary.scalar(\"train_accuracy\", acc)\n",
    "    validation_acc_log = tf.summary.scalar(\"validation_accuracy\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:52:46.884447Z",
     "start_time": "2018-01-18T15:52:46.876134Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"initializer\"):\n",
    "    inits = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T16:03:03.640986Z",
     "start_time": "2018-01-18T15:52:47.128070Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5  Training accuracy: 0.9950, \n",
      "              Training loss: 0.005974\n",
      "                  Validation accuracy: 0.9902\n",
      "                      Validation loss: 0.033956\n",
      "0 0\n",
      "epoch: 10 Training accuracy: 0.9909, \n",
      "              Training loss: 0.004205\n",
      "                  Validation accuracy: 0.9898\n",
      "                      Validation loss: 0.036859\n",
      "1 1\n",
      "epoch: 15 Training accuracy: 0.9902, \n",
      "              Training loss: 0.001278\n",
      "                  Validation accuracy: 0.9908\n",
      "                      Validation loss: 0.045906\n",
      "0 2\n",
      "epoch: 20 Training accuracy: 0.9910, \n",
      "              Training loss: 0.000021\n",
      "                  Validation accuracy: 0.9913\n",
      "                      Validation loss: 0.041380\n",
      "0 3\n",
      "epoch: 25 Training accuracy: 0.9915, \n",
      "              Training loss: 0.000008\n",
      "                  Validation accuracy: 0.9918\n",
      "                      Validation loss: 0.036323\n",
      "0 4\n",
      "epoch: 30 Training accuracy: 0.9919, \n",
      "              Training loss: 0.000004\n",
      "                  Validation accuracy: 0.9922\n",
      "                      Validation loss: 0.037636\n",
      "0 5\n",
      "epoch: 35 Training accuracy: 0.9923, \n",
      "              Training loss: 0.000001\n",
      "                  Validation accuracy: 0.9925\n",
      "                      Validation loss: 0.038828\n",
      "0 6\n",
      "epoch: 40 Training accuracy: 0.9926, \n",
      "              Training loss: 0.000002\n",
      "                  Validation accuracy: 0.9927\n",
      "                      Validation loss: 0.040459\n",
      "0 7\n",
      "epoch: 45 Training accuracy: 0.9928, \n",
      "              Training loss: 0.000001\n",
      "                  Validation accuracy: 0.9929\n",
      "                      Validation loss: 0.041929\n",
      "0 8\n",
      "epoch: 50 Training accuracy: 0.9929, \n",
      "              Training loss: 0.000000\n",
      "                  Validation accuracy: 0.9930\n",
      "                      Validation loss: 0.044197\n",
      "0 9\n",
      "epoch: 55 Training accuracy: 0.9930, \n",
      "              Training loss: 0.000000\n",
      "                  Validation accuracy: 0.9930\n",
      "                      Validation loss: 0.045809\n",
      "0 10\n",
      "epoch: 60 Training accuracy: 0.9931, \n",
      "              Training loss: 0.000000\n",
      "                  Validation accuracy: 0.9931\n",
      "                      Validation loss: 0.048417\n",
      "0 11\n",
      "epoch: 65 Training accuracy: 0.9932, \n",
      "              Training loss: 0.000000\n",
      "                  Validation accuracy: 0.9932\n",
      "                      Validation loss: 0.050597\n",
      "0 12\n",
      "epoch: 70 Training accuracy: 0.9932, \n",
      "              Training loss: 0.000000\n",
      "                  Validation accuracy: 0.9932\n",
      "                      Validation loss: 0.052825\n",
      "0 13\n",
      "epoch: 75 Training accuracy: 0.9932, \n",
      "              Training loss: 0.000000\n",
      "                  Validation accuracy: 0.9933\n",
      "                      Validation loss: 0.055474\n",
      "0 14\n",
      "epoch: 80 Training accuracy: 0.9933, \n",
      "              Training loss: 0.000000\n",
      "                  Validation accuracy: 0.9933\n",
      "                      Validation loss: 0.058049\n",
      "0 15\n",
      "epoch: 85 Training accuracy: 0.9933, \n",
      "              Training loss: 0.000000\n",
      "                  Validation accuracy: 0.9934\n",
      "                      Validation loss: 0.060739\n",
      "0 16\n",
      "epoch: 90 Training accuracy: 0.9934, \n",
      "              Training loss: 0.000000\n",
      "                  Validation accuracy: 0.9934\n",
      "                      Validation loss: 0.062802\n",
      "0 17\n",
      "epoch: 95 Training accuracy: 0.9935, \n",
      "              Training loss: 0.000000\n",
      "                  Validation accuracy: 0.9935\n",
      "                      Validation loss: 0.065059\n",
      "0 18\n",
      "epoch: 100Training accuracy: 0.9935, \n",
      "              Training loss: 0.000000\n",
      "                  Validation accuracy: 0.9935\n",
      "                      Validation loss: 0.067404\n",
      "0 19\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(inits)\n",
    "    \n",
    "    file_writer.add_graph(tf.get_default_graph())\n",
    "    \n",
    "    max_val_loss = np.inf\n",
    "    count_drop_loss = 0\n",
    "    best_model_loss = None\n",
    "    \n",
    "    max_val_acc = 0\n",
    "    count_drop_acc = 0\n",
    "    best_model_acc = None\n",
    "    \n",
    "    for epoch in range(1, n_epoch+1):\n",
    "        for batch in range(1, np.ceil(train_size/batch_size).astype(int)+1):\n",
    "            \n",
    "            X_batch, y_batch =  get_batch(X_train_04, y_train_04, batch_size, epoch, batch)\n",
    "            \n",
    "            sess.run([train_op, batch_updates], feed_dict={X_04: X_batch, y_04: y_batch})\n",
    "        if not epoch % log_freq:\n",
    "            train_acc, log_train, train_loss = sess.run([acc, train_acc_log, loss], \n",
    "                                            feed_dict={X_04: X_batch, y_04: y_batch})\n",
    "            val_acc, log_val, val_loss = sess.run([acc, validation_acc_log, loss],\n",
    "                                        feed_dict={X_04: X_valid_04, y_04: y_valid_04})\n",
    "            file_writer.add_summary(log_train, epoch)\n",
    "            file_writer.add_summary(log_val, epoch)\n",
    "            \n",
    "            print(\"epoch: {:<3}Training accuracy: {:<6.4f}, \\n              Training loss: {:<6.6f}\\n\\\n",
    "                  Validation accuracy: {:<6.4f}\\n                      Validation loss: {:<6.6f}\"\n",
    "                  .format(epoch, train_acc, train_loss, val_acc, val_loss))\n",
    "\n",
    "            save_file_path = saver.save(sess, os.path.join(save_dir, \"{}.ckpt\".format(epoch//log_freq)))\n",
    "            \n",
    "            if val_loss < max_val_loss:\n",
    "                max_val_loss = val_loss\n",
    "                best_model_loss = save_file_path\n",
    "                count_drop_loss = 0\n",
    "            else:\n",
    "                count_drop_loss += 1\n",
    "\n",
    "\n",
    "            if val_acc > max_val_acc:\n",
    "                max_val_acc = val_acc\n",
    "                best_model_acc = save_file_path\n",
    "                count_drop_acc = 0\n",
    "            else:\n",
    "                count_drop_acc += 1\n",
    "            if count_drop_acc > max_drop and count_drop_loss > max_drop:\n",
    "                break\n",
    "            print(count_drop_acc, count_drop_loss)\n",
    "                \n",
    "    file_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the result on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T16:03:04.722625Z",
     "start_time": "2018-01-18T16:03:04.010731Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from temp/4/20180119025246/saves/20.ckpt\n",
      "Accuracy for test set is: 0.9946\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    inits.run()\n",
    "    saver.restore(sess, best_model_acc)\n",
    "    acc_test = sess.run(acc, feed_dict={X_04: X_test_04, y_04: y_test_04})\n",
    "    print(\"Accuracy for test set is: {:>4.4f}\".format(acc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T16:06:27.486133Z",
     "start_time": "2018-01-18T16:06:26.995012Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/apple/AnacondaProjects/handson-ml/chapter_11_exer/temp/4/20180119025246/saves/16.ckpt\n",
      "Accuracy for test set is: 0.9944\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    inits.run()\n",
    "    saver.restore(sess, best_model_loss)\n",
    "    acc_test = sess.run(acc, feed_dict={X_04: X_test_04, y_04: y_test_04})\n",
    "    print(\"Accuracy for test set is: {:>4.4f}\".format(acc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T16:06:28.940866Z",
     "start_time": "2018-01-18T16:06:28.446732Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from temp/4/20180119025246/saves/20.ckpt\n",
      "Accuracy for test set is: 0.9946\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    inits.run()\n",
    "    saver.restore(sess, save_file_path)\n",
    "    acc_test = sess.run(acc, feed_dict={X_04: X_test_04, y_04: y_test_04})\n",
    "    print(\"Accuracy for test set is: {:>4.4f}\".format(acc_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\n",
    "## Fifth iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:45:37.127395Z",
     "start_time": "2018-01-18T15:45:37.123729Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iter_ind = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Strong>Change</Strong>\n",
    "<ol>\n",
    "    <li>Parsing the structure into a sklearn func\n",
    "    </li>\n",
    "    <li>Changing the structure of saving file\n",
    "    </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define some variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:52:41.883654Z",
     "start_time": "2018-01-18T15:52:41.873550Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_size = mnist.train.num_examples\n",
    "n_epoch = 100\n",
    "batch_size = 200\n",
    "max_drop = 5\n",
    "\n",
    "n_feature = mnist.train.images.shape[1]\n",
    "n_neuron = {\"n_hidden{}\".format(i) : 100 for i in range(6)}\n",
    "n_label = mnist.train.labels.shape[1]\n",
    "n_label_04 = 5\n",
    "\n",
    "d_dtype=tf.float64\n",
    "\n",
    "log_freq = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define some common functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:52:42.457709Z",
     "start_time": "2018-01-18T15:52:42.453116Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def my_dnn_layer(**key_args):\n",
    "    return partial(tf.layers.dense, **key_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:52:42.788473Z",
     "start_time": "2018-01-18T15:52:42.783378Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_batch_norm(**key_args):\n",
    "    return partial(tf.layers.batch_normalization, **key_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:52:42.980388Z",
     "start_time": "2018-01-18T15:52:42.974047Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(X, y, batch_size, epoch, batch):\n",
    "    num_inst = X.shape[0]\n",
    "    np.random.seed(epoch * batch * np.random.randint(100))\n",
    "    shuffle_index = np.random.randint(num_inst, size=batch_size)\n",
    "    return X[shuffle_index], y[shuffle_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:52:43.583972Z",
     "start_time": "2018-01-18T15:52:43.579232Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:52:43.600903Z",
     "start_time": "2018-01-18T15:52:43.592565Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_04 = tf.placeholder(dtype=d_dtype, shape=(None, n_feature), name=\"X_04\")\n",
    "y_04 = tf.placeholder(dtype=d_dtype, shape=(None, n_label_04), name=\"y_04\")\n",
    "training = tf.placeholder_with_default(False, None, name=\"training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:52:44.524207Z",
     "start_time": "2018-01-18T15:52:44.084202Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "selu_layer = my_dnn_layer(activation=tf.nn.selu)\n",
    "linear_layer = my_dnn_layer(activation=None)\n",
    "\n",
    "batch_layer = my_batch_norm(training=training)\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = selu_layer(X_04, n_neuron[\"n_hidden1\"], name=\"hidden1\")\n",
    "    bn1 = batch_layer(hidden1, name=\"bn1\")\n",
    "    hidden2 = selu_layer(bn1, n_neuron[\"n_hidden2\"], name=\"hidden2\")\n",
    "    bn2 = batch_layer(hidden2, name=\"bn2\")\n",
    "    hidden3 = selu_layer(bn2, n_neuron[\"n_hidden3\"], name=\"hidden3\")\n",
    "    bn3 = batch_layer(hidden3, name=\"bn3\")\n",
    "    hidden4 = selu_layer(bn3, n_neuron[\"n_hidden4\"], name=\"hidden4\")\n",
    "    bn4 = batch_layer(hidden4, name=\"bn4\")\n",
    "    hidden5 = selu_layer(bn4, n_neuron[\"n_hidden5\"], name=\"hidden5\")\n",
    "    bn5 = batch_layer(hidden5, name=\"bn5\")\n",
    "    logits = linear_layer(bn5, n_label_04, name=\"logits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:52:44.683806Z",
     "start_time": "2018-01-18T15:52:44.653379Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.softmax_cross_entropy_with_logits(labels=y_04, logits=logits, name=\"xentropy\")\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:52:45.885184Z",
     "start_time": "2018-01-18T15:52:45.165646Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"train\"):\n",
    "    batch_updates = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    adam_optimizer = tf.train.AdamOptimizer()\n",
    "    train_op = adam_optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:52:46.151431Z",
     "start_time": "2018-01-18T15:52:46.128725Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    true_labels = tf.argmax(y_04, axis=1)\n",
    "    y_pred = tf.argmax(logits, axis=1)\n",
    "    all_acc, acc = tf.metrics.accuracy(labels=true_labels, predictions=y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:52:46.602934Z",
     "start_time": "2018-01-18T15:52:46.413196Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "time_now = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "base_dir = os.path.join(\"temp\", str(iter_ind))\n",
    "save_dir = os.path.join(base_dir, \"saves\", time_now)\n",
    "log_dir = os.path.join(base_dir, \"logs\", time_now)\n",
    "\n",
    "with tf.name_scope(\"savNlog\"):\n",
    "    saver = tf.train.Saver(max_to_keep=20)\n",
    "    file_writer = tf.summary.FileWriter(log_dir) # Need to add the graph\n",
    "    train_acc_log = tf.summary.scalar(\"train_accuracy\", acc)\n",
    "    validation_acc_log = tf.summary.scalar(\"validation_accuracy\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:52:46.884447Z",
     "start_time": "2018-01-18T15:52:46.876134Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"initializer\"):\n",
    "    inits = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T16:03:03.640986Z",
     "start_time": "2018-01-18T15:52:47.128070Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5  Training accuracy: 0.9950, \n",
      "              Training loss: 0.005974\n",
      "                  Validation accuracy: 0.9902\n",
      "                      Validation loss: 0.033956\n",
      "0 0\n",
      "epoch: 10 Training accuracy: 0.9909, \n",
      "              Training loss: 0.004205\n",
      "                  Validation accuracy: 0.9898\n",
      "                      Validation loss: 0.036859\n",
      "1 1\n",
      "epoch: 15 Training accuracy: 0.9902, \n",
      "              Training loss: 0.001278\n",
      "                  Validation accuracy: 0.9908\n",
      "                      Validation loss: 0.045906\n",
      "0 2\n",
      "epoch: 20 Training accuracy: 0.9910, \n",
      "              Training loss: 0.000021\n",
      "                  Validation accuracy: 0.9913\n",
      "                      Validation loss: 0.041380\n",
      "0 3\n",
      "epoch: 25 Training accuracy: 0.9915, \n",
      "              Training loss: 0.000008\n",
      "                  Validation accuracy: 0.9918\n",
      "                      Validation loss: 0.036323\n",
      "0 4\n",
      "epoch: 30 Training accuracy: 0.9919, \n",
      "              Training loss: 0.000004\n",
      "                  Validation accuracy: 0.9922\n",
      "                      Validation loss: 0.037636\n",
      "0 5\n",
      "epoch: 35 Training accuracy: 0.9923, \n",
      "              Training loss: 0.000001\n",
      "                  Validation accuracy: 0.9925\n",
      "                      Validation loss: 0.038828\n",
      "0 6\n",
      "epoch: 40 Training accuracy: 0.9926, \n",
      "              Training loss: 0.000002\n",
      "                  Validation accuracy: 0.9927\n",
      "                      Validation loss: 0.040459\n",
      "0 7\n",
      "epoch: 45 Training accuracy: 0.9928, \n",
      "              Training loss: 0.000001\n",
      "                  Validation accuracy: 0.9929\n",
      "                      Validation loss: 0.041929\n",
      "0 8\n",
      "epoch: 50 Training accuracy: 0.9929, \n",
      "              Training loss: 0.000000\n",
      "                  Validation accuracy: 0.9930\n",
      "                      Validation loss: 0.044197\n",
      "0 9\n",
      "epoch: 55 Training accuracy: 0.9930, \n",
      "              Training loss: 0.000000\n",
      "                  Validation accuracy: 0.9930\n",
      "                      Validation loss: 0.045809\n",
      "0 10\n",
      "epoch: 60 Training accuracy: 0.9931, \n",
      "              Training loss: 0.000000\n",
      "                  Validation accuracy: 0.9931\n",
      "                      Validation loss: 0.048417\n",
      "0 11\n",
      "epoch: 65 Training accuracy: 0.9932, \n",
      "              Training loss: 0.000000\n",
      "                  Validation accuracy: 0.9932\n",
      "                      Validation loss: 0.050597\n",
      "0 12\n",
      "epoch: 70 Training accuracy: 0.9932, \n",
      "              Training loss: 0.000000\n",
      "                  Validation accuracy: 0.9932\n",
      "                      Validation loss: 0.052825\n",
      "0 13\n",
      "epoch: 75 Training accuracy: 0.9932, \n",
      "              Training loss: 0.000000\n",
      "                  Validation accuracy: 0.9933\n",
      "                      Validation loss: 0.055474\n",
      "0 14\n",
      "epoch: 80 Training accuracy: 0.9933, \n",
      "              Training loss: 0.000000\n",
      "                  Validation accuracy: 0.9933\n",
      "                      Validation loss: 0.058049\n",
      "0 15\n",
      "epoch: 85 Training accuracy: 0.9933, \n",
      "              Training loss: 0.000000\n",
      "                  Validation accuracy: 0.9934\n",
      "                      Validation loss: 0.060739\n",
      "0 16\n",
      "epoch: 90 Training accuracy: 0.9934, \n",
      "              Training loss: 0.000000\n",
      "                  Validation accuracy: 0.9934\n",
      "                      Validation loss: 0.062802\n",
      "0 17\n",
      "epoch: 95 Training accuracy: 0.9935, \n",
      "              Training loss: 0.000000\n",
      "                  Validation accuracy: 0.9935\n",
      "                      Validation loss: 0.065059\n",
      "0 18\n",
      "epoch: 100Training accuracy: 0.9935, \n",
      "              Training loss: 0.000000\n",
      "                  Validation accuracy: 0.9935\n",
      "                      Validation loss: 0.067404\n",
      "0 19\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(inits)\n",
    "    \n",
    "    file_writer.add_graph(tf.get_default_graph())\n",
    "    \n",
    "    max_val_loss = np.inf\n",
    "    count_drop_loss = 0\n",
    "    best_model_loss = None\n",
    "    \n",
    "    max_val_acc = 0\n",
    "    count_drop_acc = 0\n",
    "    best_model_acc = None\n",
    "    \n",
    "    for epoch in range(1, n_epoch+1):\n",
    "        for batch in range(1, np.ceil(train_size/batch_size).astype(int)+1):\n",
    "            \n",
    "            X_batch, y_batch =  get_batch(X_train_04, y_train_04, batch_size, epoch, batch)\n",
    "            \n",
    "            sess.run([train_op, batch_updates], feed_dict={X_04: X_batch, y_04: y_batch})\n",
    "        if not epoch % log_freq:\n",
    "            train_acc, log_train, train_loss = sess.run([acc, train_acc_log, loss], \n",
    "                                            feed_dict={X_04: X_batch, y_04: y_batch})\n",
    "            val_acc, log_val, val_loss = sess.run([acc, validation_acc_log, loss],\n",
    "                                        feed_dict={X_04: X_valid_04, y_04: y_valid_04})\n",
    "            file_writer.add_summary(log_train, epoch)\n",
    "            file_writer.add_summary(log_val, epoch)\n",
    "            \n",
    "            print(\"epoch: {:<3}Training accuracy: {:<6.4f}, \\n              Training loss: {:<6.6f}\\n\\\n",
    "                  Validation accuracy: {:<6.4f}\\n                      Validation loss: {:<6.6f}\"\n",
    "                  .format(epoch, train_acc, train_loss, val_acc, val_loss))\n",
    "\n",
    "            save_file_path = saver.save(sess, os.path.join(save_dir, \"{}.ckpt\".format(epoch//log_freq)))\n",
    "            \n",
    "            if val_loss < max_val_loss:\n",
    "                max_val_loss = val_loss\n",
    "                best_model_loss = save_file_path\n",
    "                count_drop_loss = 0\n",
    "            else:\n",
    "                count_drop_loss += 1\n",
    "\n",
    "\n",
    "            if val_acc > max_val_acc:\n",
    "                max_val_acc = val_acc\n",
    "                best_model_acc = save_file_path\n",
    "                count_drop_acc = 0\n",
    "            else:\n",
    "                count_drop_acc += 1\n",
    "            if count_drop_acc > max_drop and count_drop_loss > max_drop:\n",
    "                break\n",
    "            print(count_drop_acc, count_drop_loss)\n",
    "                \n",
    "    file_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.exceptions import NotFittedError\n",
    "\n",
    "class DNNClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, n_hidden_layers=5, n_neurons=100, optimizer_class=tf.train.AdamOptimizer,\n",
    "                 learning_rate=0.01, batch_size=50, activation=tf.nn.elu, initializer=None,\n",
    "                 batch_norm=None, dropout_rate=None, random_state=None):\n",
    "        self.n_hidden_layers = n_hidden_layers\n",
    "        self.n_neurons = n_neurons\n",
    "        self.optimizer_class = optimizer_class\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.activation = activation\n",
    "        self.initializer= initializer\n",
    "        self.batch_norm = batch_norm\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.random_state = random_state\n",
    "        self._session = None\n",
    "        self._graph = None\n",
    "        self._training = tf.placeholder_with_default(False, None, name=\"training\")\n",
    "        \n",
    "    def _dnn_layout(self):\n",
    "        if type(self.n_neurons) is int:\n",
    "            self._n_neurons = (self.n_neurons for i in range(self.n_hidden_layers))\n",
    "        elif type(self.n_neurons) is list:\n",
    "            self._n_neurons = iter(self.n_neurons)\n",
    "        else:\n",
    "            raise InvalidNumberNeurons(\"The number of neurons given must be integer or list\")\n",
    "        with tf.name_scope(\"dnn\"):\n",
    "            for lay in range(self.n_hidden_layers):\n",
    "                if self.dropout_rate:\n",
    "                    inputs = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the result on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T16:03:04.722625Z",
     "start_time": "2018-01-18T16:03:04.010731Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from temp/4/20180119025246/saves/20.ckpt\n",
      "Accuracy for test set is: 0.9946\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    inits.run()\n",
    "    saver.restore(sess, best_model_acc)\n",
    "    acc_test = sess.run(acc, feed_dict={X_04: X_test_04, y_04: y_test_04})\n",
    "    print(\"Accuracy for test set is: {:>4.4f}\".format(acc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T16:06:27.486133Z",
     "start_time": "2018-01-18T16:06:26.995012Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/apple/AnacondaProjects/handson-ml/chapter_11_exer/temp/4/20180119025246/saves/16.ckpt\n",
      "Accuracy for test set is: 0.9944\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    inits.run()\n",
    "    saver.restore(sess, best_model_loss)\n",
    "    acc_test = sess.run(acc, feed_dict={X_04: X_test_04, y_04: y_test_04})\n",
    "    print(\"Accuracy for test set is: {:>4.4f}\".format(acc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T16:06:28.940866Z",
     "start_time": "2018-01-18T16:06:28.446732Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from temp/4/20180119025246/saves/20.ckpt\n",
      "Accuracy for test set is: 0.9946\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    inits.run()\n",
    "    saver.restore(sess, save_file_path)\n",
    "    acc_test = sess.run(acc, feed_dict={X_04: X_test_04, y_04: y_test_04})\n",
    "    print(\"Accuracy for test set is: {:>4.4f}\".format(acc_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 483,
   "position": {
    "height": "505px",
    "left": "1542px",
    "right": "20px",
    "top": "67px",
    "width": "334px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
