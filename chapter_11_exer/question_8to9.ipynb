{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <i>Question8</i> --Training for digit 0 to 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Load framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-20T04:20:28.348408Z",
     "start_time": "2018-01-20T04:20:23.551841Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from datetime import datetime\n",
    "import os\n",
    "from time import clock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Set up matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-20T04:20:30.174486Z",
     "start_time": "2018-01-20T04:20:28.459563Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-20T04:20:32.276943Z",
     "start_time": "2018-01-20T04:20:30.251511Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "X_train = mnist.train.images\n",
    "y_train = mnist.train.labels\n",
    "X_test = mnist.test.images\n",
    "y_test = mnist.test.labels\n",
    "X_valid = mnist.validation.images\n",
    "y_valid = mnist.validation.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-20T04:20:32.618207Z",
     "start_time": "2018-01-20T04:20:32.288105Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ind_04_train = (np.argmax(y_train, axis=1) <= 4)\n",
    "ind_04_test = (np.argmax(y_test, axis=1) <= 4)\n",
    "ind_04_valid = (np.argmax(y_valid, axis=1) <= 4)\n",
    "X_train_04 = X_train[ind_04_train]\n",
    "y_train_04 = np.delete(y_train[ind_04_train], [5,6,7,8,9], axis=1)\n",
    "X_test_04 = X_test[ind_04_test]\n",
    "y_test_04 = np.delete(y_test[ind_04_test], [5,6,7,8,9], axis=1)\n",
    "X_valid_04 = X_valid[ind_04_valid]\n",
    "y_valid_04 = np.delete(y_valid[ind_04_valid], [5,6,7,8,9], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Check data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-20T04:20:32.816950Z",
     "start_time": "2018-01-20T04:20:32.766768Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55000\n",
      "10000\n",
      "5000\n",
      "(28038, 5)\n"
     ]
    }
   ],
   "source": [
    "print(len(mnist.train.images))\n",
    "print(len(mnist.test.images))\n",
    "print(len(mnist.validation.images))\n",
    "print(y_train_04.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Load a image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-20T04:20:34.705252Z",
     "start_time": "2018-01-20T04:20:33.224487Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAADIZJREFUeJzt3X/oXfV9x/Hn25+RtKChxMXULV2J\nY0PQypcwcA5lRNwoxAaU+sfISFlKULAwdRIEFSnIMrsNhEJKgym02oI/EmSsLTqWDURMpFSr8wcl\na5Pvl2TGgNY/LJr3/viejG/1e8+9ub/OTd7PB4R773nfe86bS17fz7n3nHs+kZlIquecrhuQ1A3D\nLxVl+KWiDL9UlOGXijL8UlGGXyrK8EtFGX6pqPOmubGI8HRCacIyMwZ53kgjf0TcFBFvRMTbEXHv\nKOuSNF0x7Ln9EXEu8CawETgMvATclpmvtbzGkV+asGmM/BuAtzPzl5n5W+AJYNMI65M0RaOEfy3w\n6yWPDzfLfkdEbIuIAxFxYIRtSRqzUb7wW27X4lO79Zm5C9gF7vZLs2SUkf8wcPmSx58H5kdrR9K0\njBL+l4D1EfGFiLgA+CqwbzxtSZq0oXf7M/OjiLgD+DFwLrA7M38xts4kTdTQh/qG2pif+aWJm8pJ\nPpLOXIZfKsrwS0UZfqkowy8VZfilogy/VJThl4oy/FJRhl8qyvBLRRl+qSjDLxVl+KWiDL9UlOGX\nijL8UlGGXyrK8EtFGX6pKMMvFTXVKbql03HFFVe01l944YXW+okTJ3rWrrrqqtbXfvDBB631s4Ej\nv1SU4ZeKMvxSUYZfKsrwS0UZfqkowy8VNdJx/og4BLwPfAx8lJlz42hKAti+fXtr/eKLL26tb968\nuWft/PPPH6qns8k4TvK5ITPfGcN6JE2Ru/1SUaOGP4GfRMTBiNg2joYkTceou/3XZuZ8RKwGfhoR\n/52Z+5c+ofmj4B8GacaMNPJn5nxzewx4GtiwzHN2ZeacXwZKs2Xo8EfEyoj47Kn7wI3Aq+NqTNJk\njbLbfynwdEScWs8PMvPfxtKVpImLzJzexiKmtzHNvHvuuae1/uCDD7bWT5482VpfuXLlafd0NsjM\nGOR5HuqTijL8UlGGXyrK8EtFGX6pKMMvFeWluzVRq1ev7lm75ZZbWl97wQUXtNbvu+++oXrSIkd+\nqSjDLxVl+KWiDL9UlOGXijL8UlGGXyrK4/yaqNtvv71n7Zprrml97fz8fGv9scceG6YlNRz5paIM\nv1SU4ZeKMvxSUYZfKsrwS0UZfqkoj/NrJGvWrGmtb926tWftnHPax5633nqrtb6wsNBaVztHfqko\nwy8VZfilogy/VJThl4oy/FJRhl8qqu9x/ojYDXwZOJaZVzbLVgE/BNYBh4BbM/PE5NrUrGo7jg9w\n2WWX9az1m2J77969Q/WkwQwy8j8G3PSJZfcCz2XmeuC55rGkM0jf8GfmfuDdTyzeBOxp7u8Bbh5z\nX5ImbNjP/Jdm5gJAc9t7TiZJM2ni5/ZHxDZg26S3I+n0DDvyH42INQDN7bFeT8zMXZk5l5lzQ25L\n0gQMG/59wJbm/hbAr2WlM0zf8EfE48ALwB9FxOGI+BrwMLAxIt4CNjaPJZ1BIjOnt7GI6W1MY3HR\nRRe11o8fP95av/DCC3vWdu7c2fra+++/v7X+4YcfttaryswY5Hme4ScVZfilogy/VJThl4oy/FJR\nhl8qykt3q9WOHTta6ytWrGittx1KfuaZZ1pf66G8yXLkl4oy/FJRhl8qyvBLRRl+qSjDLxVl+KWi\nPM5f3IYNG1rrd91110jrf+SRR3rWDhw4MNK6NRpHfqkowy8VZfilogy/VJThl4oy/FJRhl8qykt3\nn+XOO6/9VI5nn322tX7jjTe21o8ePdpan5vrPVHTkSNHWl+r4XjpbkmtDL9UlOGXijL8UlGGXyrK\n8EtFGX6pqL6/54+I3cCXgWOZeWWz7AHgb4H/bZ62IzP/dVJNanhbt25trW/cuLG13u88kH7X9fdY\n/uwaZOR/DLhpmeX/lJlXN/8MvnSG6Rv+zNwPvDuFXiRN0Sif+e+IiJ9HxO6IuGRsHUmaimHD/23g\ni8DVwALQ80JtEbEtIg5EhBdsk2bIUOHPzKOZ+XFmngS+A/S8CmRm7srMuczs/QsPSVM3VPgjYs2S\nh18BXh1PO5KmZZBDfY8D1wOfi4jDwP3A9RFxNZDAIeDrE+xR0gT4e/6zwIoVK3rW3nzzzdbXrl27\ntrW+f//+1voNN9zQWtf0+Xt+Sa0Mv1SU4ZeKMvxSUYZfKsrwS0U5RfdZ4NFHH+1Z63co77XXXmut\nb9q0aaieNPsc+aWiDL9UlOGXijL8UlGGXyrK8EtFGX6pKI/znwFWrVrVWr/uuuuGXvfzzz/fWn/v\nvfeGXrdmmyO/VJThl4oy/FJRhl8qyvBLRRl+qSjDLxXlcf4zwPbt21vr69ev71l74403Wl975513\nDtWTznyO/FJRhl8qyvBLRRl+qSjDLxVl+KWiDL9UVN/j/BFxOfA94PeAk8CuzPyXiFgF/BBYBxwC\nbs3ME5Nrta6HHnqotd42zfoTTzwx7nZ0lhhk5P8I+LvM/GPgT4HbI+JPgHuB5zJzPfBc81jSGaJv\n+DNzITNfbu6/D7wOrAU2AXuap+0Bbp5Uk5LG77Q+80fEOuBLwIvApZm5AIt/IIDV425O0uQMfG5/\nRHwGeBL4Rma+FxGDvm4bsG249iRNykAjf0Scz2Lwv5+ZTzWLj0bEmqa+Bji23Gszc1dmzmXm3Dga\nljQefcMfi0P8d4HXM/NbS0r7gC3N/S3A3vG3J2lSBtntvxb4a+CViPhZs2wH8DDwo4j4GvAr4JbJ\ntHjmW7duXWt9586dI63/4MGDPWtt03ertr7hz8z/Anp9wP+L8bYjaVo8w08qyvBLRRl+qSjDLxVl\n+KWiDL9UlJfunoJ+x/k3b9480vrvvvvunrXjx4+PtG6dvRz5paIMv1SU4ZeKMvxSUYZfKsrwS0UZ\nfqkoj/NPwfz8fGv9xIn2K573u2TakSNHTrsnyZFfKsrwS0UZfqkowy8VZfilogy/VJThl4qKtumd\nx76xiOltTCoqMweaS8+RXyrK8EtFGX6pKMMvFWX4paIMv1SU4ZeK6hv+iLg8Iv49Il6PiF9ExJ3N\n8gci4khE/Kz591eTb1fSuPQ9ySci1gBrMvPliPgscBC4GbgV+E1m/uPAG/MkH2niBj3Jp++VfDJz\nAVho7r8fEa8Da0drT1LXTuszf0SsA74EvNgsuiMifh4RuyPikh6v2RYRByLiwEidShqrgc/tj4jP\nAP8BfDMzn4qIS4F3gAQeYvGjwdY+63C3X5qwQXf7Bwp/RJwPPAv8ODO/tUx9HfBsZl7ZZz2GX5qw\nsf2wJxYvHftd4PWlwW++CDzlK8Crp9ukpO4M8m3/nwH/CbwCnGwW7wBuA65mcbf/EPD15svBtnU5\n8ksTNtbd/nEx/NLk+Xt+Sa0Mv1SU4ZeKMvxSUYZfKsrwS0UZfqkowy8VZfilogy/VJThl4oy/FJR\nhl8qyvBLRfW9gOeYvQP8z5LHn2uWzaJZ7W1W+wJ7G9Y4e/uDQZ841d/zf2rjEQcyc66zBlrMam+z\n2hfY27C66s3dfqkowy8V1XX4d3W8/Taz2tus9gX2NqxOeuv0M7+k7nQ98kvqSCfhj4ibIuKNiHg7\nIu7toodeIuJQRLzSzDzc6RRjzTRoxyLi1SXLVkXETyPireZ22WnSOuptJmZubplZutP3btZmvJ76\nbn9EnAu8CWwEDgMvAbdl5mtTbaSHiDgEzGVm58eEI+LPgd8A3zs1G1JE/APwbmY+3PzhvCQz/35G\nenuA05y5eUK99ZpZ+m/o8L0b54zX49DFyL8BeDszf5mZvwWeADZ10MfMy8z9wLufWLwJ2NPc38Pi\nf56p69HbTMjMhcx8ubn/PnBqZulO37uWvjrRRfjXAr9e8vgwszXldwI/iYiDEbGt62aWcempmZGa\n29Ud9/NJfWdunqZPzCw9M+/dMDNej1sX4V9uNpFZOuRwbWZeA/wlcHuze6vBfBv4IovTuC0Aj3TZ\nTDOz9JPANzLzvS57WWqZvjp537oI/2Hg8iWPPw/Md9DHsjJzvrk9BjzN4seUWXL01CSpze2xjvv5\nf5l5NDM/zsyTwHfo8L1rZpZ+Evh+Zj7VLO78vVuur67ety7C/xKwPiK+EBEXAF8F9nXQx6dExMrm\nixgiYiVwI7M3+/A+YEtzfwuwt8NefseszNzca2ZpOn7vZm3G605O8mkOZfwzcC6wOzO/OfUmlhER\nf8jiaA+Lv3j8QZe9RcTjwPUs/urrKHA/8AzwI+D3gV8Bt2Tm1L9469Hb9ZzmzM0T6q3XzNIv0uF7\nN84Zr8fSj2f4STV5hp9UlOGXijL8UlGGXyrK8EtFGX6pKMMvFWX4paL+D/jSqtytRMHQAAAAAElF\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a1e9ee208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "some_index = 50000\n",
    "some_digit = mnist.train.images[some_index]\n",
    "\n",
    "plt.imshow(some_digit.reshape(28, 28), cmap=\"gray\")\n",
    "plt.show()\n",
    "print(np.argmax(mnist.train.labels[some_index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## \\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\n",
    "## First iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T14:33:16.796480Z",
     "start_time": "2018-01-18T14:33:16.790781Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "iter_ind=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### define some variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T14:33:17.063994Z",
     "start_time": "2018-01-18T14:33:17.054427Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_size = mnist.train.num_examples\n",
    "n_epoch = 50\n",
    "batch_size = 200\n",
    "\n",
    "n_feature = mnist.train.images.shape[1]\n",
    "n_neuron = {\"n_hidden{}\".format(i) : 100 for i in range(6)}\n",
    "n_label = mnist.train.labels.shape[1]\n",
    "n_label_04 = 5\n",
    "\n",
    "d_dtype=tf.float64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### first define some common functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T14:33:17.342050Z",
     "start_time": "2018-01-18T14:33:17.333967Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T14:33:17.772510Z",
     "start_time": "2018-01-18T14:33:17.767626Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def my_dnn_layer(activation=None, \n",
    "                 kernel_initializer=tf.contrib.layers.variance_scaling_initializer()):\n",
    "    return partial(tf.layers.dense, activation=activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T14:33:17.993383Z",
     "start_time": "2018-01-18T14:33:17.986808Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_batch(X, y, batch_size, epoch, batch):\n",
    "    num_inst = X.shape[0]\n",
    "    np.random.seed(epoch * batch * np.random.randint(100))\n",
    "    shuffle_index = np.random.randint(num_inst, size=batch_size)\n",
    "    return X[shuffle_index], y[shuffle_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Build DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T14:33:18.246072Z",
     "start_time": "2018-01-18T14:33:18.239220Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T14:33:18.704417Z",
     "start_time": "2018-01-18T14:33:18.697745Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_04 = tf.placeholder(dtype=d_dtype, shape=(None, n_feature), name=\"X_04\")\n",
    "y_04 = tf.placeholder(dtype=d_dtype, shape=(None, n_label_04), name=\"y_04\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T14:33:19.069045Z",
     "start_time": "2018-01-18T14:33:18.982240Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "selu_layer = my_dnn_layer(activation=tf.nn.selu)\n",
    "linear_layer = my_dnn_layer(activation=None)\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = selu_layer(X_04, n_neuron[\"n_hidden1\"], name=\"hidden1\")\n",
    "    hidden2 = selu_layer(hidden1, n_neuron[\"n_hidden2\"], name=\"hidden2\")\n",
    "    hidden3 = selu_layer(hidden2, n_neuron[\"n_hidden3\"], name=\"hidden3\")\n",
    "    hidden4 = selu_layer(hidden3, n_neuron[\"n_hidden4\"], name=\"hidden4\")\n",
    "    hidden5 = selu_layer(hidden4, n_neuron[\"n_hidden5\"], name=\"hidden5\")\n",
    "    logits = linear_layer(hidden5, n_label_04, name=\"logits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T14:33:19.607164Z",
     "start_time": "2018-01-18T14:33:19.345418Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.softmax_cross_entropy_with_logits(labels=y_04, logits=logits, name=\"xentropy\")\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T14:33:19.866325Z",
     "start_time": "2018-01-18T14:33:19.824221Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    true_labels = tf.argmax(y_04, axis=1)\n",
    "    y_pred = tf.argmax(logits, axis=1)\n",
    "    all_acc, acc = tf.metrics.accuracy(labels=true_labels, predictions=y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T14:33:20.265293Z",
     "start_time": "2018-01-18T14:33:20.084043Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"train\"):\n",
    "    adam_optimizer = tf.train.AdamOptimizer()\n",
    "    train_op = adam_optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T14:33:31.818495Z",
     "start_time": "2018-01-18T14:33:31.710795Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "time_now = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "base_dir = os.path.join(\"temp\", str(iter_ind))\n",
    "save_dir = os.path.join(base_dir, time_now, \"saves\")\n",
    "log_dir = os.path.join(base_dir, time_now, \"logs\")\n",
    "\n",
    "with tf.name_scope(\"savNlog\"):\n",
    "    saver = tf.train.Saver()\n",
    "    file_writer = tf.summary.FileWriter(log_dir) # Need to add the graph\n",
    "    train_acc_log = tf.summary.scalar(\"train_accuracy\", acc)\n",
    "    validation_acc_log = tf.summary.scalar(\"validation_accuracy\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T14:34:31.564922Z",
     "start_time": "2018-01-18T14:34:31.559173Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"initializer\"):\n",
    "    inits = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T14:37:12.030105Z",
     "start_time": "2018-01-18T14:34:31.948935Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5  Training accuracy: 0.9950, \n",
      "Validation accuracy: 0.9880\n",
      "epoch: 10 Training accuracy: 0.9888, \n",
      "Validation accuracy: 0.9889\n",
      "epoch: 15 Training accuracy: 0.9893, \n",
      "Validation accuracy: 0.9906\n",
      "epoch: 20 Training accuracy: 0.9908, \n",
      "Validation accuracy: 0.9915\n",
      "epoch: 25 Training accuracy: 0.9916, \n",
      "Validation accuracy: 0.9916\n",
      "epoch: 30 Training accuracy: 0.9917, \n",
      "Validation accuracy: 0.9917\n",
      "epoch: 35 Training accuracy: 0.9918, \n",
      "Validation accuracy: 0.9918\n",
      "epoch: 40 Training accuracy: 0.9919, \n",
      "Validation accuracy: 0.9919\n",
      "epoch: 45 Training accuracy: 0.9920, \n",
      "Validation accuracy: 0.9920\n",
      "epoch: 50 Training accuracy: 0.9921, \n",
      "Validation accuracy: 0.9921\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(inits)\n",
    "    file_writer.add_graph(tf.get_default_graph())\n",
    "    tf.train.start_queue_runners(sess=sess)\n",
    "    for epoch in range(1, n_epoch+1):\n",
    "        for batch in range(1, np.ceil(train_size/batch_size).astype(int)+1):\n",
    "            X_batch, y_batch =  get_batch(X_train_04, y_train_04, batch_size, epoch, batch)\n",
    "            sess.run(train_op, feed_dict={X_04: X_batch, y_04: y_batch})\n",
    "        if not epoch % 5:\n",
    "            train_acc, log_train = sess.run([acc, train_acc_log], \n",
    "                                            feed_dict={X_04: X_batch, y_04: y_batch})\n",
    "            val_acc, log_val = sess.run([acc, validation_acc_log],\n",
    "                                        feed_dict={X_04: X_valid_04, y_04: y_valid_04})\n",
    "            file_writer.add_summary(log_train, epoch)\n",
    "            file_writer.add_summary(log_val, epoch)\n",
    "            print(\"epoch: {:<3}Training accuracy: {:<6.4f}, \\nValidation accuracy: {:<6.4f}\"\n",
    "                  .format(epoch, train_acc, val_acc))\n",
    "            if not epoch % 10:\n",
    "                save_file_path = saver.save(sess, os.path.join(save_dir, \"{}.ckpt\".format(epoch//10)))\n",
    "                \n",
    "    file_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Test the result on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T14:37:12.678862Z",
     "start_time": "2018-01-18T14:37:12.406137Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from temp/1/20180119013331/saves/5.ckpt\n",
      "Accuracy for test set is: 0.9957\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    inits.run()\n",
    "    saver.restore(sess, save_file_path)\n",
    "    acc_test = sess.run(acc, feed_dict={X_04: X_test_04, y_04: y_test_04})\n",
    "    print(\"Accuracy for test set is: {:>4.4f}\".format(acc_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## \\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\n",
    "## Second iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T14:37:13.021453Z",
     "start_time": "2018-01-18T14:37:13.017125Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "iter_ind = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<Strong>Change</Strong>\n",
    "<ol>\n",
    "    <li>changed neuron per layer to 150\n",
    "    </li>\n",
    "    <li>changed activation function to elu for comparison\n",
    "    </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### define some variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T14:40:43.390644Z",
     "start_time": "2018-01-18T14:40:43.377638Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_size = mnist.train.num_examples\n",
    "n_epoch = 50\n",
    "batch_size = 200\n",
    "\n",
    "n_feature = mnist.train.images.shape[1]\n",
    "n_neuron = {\"n_hidden{}\".format(i) : 150 for i in range(6)}\n",
    "n_label = mnist.train.labels.shape[1]\n",
    "n_label_04 = 5\n",
    "\n",
    "d_dtype=tf.float64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### define some common functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T14:40:43.732313Z",
     "start_time": "2018-01-18T14:40:43.725749Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def my_dnn_layer(activation=None, \n",
    "                 kernel_initializer=tf.contrib.layers.variance_scaling_initializer()):\n",
    "    return partial(tf.layers.dense, activation=activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T14:40:44.141131Z",
     "start_time": "2018-01-18T14:40:44.133009Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_batch(X, y, batch_size, epoch, batch):\n",
    "    num_inst = X.shape[0]\n",
    "    np.random.seed(epoch * batch * np.random.randint(100))\n",
    "    shuffle_index = np.random.randint(num_inst, size=batch_size)\n",
    "    return X[shuffle_index], y[shuffle_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Build DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T14:40:45.095412Z",
     "start_time": "2018-01-18T14:40:45.091750Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T14:40:45.416421Z",
     "start_time": "2018-01-18T14:40:45.407148Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_04 = tf.placeholder(dtype=d_dtype, shape=(None, n_feature), name=\"X_04\")\n",
    "y_04 = tf.placeholder(dtype=d_dtype, shape=(None, n_label_04), name=\"y_04\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T14:40:45.802519Z",
     "start_time": "2018-01-18T14:40:45.720694Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "elu_layer = my_dnn_layer(activation=tf.nn.elu)\n",
    "linear_layer = my_dnn_layer(activation=None)\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = elu_layer(X_04, n_neuron[\"n_hidden1\"], name=\"hidden1\")\n",
    "    hidden2 = elu_layer(hidden1, n_neuron[\"n_hidden2\"], name=\"hidden2\")\n",
    "    hidden3 = elu_layer(hidden2, n_neuron[\"n_hidden3\"], name=\"hidden3\")\n",
    "    hidden4 = elu_layer(hidden3, n_neuron[\"n_hidden4\"], name=\"hidden4\")\n",
    "    hidden5 = elu_layer(hidden4, n_neuron[\"n_hidden5\"], name=\"hidden5\")\n",
    "    logits = linear_layer(hidden5, n_label_04, name=\"logits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T14:40:46.068366Z",
     "start_time": "2018-01-18T14:40:46.038493Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.softmax_cross_entropy_with_logits(labels=y_04, logits=logits, name=\"xentropy\")\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T14:40:46.289461Z",
     "start_time": "2018-01-18T14:40:46.266335Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    true_labels = tf.argmax(y_04, axis=1)\n",
    "    y_pred = tf.argmax(logits, axis=1)\n",
    "    all_acc, acc = tf.metrics.accuracy(labels=true_labels, predictions=y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T14:40:46.685628Z",
     "start_time": "2018-01-18T14:40:46.464915Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"train\"):\n",
    "    adam_optimizer = tf.train.AdamOptimizer()\n",
    "    train_op = adam_optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T14:41:00.737273Z",
     "start_time": "2018-01-18T14:41:00.478633Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "time_now = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "base_dir = os.path.join(\"temp\", str(iter_ind))\n",
    "save_dir = os.path.join(base_dir, time_now, \"saves\")\n",
    "log_dir = os.path.join(base_dir, time_now, \"logs\")\n",
    "\n",
    "with tf.name_scope(\"savNlog\"):\n",
    "    saver = tf.train.Saver()\n",
    "    file_writer = tf.summary.FileWriter(log_dir) # Need to add the graph\n",
    "    train_acc_log = tf.summary.scalar(\"train_accuracy\", acc)\n",
    "    validation_acc_log = tf.summary.scalar(\"validation_accuracy\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T14:41:00.999097Z",
     "start_time": "2018-01-18T14:41:00.992364Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"initializer\"):\n",
    "    inits = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T14:45:11.907733Z",
     "start_time": "2018-01-18T14:41:01.648679Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5  Training accuracy: 1.0000, \n",
      "Validation accuracy: 0.9840\n",
      "epoch: 10 Training accuracy: 0.9851, \n",
      "Validation accuracy: 0.9868\n",
      "epoch: 15 Training accuracy: 0.9872, \n",
      "Validation accuracy: 0.9888\n",
      "epoch: 20 Training accuracy: 0.9890, \n",
      "Validation accuracy: 0.9898\n",
      "epoch: 25 Training accuracy: 0.9900, \n",
      "Validation accuracy: 0.9906\n",
      "epoch: 30 Training accuracy: 0.9907, \n",
      "Validation accuracy: 0.9911\n",
      "epoch: 35 Training accuracy: 0.9912, \n",
      "Validation accuracy: 0.9915\n",
      "epoch: 40 Training accuracy: 0.9916, \n",
      "Validation accuracy: 0.9918\n",
      "epoch: 45 Training accuracy: 0.9919, \n",
      "Validation accuracy: 0.9920\n",
      "epoch: 50 Training accuracy: 0.9921, \n",
      "Validation accuracy: 0.9922\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(inits)\n",
    "    file_writer.add_graph(tf.get_default_graph())\n",
    "    tf.train.start_queue_runners(sess=sess)\n",
    "    for epoch in range(1, n_epoch+1):\n",
    "        for batch in range(1, np.ceil(train_size/batch_size).astype(int)+1):\n",
    "            X_batch, y_batch =  get_batch(X_train_04, y_train_04, batch_size, epoch, batch)\n",
    "            sess.run(train_op, feed_dict={X_04: X_batch, y_04: y_batch})\n",
    "        if not epoch % 5:\n",
    "            train_acc, log_train = sess.run([acc, train_acc_log], \n",
    "                                            feed_dict={X_04: X_batch, y_04: y_batch})\n",
    "            val_acc, log_val = sess.run([acc, validation_acc_log],\n",
    "                                        feed_dict={X_04: X_valid_04, y_04: y_valid_04})\n",
    "            file_writer.add_summary(log_train, epoch)\n",
    "            file_writer.add_summary(log_val, epoch)\n",
    "            print(\"epoch: {:<3}Training accuracy: {:<6.4f}, \\nValidation accuracy: {:<6.4f}\"\n",
    "                  .format(epoch, train_acc, val_acc))\n",
    "            if not epoch % 10:\n",
    "                save_file_path = saver.save(sess, os.path.join(save_dir, \"{}.ckpt\".format(epoch//10)))\n",
    "                \n",
    "    file_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Test the result on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T14:45:12.624793Z",
     "start_time": "2018-01-18T14:45:12.276399Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from temp/2/20180119014100/saves/5.ckpt\n",
      "Accuracy for test set is: 0.9953\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    inits.run()\n",
    "    saver.restore(sess, save_file_path)\n",
    "    acc_test = sess.run(acc, feed_dict={X_04: X_test_04, y_04: y_test_04})\n",
    "    print(\"Accuracy for test set is: {:>4.4f}\".format(acc_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## \\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\n",
    "## Third iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:27:22.613119Z",
     "start_time": "2018-01-18T15:27:22.609012Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "iter_ind = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<Strong>Change</Strong>\n",
    "<ol>\n",
    "    <li>Layer number back to 100 for faster training, same for selu\n",
    "    </li>\n",
    "    <li>Adding early stopping\n",
    "    </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### define some variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:27:23.348460Z",
     "start_time": "2018-01-18T15:27:23.337886Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_size = mnist.train.num_examples\n",
    "n_epoch = 50\n",
    "batch_size = 200\n",
    "max_drop = 5\n",
    "\n",
    "n_feature = mnist.train.images.shape[1]\n",
    "n_neuron = {\"n_hidden{}\".format(i) : 100 for i in range(6)}\n",
    "n_label = mnist.train.labels.shape[1]\n",
    "n_label_04 = 5\n",
    "\n",
    "d_dtype=tf.float64\n",
    "\n",
    "log_freq = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### define some common functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:27:24.064403Z",
     "start_time": "2018-01-18T15:27:24.059622Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def my_dnn_layer(activation=None, \n",
    "                 kernel_initializer=tf.contrib.layers.variance_scaling_initializer()):\n",
    "    return partial(tf.layers.dense, activation=activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:27:24.445745Z",
     "start_time": "2018-01-18T15:27:24.439675Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_batch(X, y, batch_size, epoch, batch):\n",
    "    num_inst = X.shape[0]\n",
    "    np.random.seed(epoch * batch * np.random.randint(100))\n",
    "    shuffle_index = np.random.randint(num_inst, size=batch_size)\n",
    "    return X[shuffle_index], y[shuffle_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Build DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:27:25.322846Z",
     "start_time": "2018-01-18T15:27:25.319169Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:27:25.680235Z",
     "start_time": "2018-01-18T15:27:25.665447Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_04 = tf.placeholder(dtype=d_dtype, shape=(None, n_feature), name=\"X_04\")\n",
    "y_04 = tf.placeholder(dtype=d_dtype, shape=(None, n_label_04), name=\"y_04\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:27:26.063400Z",
     "start_time": "2018-01-18T15:27:25.978162Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "selu_layer = my_dnn_layer(activation=tf.nn.selu)\n",
    "linear_layer = my_dnn_layer(activation=None)\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = selu_layer(X_04, n_neuron[\"n_hidden1\"], name=\"hidden1\")\n",
    "    hidden2 = selu_layer(hidden1, n_neuron[\"n_hidden2\"], name=\"hidden2\")\n",
    "    hidden3 = selu_layer(hidden2, n_neuron[\"n_hidden3\"], name=\"hidden3\")\n",
    "    hidden4 = selu_layer(hidden3, n_neuron[\"n_hidden4\"], name=\"hidden4\")\n",
    "    hidden5 = selu_layer(hidden4, n_neuron[\"n_hidden5\"], name=\"hidden5\")\n",
    "    logits = linear_layer(hidden5, n_label_04, name=\"logits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:27:26.351684Z",
     "start_time": "2018-01-18T15:27:26.309998Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.softmax_cross_entropy_with_logits(labels=y_04, logits=logits, name=\"xentropy\")\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:27:26.627796Z",
     "start_time": "2018-01-18T15:27:26.403645Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"train\"):\n",
    "    adam_optimizer = tf.train.AdamOptimizer()\n",
    "    train_op = adam_optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:27:27.059868Z",
     "start_time": "2018-01-18T15:27:27.033761Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    true_labels = tf.argmax(y_04, axis=1)\n",
    "    y_pred = tf.argmax(logits, axis=1)\n",
    "    all_acc, acc = tf.metrics.accuracy(labels=true_labels, predictions=y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:27:27.267655Z",
     "start_time": "2018-01-18T15:27:27.110576Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "time_now = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "base_dir = os.path.join(\"temp\", str(iter_ind))\n",
    "save_dir = os.path.join(base_dir, time_now, \"saves\")\n",
    "log_dir = os.path.join(base_dir, time_now, \"logs\")\n",
    "\n",
    "with tf.name_scope(\"savNlog\"):\n",
    "    saver = tf.train.Saver()\n",
    "    file_writer = tf.summary.FileWriter(log_dir) # Need to add the graph\n",
    "    train_acc_log = tf.summary.scalar(\"train_accuracy\", acc)\n",
    "    validation_acc_log = tf.summary.scalar(\"validation_accuracy\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:27:27.397440Z",
     "start_time": "2018-01-18T15:27:27.390490Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"initializer\"):\n",
    "    inits = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:30:06.097853Z",
     "start_time": "2018-01-18T15:27:27.762542Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5  Training accuracy: 1.0000, \n",
      "              Training loss: 0.002594\n",
      "                  Validation accuracy: 0.9880\n",
      "                      Validation loss: 0.047893\n",
      "0 0\n",
      "epoch: 10 Training accuracy: 0.9888, \n",
      "              Training loss: 0.000118\n",
      "                  Validation accuracy: 0.9898\n",
      "                      Validation loss: 0.054172\n",
      "0 1\n",
      "epoch: 15 Training accuracy: 0.9902, \n",
      "              Training loss: 0.001589\n",
      "                  Validation accuracy: 0.9911\n",
      "                      Validation loss: 0.044657\n",
      "0 0\n",
      "epoch: 20 Training accuracy: 0.9913, \n",
      "              Training loss: 0.000295\n",
      "                  Validation accuracy: 0.9912\n",
      "                      Validation loss: 0.054318\n",
      "0 1\n",
      "epoch: 25 Training accuracy: 0.9914, \n",
      "              Training loss: 0.000024\n",
      "                  Validation accuracy: 0.9910\n",
      "                      Validation loss: 0.055939\n",
      "1 2\n",
      "epoch: 30 Training accuracy: 0.9911, \n",
      "              Training loss: 0.005227\n",
      "                  Validation accuracy: 0.9907\n",
      "                      Validation loss: 0.083740\n",
      "2 3\n",
      "epoch: 35 Training accuracy: 0.9908, \n",
      "              Training loss: 0.000007\n",
      "                  Validation accuracy: 0.9910\n",
      "                      Validation loss: 0.042891\n",
      "3 0\n",
      "epoch: 40 Training accuracy: 0.9911, \n",
      "              Training loss: 0.000000\n",
      "                  Validation accuracy: 0.9912\n",
      "                      Validation loss: 0.043432\n",
      "4 1\n",
      "epoch: 45 Training accuracy: 0.9913, \n",
      "              Training loss: 0.000004\n",
      "                  Validation accuracy: 0.9914\n",
      "                      Validation loss: 0.044455\n",
      "0 2\n",
      "epoch: 50 Training accuracy: 0.9915, \n",
      "              Training loss: 0.000000\n",
      "                  Validation accuracy: 0.9916\n",
      "                      Validation loss: 0.045661\n",
      "0 3\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(inits)\n",
    "    \n",
    "    file_writer.add_graph(tf.get_default_graph())\n",
    "    \n",
    "    max_val_loss = np.inf\n",
    "    count_drop_loss = 0\n",
    "    best_model_loss = None\n",
    "    \n",
    "    max_val_acc = 0\n",
    "    count_drop_acc = 0\n",
    "    best_model_acc = None\n",
    "    \n",
    "    for epoch in range(1, n_epoch+1):\n",
    "        for batch in range(1, np.ceil(train_size/batch_size).astype(int)+1):\n",
    "            \n",
    "            X_batch, y_batch =  get_batch(X_train_04, y_train_04, batch_size, epoch, batch)\n",
    "            \n",
    "            sess.run(train_op, feed_dict={X_04: X_batch, y_04: y_batch})\n",
    "        if not epoch % log_freq:\n",
    "            train_acc, log_train, train_loss = sess.run([acc, train_acc_log, loss], \n",
    "                                            feed_dict={X_04: X_batch, y_04: y_batch})\n",
    "            val_acc, log_val, val_loss = sess.run([acc, validation_acc_log, loss],\n",
    "                                        feed_dict={X_04: X_valid_04, y_04: y_valid_04})\n",
    "            file_writer.add_summary(log_train, epoch)\n",
    "            file_writer.add_summary(log_val, epoch)\n",
    "            \n",
    "            print(\"epoch: {:<3}Training accuracy: {:<6.4f}, \\n              Training loss: {:<6.6f}\\n\\\n",
    "                  Validation accuracy: {:<6.4f}\\n                      Validation loss: {:<6.6f}\"\n",
    "                  .format(epoch, train_acc, train_loss, val_acc, val_loss))\n",
    "\n",
    "            save_file_path = saver.save(sess, os.path.join(save_dir, \"{}.ckpt\".format(epoch//log_freq)))\n",
    "            \n",
    "            if val_loss < max_val_loss:\n",
    "                max_val_loss = val_loss\n",
    "                best_model_loss = save_file_path\n",
    "                count_drop_loss = 0\n",
    "            else:\n",
    "                count_drop_loss += 1\n",
    "\n",
    "\n",
    "            if val_acc > max_val_acc:\n",
    "                max_val_acc = val_acc\n",
    "                best_model_acc = save_file_path\n",
    "                count_drop_acc = 0\n",
    "            else:\n",
    "                count_drop_acc += 1\n",
    "            if count_drop_acc > max_drop and count_drop_loss > max_drop:\n",
    "                break\n",
    "            print(count_drop_acc, count_drop_loss)\n",
    "                \n",
    "    file_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Test the result on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:30:06.909601Z",
     "start_time": "2018-01-18T15:30:06.589187Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from temp/3/20180119022727/saves/10.ckpt\n",
      "Accuracy for test set is: 0.9940\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    inits.run()\n",
    "    saver.restore(sess, best_model_acc)\n",
    "    acc_test = sess.run(acc, feed_dict={X_04: X_test_04, y_04: y_test_04})\n",
    "    print(\"Accuracy for test set is: {:>4.4f}\".format(acc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:30:07.888343Z",
     "start_time": "2018-01-18T15:30:07.658117Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from temp/3/20180119022727/saves/7.ckpt\n",
      "Accuracy for test set is: 0.9942\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    inits.run()\n",
    "    saver.restore(sess, best_model_loss)\n",
    "    acc_test = sess.run(acc, feed_dict={X_04: X_test_04, y_04: y_test_04})\n",
    "    print(\"Accuracy for test set is: {:>4.4f}\".format(acc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:30:07.888343Z",
     "start_time": "2018-01-18T15:30:07.658117Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from temp/3/20180119022727/saves/10.ckpt\n",
      "Accuracy for test set is: 0.9940\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    inits.run()\n",
    "    saver.restore(sess, save_file_path)\n",
    "    acc_test = sess.run(acc, feed_dict={X_04: X_test_04, y_04: y_test_04})\n",
    "    print(\"Accuracy for test set is: {:>4.4f}\".format(acc_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## \\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\n",
    "## Fourth iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:45:37.127395Z",
     "start_time": "2018-01-18T15:45:37.123729Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "iter_ind = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<Strong>Change</Strong>\n",
    "<ol>\n",
    "    <li>Adding BN\n",
    "    </li>\n",
    "<!--     <li>Adding early stopping\n",
    "    </li> -->\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### define some variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:52:41.883654Z",
     "start_time": "2018-01-18T15:52:41.873550Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_size = mnist.train.num_examples\n",
    "n_epoch = 100\n",
    "batch_size = 200\n",
    "max_drop = 5\n",
    "\n",
    "n_feature = mnist.train.images.shape[1]\n",
    "n_neuron = {\"n_hidden{}\".format(i) : 100 for i in range(6)}\n",
    "n_label = mnist.train.labels.shape[1]\n",
    "n_label_04 = 5\n",
    "\n",
    "d_dtype=tf.float64\n",
    "\n",
    "log_freq = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### define some common functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:52:42.457709Z",
     "start_time": "2018-01-18T15:52:42.453116Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def my_dnn_layer(**key_args):\n",
    "    return partial(tf.layers.dense, **key_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:52:42.788473Z",
     "start_time": "2018-01-18T15:52:42.783378Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def my_batch_norm(**key_args):\n",
    "    return partial(tf.layers.batch_normalization, **key_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:52:42.980388Z",
     "start_time": "2018-01-18T15:52:42.974047Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_batch(X, y, batch_size, epoch, batch):\n",
    "    num_inst = X.shape[0]\n",
    "    np.random.seed(epoch * batch * np.random.randint(100))\n",
    "    shuffle_index = np.random.randint(num_inst, size=batch_size)\n",
    "    return X[shuffle_index], y[shuffle_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Build DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:52:43.583972Z",
     "start_time": "2018-01-18T15:52:43.579232Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:52:43.600903Z",
     "start_time": "2018-01-18T15:52:43.592565Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_04 = tf.placeholder(dtype=d_dtype, shape=(None, n_feature), name=\"X_04\")\n",
    "y_04 = tf.placeholder(dtype=d_dtype, shape=(None, n_label_04), name=\"y_04\")\n",
    "training = tf.placeholder_with_default(False, None, name=\"training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:52:44.524207Z",
     "start_time": "2018-01-18T15:52:44.084202Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "selu_layer = my_dnn_layer(activation=tf.nn.selu)\n",
    "linear_layer = my_dnn_layer(activation=None)\n",
    "\n",
    "batch_layer = my_batch_norm(training=training)\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = selu_layer(X_04, n_neuron[\"n_hidden1\"], name=\"hidden1\")\n",
    "    bn1 = batch_layer(hidden1, name=\"bn1\")\n",
    "    hidden2 = selu_layer(bn1, n_neuron[\"n_hidden2\"], name=\"hidden2\")\n",
    "    bn2 = batch_layer(hidden2, name=\"bn2\")\n",
    "    hidden3 = selu_layer(bn2, n_neuron[\"n_hidden3\"], name=\"hidden3\")\n",
    "    bn3 = batch_layer(hidden3, name=\"bn3\")\n",
    "    hidden4 = selu_layer(bn3, n_neuron[\"n_hidden4\"], name=\"hidden4\")\n",
    "    bn4 = batch_layer(hidden4, name=\"bn4\")\n",
    "    hidden5 = selu_layer(bn4, n_neuron[\"n_hidden5\"], name=\"hidden5\")\n",
    "    bn5 = batch_layer(hidden5, name=\"bn5\")\n",
    "    logits = linear_layer(bn5, n_label_04, name=\"logits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:52:44.683806Z",
     "start_time": "2018-01-18T15:52:44.653379Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.softmax_cross_entropy_with_logits(labels=y_04, logits=logits, name=\"xentropy\")\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:52:45.885184Z",
     "start_time": "2018-01-18T15:52:45.165646Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"train\"):\n",
    "    batch_updates = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    adam_optimizer = tf.train.AdamOptimizer()\n",
    "    train_op = adam_optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:52:46.151431Z",
     "start_time": "2018-01-18T15:52:46.128725Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    true_labels = tf.argmax(y_04, axis=1)\n",
    "    y_pred = tf.argmax(logits, axis=1)\n",
    "    all_acc, acc = tf.metrics.accuracy(labels=true_labels, predictions=y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:52:46.602934Z",
     "start_time": "2018-01-18T15:52:46.413196Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "time_now = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "base_dir = os.path.join(\"temp\", str(iter_ind))\n",
    "save_dir = os.path.join(base_dir, time_now, \"saves\")\n",
    "log_dir = os.path.join(base_dir, time_now, \"logs\")\n",
    "\n",
    "with tf.name_scope(\"savNlog\"):\n",
    "    saver = tf.train.Saver(max_to_keep=20)\n",
    "    file_writer = tf.summary.FileWriter(log_dir) # Need to add the graph\n",
    "    train_acc_log = tf.summary.scalar(\"train_accuracy\", acc)\n",
    "    validation_acc_log = tf.summary.scalar(\"validation_accuracy\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:52:46.884447Z",
     "start_time": "2018-01-18T15:52:46.876134Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"initializer\"):\n",
    "    inits = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T16:03:03.640986Z",
     "start_time": "2018-01-18T15:52:47.128070Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5  Training accuracy: 0.9950, \n",
      "              Training loss: 0.005974\n",
      "                  Validation accuracy: 0.9902\n",
      "                      Validation loss: 0.033956\n",
      "0 0\n",
      "epoch: 10 Training accuracy: 0.9909, \n",
      "              Training loss: 0.004205\n",
      "                  Validation accuracy: 0.9898\n",
      "                      Validation loss: 0.036859\n",
      "1 1\n",
      "epoch: 15 Training accuracy: 0.9902, \n",
      "              Training loss: 0.001278\n",
      "                  Validation accuracy: 0.9908\n",
      "                      Validation loss: 0.045906\n",
      "0 2\n",
      "epoch: 20 Training accuracy: 0.9910, \n",
      "              Training loss: 0.000021\n",
      "                  Validation accuracy: 0.9913\n",
      "                      Validation loss: 0.041380\n",
      "0 3\n",
      "epoch: 25 Training accuracy: 0.9915, \n",
      "              Training loss: 0.000008\n",
      "                  Validation accuracy: 0.9918\n",
      "                      Validation loss: 0.036323\n",
      "0 4\n",
      "epoch: 30 Training accuracy: 0.9919, \n",
      "              Training loss: 0.000004\n",
      "                  Validation accuracy: 0.9922\n",
      "                      Validation loss: 0.037636\n",
      "0 5\n",
      "epoch: 35 Training accuracy: 0.9923, \n",
      "              Training loss: 0.000001\n",
      "                  Validation accuracy: 0.9925\n",
      "                      Validation loss: 0.038828\n",
      "0 6\n",
      "epoch: 40 Training accuracy: 0.9926, \n",
      "              Training loss: 0.000002\n",
      "                  Validation accuracy: 0.9927\n",
      "                      Validation loss: 0.040459\n",
      "0 7\n",
      "epoch: 45 Training accuracy: 0.9928, \n",
      "              Training loss: 0.000001\n",
      "                  Validation accuracy: 0.9929\n",
      "                      Validation loss: 0.041929\n",
      "0 8\n",
      "epoch: 50 Training accuracy: 0.9929, \n",
      "              Training loss: 0.000000\n",
      "                  Validation accuracy: 0.9930\n",
      "                      Validation loss: 0.044197\n",
      "0 9\n",
      "epoch: 55 Training accuracy: 0.9930, \n",
      "              Training loss: 0.000000\n",
      "                  Validation accuracy: 0.9930\n",
      "                      Validation loss: 0.045809\n",
      "0 10\n",
      "epoch: 60 Training accuracy: 0.9931, \n",
      "              Training loss: 0.000000\n",
      "                  Validation accuracy: 0.9931\n",
      "                      Validation loss: 0.048417\n",
      "0 11\n",
      "epoch: 65 Training accuracy: 0.9932, \n",
      "              Training loss: 0.000000\n",
      "                  Validation accuracy: 0.9932\n",
      "                      Validation loss: 0.050597\n",
      "0 12\n",
      "epoch: 70 Training accuracy: 0.9932, \n",
      "              Training loss: 0.000000\n",
      "                  Validation accuracy: 0.9932\n",
      "                      Validation loss: 0.052825\n",
      "0 13\n",
      "epoch: 75 Training accuracy: 0.9932, \n",
      "              Training loss: 0.000000\n",
      "                  Validation accuracy: 0.9933\n",
      "                      Validation loss: 0.055474\n",
      "0 14\n",
      "epoch: 80 Training accuracy: 0.9933, \n",
      "              Training loss: 0.000000\n",
      "                  Validation accuracy: 0.9933\n",
      "                      Validation loss: 0.058049\n",
      "0 15\n",
      "epoch: 85 Training accuracy: 0.9933, \n",
      "              Training loss: 0.000000\n",
      "                  Validation accuracy: 0.9934\n",
      "                      Validation loss: 0.060739\n",
      "0 16\n",
      "epoch: 90 Training accuracy: 0.9934, \n",
      "              Training loss: 0.000000\n",
      "                  Validation accuracy: 0.9934\n",
      "                      Validation loss: 0.062802\n",
      "0 17\n",
      "epoch: 95 Training accuracy: 0.9935, \n",
      "              Training loss: 0.000000\n",
      "                  Validation accuracy: 0.9935\n",
      "                      Validation loss: 0.065059\n",
      "0 18\n",
      "epoch: 100Training accuracy: 0.9935, \n",
      "              Training loss: 0.000000\n",
      "                  Validation accuracy: 0.9935\n",
      "                      Validation loss: 0.067404\n",
      "0 19\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(inits)\n",
    "    \n",
    "    file_writer.add_graph(tf.get_default_graph())\n",
    "    \n",
    "    max_val_loss = np.inf\n",
    "    count_drop_loss = 0\n",
    "    best_model_loss = None\n",
    "    \n",
    "    max_val_acc = 0\n",
    "    count_drop_acc = 0\n",
    "    best_model_acc = None\n",
    "    \n",
    "    for epoch in range(1, n_epoch+1):\n",
    "        for batch in range(1, np.ceil(train_size/batch_size).astype(int)+1):\n",
    "            \n",
    "            X_batch, y_batch =  get_batch(X_train_04, y_train_04, batch_size, epoch, batch)\n",
    "            \n",
    "            sess.run([train_op, batch_updates], feed_dict={X_04: X_batch, y_04: y_batch})\n",
    "        if not epoch % log_freq:\n",
    "            train_acc, log_train, train_loss = sess.run([acc, train_acc_log, loss], \n",
    "                                            feed_dict={X_04: X_batch, y_04: y_batch})\n",
    "            val_acc, log_val, val_loss = sess.run([acc, validation_acc_log, loss],\n",
    "                                        feed_dict={X_04: X_valid_04, y_04: y_valid_04})\n",
    "            file_writer.add_summary(log_train, epoch)\n",
    "            file_writer.add_summary(log_val, epoch)\n",
    "            \n",
    "            print(\"epoch: {:<3}Training accuracy: {:<6.4f}, \\n              Training loss: {:<6.6f}\\n\\\n",
    "                  Validation accuracy: {:<6.4f}\\n                      Validation loss: {:<6.6f}\"\n",
    "                  .format(epoch, train_acc, train_loss, val_acc, val_loss))\n",
    "\n",
    "            save_file_path = saver.save(sess, os.path.join(save_dir, \"{}.ckpt\".format(epoch//log_freq)))\n",
    "            \n",
    "            if val_loss < max_val_loss:\n",
    "                max_val_loss = val_loss\n",
    "                best_model_loss = save_file_path\n",
    "                count_drop_loss = 0\n",
    "            else:\n",
    "                count_drop_loss += 1\n",
    "\n",
    "\n",
    "            if val_acc > max_val_acc:\n",
    "                max_val_acc = val_acc\n",
    "                best_model_acc = save_file_path\n",
    "                count_drop_acc = 0\n",
    "            else:\n",
    "                count_drop_acc += 1\n",
    "            if count_drop_acc > max_drop and count_drop_loss > max_drop:\n",
    "                break\n",
    "            print(count_drop_acc, count_drop_loss)\n",
    "                \n",
    "    file_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Test the result on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T16:03:04.722625Z",
     "start_time": "2018-01-18T16:03:04.010731Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from temp/4/20180119025246/saves/20.ckpt\n",
      "Accuracy for test set is: 0.9946\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    inits.run()\n",
    "    saver.restore(sess, best_model_acc)\n",
    "    acc_test = sess.run(acc, feed_dict={X_04: X_test_04, y_04: y_test_04})\n",
    "    print(\"Accuracy for test set is: {:>4.4f}\".format(acc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T16:06:27.486133Z",
     "start_time": "2018-01-18T16:06:26.995012Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/apple/AnacondaProjects/handson-ml/chapter_11_exer/temp/4/20180119025246/saves/16.ckpt\n",
      "Accuracy for test set is: 0.9944\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    inits.run()\n",
    "    saver.restore(sess, best_model_loss)\n",
    "    acc_test = sess.run(acc, feed_dict={X_04: X_test_04, y_04: y_test_04})\n",
    "    print(\"Accuracy for test set is: {:>4.4f}\".format(acc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T16:06:28.940866Z",
     "start_time": "2018-01-18T16:06:28.446732Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from temp/4/20180119025246/saves/20.ckpt\n",
      "Accuracy for test set is: 0.9946\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    inits.run()\n",
    "    saver.restore(sess, save_file_path)\n",
    "    acc_test = sess.run(acc, feed_dict={X_04: X_test_04, y_04: y_test_04})\n",
    "    print(\"Accuracy for test set is: {:>4.4f}\".format(acc_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## \\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\n",
    "## Fifth iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:45:37.127395Z",
     "start_time": "2018-01-18T15:45:37.123729Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "iter_ind = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<Strong>Change</Strong>\n",
    "<ol>\n",
    "    <li>Parsing the structure into a sklearn func\n",
    "    </li>\n",
    "    <li>Changing the structure of saving file\n",
    "    </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-19T08:46:56.006848Z",
     "start_time": "2018-01-19T08:46:54.266798Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.exceptions import NotFittedError\n",
    "\n",
    "class DNNClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, n_hidden_layers=5, n_neurons=100, optimizer_class=tf.train.AdamOptimizer,\n",
    "                 learning_rate=0.01, n_epochs=100, batch_size=50, \n",
    "                 activation=tf.nn.selu, initializer=None,\n",
    "                 batch_norm=None, dropout_rate=None, \n",
    "                 regularizer=None, base_dir=\"temp\", \n",
    "                 log_freq=5, max_stop=5,\n",
    "                 random_state=None):\n",
    "        self.n_hidden_layers = n_hidden_layers\n",
    "        self.n_neurons = n_neurons\n",
    "        self.optimizer_class = optimizer_class\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_epochs = n_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.activation = activation\n",
    "        self.initializer= initializer\n",
    "        self.batch_norm = batch_norm\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.regularizer = regularizer\n",
    "        self.base_dir = base_dir\n",
    "        self.log_freq = log_freq\n",
    "        self.max_stop = max_stop\n",
    "        self.random_state = random_state\n",
    "        self._session = None\n",
    "        self._graph = None\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "    def _dnn_layout(self, inputs, n_labels):\n",
    "        \n",
    "        if type(self.n_neurons) is int:\n",
    "            self._n_neurons = (self.n_neurons for i in range(self.n_hidden_layers))\n",
    "        elif type(self.n_neurons) is list:\n",
    "            self._n_neurons = iter(self.n_neurons)\n",
    "        else:\n",
    "            raise TypeError(\"The type of n_neurons can't be %s\" % str(type(self.n_neurons)))\n",
    "        with tf.name_scope(\"dnn\"):\n",
    "            for lay in range(1, self.n_hidden_layers+1):\n",
    "                if self.dropout_rate:\n",
    "                    inputs = tf.layers.dropout(\n",
    "                                      inputs, \n",
    "                                      rate=self.dropout_rate, \n",
    "                                      training=self._training, \n",
    "                                      name=\"dropout%d\" % lay\n",
    "                                    )\n",
    "                inputs = tf.layers.dense(\n",
    "                                    inputs,\n",
    "                                    next(self._n_neurons),\n",
    "                                    activation=self.activation,\n",
    "                                    kernel_initializer=self.initializer,\n",
    "                                    kernel_regularizer=self.regularizer,\n",
    "                                    trainable=True,\n",
    "                                    name=\"hidden%d\" % lay\n",
    "                                )\n",
    "                if self.batch_norm:\n",
    "                    inputs = tf.layers.batch_normalization(\n",
    "                                        inputs,\n",
    "                                        training=self._training,\n",
    "                                        name=\"bn%d\" % lay\n",
    "                                    )\n",
    "            logits = tf.layers.dense(\n",
    "                                    inputs,\n",
    "                                    n_labels,\n",
    "                                    name=\"logits\"\n",
    "                                )\n",
    "            return logits\n",
    "\n",
    "    def _build_graph(self, n_features, n_labels):\n",
    "        \"\"\"Build the same model as earlier\"\"\"\n",
    "        if self.random_state is not None:\n",
    "            tf.set_random_seed(self.random_state)\n",
    "            np.random.seed(self.random_state)\n",
    "        self._graph = tf.Graph()\n",
    "        self._graph.as_default()\n",
    "        \n",
    "        X = tf.placeholder(\n",
    "                    tf.float32,\n",
    "                    shape=(None, n_features),\n",
    "                    name=\"X\"\n",
    "                )\n",
    "        y = tf.placeholder(\n",
    "                    tf.float32,\n",
    "                    shape=(None, n_labels),\n",
    "                    name=\"y\"\n",
    "                )\n",
    "\n",
    "        self._training = tf.placeholder_with_default(False, None, name=\"training\")\n",
    "        \n",
    "        logits = self._dnn_layout(X, n_labels)\n",
    "        y_proba = tf.nn.softmax(logits, name=\"y_proba\")\n",
    "        \n",
    "        with tf.name_scope(\"loss\"):\n",
    "            xentropy = tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=logits, name=\"xentropy\")\n",
    "            loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "            \n",
    "        with tf.name_scope(\"train\"):\n",
    "            batch_updates = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "            optimizer = self.optimizer_class()\n",
    "            training_op = optimizer.minimize(loss)\n",
    "            \n",
    "        with tf.name_scope(\"eval\"):\n",
    "            true_labels = tf.argmax(y, axis=1)\n",
    "            y_pred = tf.argmax(logits, axis=1)\n",
    "            all_acc, acc = tf.metrics.accuracy(labels=true_labels, predictions=y_pred)\n",
    "            \n",
    "        time_now = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "        save_dir = os.path.join(base_dir, \"saves\", time_now)\n",
    "        log_dir = os.path.join(base_dir, \"logs\", time_now)\n",
    "        if not os.path.isdir(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "        if not os.path.isdir(log_dir):\n",
    "            os.makedirs(log_dir)\n",
    "        with tf.name_scope(\"savNlog\"):\n",
    "            saver = tf.train.Saver(max_to_keep=20)\n",
    "            file_writer = tf.summary.FileWriter(log_dir) # Need to add the graph\n",
    "            train_acc_log = tf.summary.scalar(\"train_accuracy\", acc)\n",
    "            validation_acc_log = tf.summary.scalar(\"validation_accuracy\", acc)\n",
    "\n",
    "            \n",
    "        with tf.name_scope(\"initializer\"):\n",
    "            inits = tf.group(tf.global_variables_initializer(),\n",
    "                                    tf.local_variables_initializer())\n",
    "            \n",
    "        \n",
    "        self._X = X\n",
    "        self._y = y\n",
    "        self._y_proba = y_proba\n",
    "        self._logits = logits\n",
    "        self._xentropy = xentropy\n",
    "        self._loss = loss\n",
    "        self._batch_updates = batch_updates\n",
    "        self._y_pred = y_pred\n",
    "        self._optimizer = optimizer\n",
    "        self._training_op = training_op\n",
    "        self._true_labels = true_labels\n",
    "        self._acc = acc\n",
    "        self._save_dir = save_dir\n",
    "        self._log_dir = log_dir\n",
    "        self._saver = saver\n",
    "        self._file_writer = file_writer\n",
    "        self._train_acc_log = train_acc_log\n",
    "        self._validation_acc_log = validation_acc_log\n",
    "        self._inits = inits\n",
    "\n",
    "    def close_session(self):\n",
    "        if self._session:\n",
    "            self._session.close()\n",
    "            \n",
    "    def _get_batch(self, X, y, batch_size, epoch, batch):\n",
    "        num_inst = X.shape[0]\n",
    "        np.random.seed(epoch * batch * np.random.randint(100))\n",
    "        shuffle_index = np.random.randint(num_inst, size=batch_size)\n",
    "        return X[shuffle_index], y[shuffle_index]\n",
    "\n",
    "    def fit(self, X, y, X_val=None, y_val=None, model=\"loss\"):\n",
    "        self.close_session()\n",
    "        if self._graph:\n",
    "            print(\"Resetting Old Graph\")\n",
    "            tf.reset_default_graph()\n",
    "        \n",
    "        n_features = X.shape[1]\n",
    "        n_labels = y.shape[1]\n",
    "        self._build_graph(n_features, n_labels)\n",
    "        \n",
    "        training_size = X.shape[0]\n",
    "        self._file_writer.add_graph(tf.get_default_graph())\n",
    "        \n",
    "        if X_val is not None and y_val is not None:\n",
    "            max_val_loss = np.inf\n",
    "            count_drop_loss = 0\n",
    "            self.best_model_loss_ = None\n",
    "\n",
    "            max_val_acc = 0\n",
    "            count_drop_acc = 0\n",
    "            self.best_model_acc_ = None\n",
    "        \n",
    "\n",
    "        sess = tf.Session()\n",
    "        self._session = sess\n",
    "        sess.as_default()\n",
    "        \n",
    "        sess.run(self._inits)\n",
    "        \n",
    "        for epoch in range(1, self.n_epochs+1):\n",
    "            for batch in range(1, np.ceil(training_size/self.batch_size).astype(int)+1):\n",
    "                X_batch, y_batch =  self._get_batch(X, y, self.batch_size, \n",
    "                                                    epoch, batch)\n",
    "\n",
    "                sess.run([self._training_op, self._batch_updates], \n",
    "                         feed_dict={self._X:X_batch, self._y: y_batch, self._training:True})\n",
    "            if not epoch % self.log_freq:\n",
    "                train_acc, log_train, train_loss = sess.run(\n",
    "                                                    [self._acc, self._train_acc_log, self._loss], \n",
    "                                                    feed_dict={self._X: X_batch, self._y: y_batch})\n",
    "\n",
    "                self._file_writer.add_summary(log_train, epoch)\n",
    "\n",
    "\n",
    "                save_file_path = self._saver.save(sess, os.path.join(self._save_dir, \n",
    "                                                                \"{}.ckpt\".format(epoch//log_freq)))\n",
    "                \n",
    "                self.last_epoch_model = save_file_path\n",
    "                \n",
    "                if X_val is not None and y_val is not None:\n",
    "                    val_acc, log_val, val_loss = sess.run(\n",
    "                                                    [self._acc, self._validation_acc_log, self._loss],\n",
    "                                                    feed_dict={self._X: X_val, self._y: y_val})\n",
    "                    self._file_writer.add_summary(log_val, epoch)\n",
    "\n",
    "                    if val_loss < max_val_loss:\n",
    "                        max_val_loss = val_loss\n",
    "                        self.best_model_loss_ = save_file_path\n",
    "                        count_drop_loss = 0\n",
    "                    else:\n",
    "                        count_drop_loss += 1\n",
    "\n",
    "\n",
    "                    if val_acc > max_val_acc:\n",
    "                        max_val_acc = val_acc\n",
    "                        self.best_model_acc_ = save_file_path\n",
    "                        count_drop_acc = 0\n",
    "                    else:\n",
    "                        count_drop_acc += 1\n",
    "\n",
    "                    print(\"epoch: {:<3}Training accuracy: {:<6.4f}, \\n\\\n",
    "              Training loss: {:<6.6f}\\n\\\n",
    "                                        Validation accuracy: {:<6.4f}\\n\\\n",
    "                                            Validation loss: {:<6.6f}\"\n",
    "                      .format(epoch, train_acc, train_loss, val_acc, val_loss))\n",
    "                    if count_drop_acc > self.max_stop and count_drop_loss > self.max_stop:\n",
    "                        print(\"Early stopping!\\n    Count acc = %d\\n    Count loss = %d\" \\\n",
    "                              % (count_drop_acc, count_drop_loss))\n",
    "                        self._restore_params(model)\n",
    "                        break\n",
    "                else:\n",
    "                    print(\"epoch: {:<3}Training accuracy: {:<6.4f}, \\n\\\n",
    "                              Training loss: {:<6.6f}\\n\".format(epoch, train_acc, train_loss))\n",
    "\n",
    "        self._file_writer.close()\n",
    "        \n",
    "    \n",
    "    def _restore_params(self, model):\n",
    "        with self._session.as_default() as sess:\n",
    "            if model == \"acc\":\n",
    "                self._saver.restore(sess, self.best_model_acc_)\n",
    "            elif model == \"loss\":\n",
    "                self._saver.restore(sess, self.best_model_loss_)\n",
    "            else:\n",
    "                try:\n",
    "                     self._saver.restore(sess, model)\n",
    "                except:\n",
    "                    raise AttributeError(\"Restore for '%s' is invalid\" % str(model))\n",
    "\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        if self._session is None:\n",
    "            raise NotFittedError(\"This %s instamance is not fitted yet\" % self.__class__.__name__)\n",
    "        with self._session.as_default() as sess:\n",
    "            return self._y_proba.eval(feed_dict={self._X: X})\n",
    "        \n",
    "    def predict(self, X, onehot=True):\n",
    "        if self._session is None:\n",
    "            raise NotFittedError(\"This %s instance is not fitted yet\" % self.__class__.__name__)\n",
    "        with self._session.as_default() as sess:\n",
    "            if onehot:\n",
    "                pred_proba = self._y_proba.eval(feed_dict={self._X: X})\n",
    "                return (pred_proba == pred_proba.max(axis=1)[:,None]).astype(int)\n",
    "            return self._y_pred.eval(feed_dict={self._X: X})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-19T08:46:56.127377Z",
     "start_time": "2018-01-19T08:46:56.120256Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "he_init = tf.contrib.layers.variance_scaling_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-19T08:46:57.128663Z",
     "start_time": "2018-01-19T08:46:57.121548Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dnn_clf = DNNClassifier(n_hidden_layers=6, n_neurons=120, optimizer_class=tf.train.AdamOptimizer,\n",
    "                 learning_rate=0.05, n_epochs=10, batch_size=100, \n",
    "                 activation=tf.nn.selu, initializer=None,\n",
    "                 batch_norm=True, dropout_rate=0.5, \n",
    "                 regularizer=None, base_dir=\"temp\", \n",
    "                 log_freq=5, max_stop = 5,\n",
    "                 random_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-19T08:47:37.535062Z",
     "start_time": "2018-01-19T08:46:58.840015Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5  Training accuracy: 1.0000, \n",
      "              Training loss: 0.008145\n",
      "                                        Validation accuracy: 0.9872\n",
      "                                            Validation loss: 0.045174\n",
      "epoch: 10 Training accuracy: 0.9873, \n",
      "              Training loss: 0.038048\n",
      "                                        Validation accuracy: 0.9870\n",
      "                                            Validation loss: 0.057140\n"
     ]
    }
   ],
   "source": [
    "dnn_clf.fit(X_train_04, y_train_04, X_valid_04, y_valid_04)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Test the result on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-19T08:47:37.679203Z",
     "start_time": "2018-01-19T08:47:37.670633Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-19T08:47:38.221764Z",
     "start_time": "2018-01-19T08:47:37.848593Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from temp/4/saves/20180119194700/2.ckpt\n",
      "The final model: Accuracy for test set is: 0.9879\n"
     ]
    }
   ],
   "source": [
    "dnn_clf._restore_params(dnn_clf.last_epoch_model)\n",
    "y_pred = dnn_clf.predict(X_test_04)\n",
    "acc_test = accuracy_score(y_test_04, y_pred)\n",
    "print(\"The final model: Accuracy for test set is: {:>4.4f}\".format(acc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-19T08:47:38.538545Z",
     "start_time": "2018-01-19T08:47:38.325760Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from temp/4/saves/20180119194700/1.ckpt\n",
      "The best validation acc model: Accuracy for test set is: 0.9887\n"
     ]
    }
   ],
   "source": [
    "dnn_clf._restore_params(\"acc\")\n",
    "y_pred = dnn_clf.predict(X_test_04)\n",
    "acc_test = accuracy_score(y_test_04, y_pred)\n",
    "print(\"The best validation acc model: Accuracy for test set is: {:>4.4f}\".format(acc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-19T08:47:38.872946Z",
     "start_time": "2018-01-19T08:47:38.690009Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from temp/4/saves/20180119194700/1.ckpt\n",
      "The best validation loss model: Accuracy for test set is: 0.9887\n"
     ]
    }
   ],
   "source": [
    "dnn_clf._restore_params(\"loss\")\n",
    "y_pred = dnn_clf.predict(X_test_04)\n",
    "acc_test = accuracy_score(y_test_04, y_pred)\n",
    "print(\"The best validation loss model: Accuracy for test set is: {:>4.4f}\"\n",
    "      .format(acc_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\n",
    "## Sixth iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-20T04:20:54.374812Z",
     "start_time": "2018-01-20T04:20:54.370945Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iter_ind = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Strong>Change</Strong>\n",
    "<ol>\n",
    "    <li>Adding randomized grid search\n",
    "    </li>\n",
    "    <li>Fixed bugs with accuracy and graph logging\n",
    "    </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Classifier function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-20T04:35:17.255397Z",
     "start_time": "2018-01-20T04:35:12.419073Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.exceptions import NotFittedError\n",
    "\n",
    "class DNNClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"\n",
    "    Brief: This class implements the methods with sklearn compatible \n",
    "           structure,and is aimed to enable training with sklearn \n",
    "           RandomizedSearchCV class of sklearn.\n",
    "           Available for one-hot encoded labels.\n",
    "    \n",
    "    Authorship: Calvin Zhuoqun Huang, 19 Jan 2018\n",
    "    \n",
    "    Requirements: tensorflow 1.4 and above\n",
    "                  numpy 1.13 and above\n",
    "                  sklearn 0.19 and above\n",
    "                  \n",
    "    Known issues: A lot :D\n",
    "                  Accuracy implementation is not entirely correct\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_hidden_layers=5, n_neurons=100, \n",
    "                 optimizer_class=tf.train.AdamOptimizer,\n",
    "                 learning_rate=0.01, n_epochs=100, batch_size=50, \n",
    "                 activation=tf.nn.selu, initializer=None,\n",
    "                 batch_norm_momentum=None, dropout_rate=None, \n",
    "                 regularizer=None, reg_scale=0.001, base_dir=\"temp\", \n",
    "                 log_freq=5, max_stop=5,\n",
    "                 random_state=None):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        n_hidden_layers: \n",
    "            Type: int, default to 5\n",
    "            Number of neural network layers you wish to use\n",
    "\n",
    "        n_neurons: \n",
    "            Type: int or sequence type containing int, default to 100.\n",
    "            Number of neurons you wish to use in each layer.\n",
    "            If integer is passed, then implements constant number of\n",
    "            neuron each layer\n",
    "\n",
    "\n",
    "        optimizer_class: \n",
    "            Type: default to tf.train.AdamOptimizer\n",
    "            Optimizer class from tensorflow train optimizer or any\n",
    "            other compatible optimizer\n",
    "\n",
    "\n",
    "        learning_rate: \n",
    "            Type: float, default to 0.01\n",
    "            Learning rate of the neural network.\n",
    "        \n",
    "        n_epochs:\n",
    "            Type: int, defaults to 100\n",
    "            The number of full epoches get used to train the model\n",
    "        \n",
    "        batch_size:\n",
    "            Type: int, defaults to 50\n",
    "            The number of examples in each batch\n",
    "            \n",
    "        activation:\n",
    "            Type: function, defaults to tf.nn.selu\n",
    "            The activation functions used in hidden layers\n",
    "            \n",
    "        batch_norm_momentum:\n",
    "            Type: 0 < float < 1, defaults to None\n",
    "            When given, applies batch normalization to the model\n",
    "            \n",
    "        dropout_rate:\n",
    "            Type: 0 < float < 1, defaults to None\n",
    "            When given, applies dropout regularization to the model\n",
    "        \n",
    "        regularizer:\n",
    "            Type: defaults to None\n",
    "            The type of regularization applied to hidden layer.\n",
    "            Supports tf.contrib.layers.l2/l1regularizer.\n",
    "        \n",
    "        reg_scale:\n",
    "            Type: float, defaults to 0.001\n",
    "            paramter beta for regularization.\n",
    "            \n",
    "        base_dir:\n",
    "            Type: str or os.path type\n",
    "            The directory you wantted to store the log file and session\n",
    "            record(maxima 20). With unique timestamp on the folder and\n",
    "            Record number on the files.\n",
    "            \n",
    "        log_freq:\n",
    "            Type: int, defaults to 5\n",
    "            The interval of epochs for storing logs and session parameters\n",
    "            \n",
    "        max_stop:\n",
    "            Type: int, defaults to 5\n",
    "            Specify the number of intervals before early stopping is \n",
    "            happens if both the accuracy and loss on\n",
    "            validation set drops belows the best mark for that long interval\n",
    "            (i.e. max_stop * log_freq)\n",
    "            (Only triggers if during fit stage the validation set X and y\n",
    "            are given)\n",
    "            \n",
    "        random_state:\n",
    "            Type: int, defaults to None\n",
    "            Random state for tensorflow random and numpy random.\n",
    "            \n",
    "            \n",
    "        Return: None\n",
    "        \"\"\"\n",
    "        self.n_hidden_layers = n_hidden_layers\n",
    "        self.n_neurons = n_neurons\n",
    "        self.optimizer_class = optimizer_class\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_epochs = n_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.activation = activation\n",
    "        self.initializer= initializer\n",
    "        self.batch_norm_momentum = batch_norm_momentum\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.regularizer = regularizer\n",
    "        self.reg_scale = reg_scale\n",
    "        self.base_dir = base_dir\n",
    "        self.log_freq = log_freq\n",
    "        self.max_stop = max_stop\n",
    "        self.random_state = random_state\n",
    "        self._session = None\n",
    "        self._graph = None\n",
    "#         clear the local graph\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "    def _dnn_layout(self, inputs, n_labels):\n",
    "        \"\"\"\n",
    "        Constructs the deep neural network part of the model\n",
    "        \n",
    "        Parameters:\n",
    "        inputs: \n",
    "            The inputs tensor for deep NN\n",
    "            \n",
    "        n_labels:\n",
    "            The output label number of NN\n",
    "            \n",
    "        \n",
    "        Return: logits from the last tensor\n",
    "        \n",
    "        TensorName:\n",
    "            dropout1...n, hidden1...n, bn1...n \n",
    "        \"\"\"\n",
    "        if type(self.n_neurons) is int:\n",
    "#             convert the number of neurons to a generator\n",
    "            self._n_neurons = (self.n_neurons for i in \n",
    "                               range(self.n_hidden_layers))\n",
    "        elif type(self.n_neurons) is list:\n",
    "            self._n_neurons = iter(self.n_neurons)\n",
    "        else:\n",
    "            raise TypeError(\"The type of n_neurons can't be %s\" \n",
    "                            % str(type(self.n_neurons)))\n",
    "        with tf.name_scope(\"dnn\"):\n",
    "            for lay in range(1, self.n_hidden_layers+1):\n",
    "                if self.dropout_rate:\n",
    "#                     Dropout layer\n",
    "                    inputs = tf.layers.dropout(\n",
    "                                      inputs, \n",
    "                                      rate=self.dropout_rate, \n",
    "                                      training=self._training, \n",
    "                                      name=\"dropout%d\" % lay\n",
    "                                    )\n",
    "#                 Hidden layer\n",
    "                inputs = tf.layers.dense(\n",
    "                                    inputs,\n",
    "                                    next(self._n_neurons),\n",
    "                                    activation=self.activation,\n",
    "                                    kernel_initializer=self.initializer(),\n",
    "                                    kernel_regularizer=\n",
    "                                        self.regularizer(self.reg_scale),\n",
    "                                    trainable=True,\n",
    "                                    name=\"hidden%d\" % lay\n",
    "                                )\n",
    "                if self.batch_norm_momentum:\n",
    "#                     BN layer\n",
    "                    inputs = tf.layers.batch_normalization(\n",
    "                                        inputs,\n",
    "                                        momentum=self.batch_norm_momentum,\n",
    "                                        training=self._training,\n",
    "                                        name=\"bn%d\" % lay\n",
    "                                    )\n",
    "            logits = tf.layers.dense(\n",
    "                                    inputs,\n",
    "                                    n_labels,\n",
    "                                    name=\"logits\"\n",
    "                                )\n",
    "            return logits\n",
    "\n",
    "    def _build_graph(self, n_features, n_labels):\n",
    "        \"\"\"Build the full model with all units\"\"\"\n",
    "        if self.random_state is not None:\n",
    "            tf.set_random_seed(self.random_state)\n",
    "            np.random.seed(self.random_state)\n",
    "        self._graph = tf.Graph()\n",
    "        self._graph.as_default()\n",
    "        \n",
    "        X = tf.placeholder(\n",
    "                    tf.float32,\n",
    "                    shape=(None, n_features),\n",
    "                    name=\"X\"\n",
    "                )\n",
    "        y = tf.placeholder(\n",
    "                    tf.float32,\n",
    "                    shape=(None, n_labels),\n",
    "                    name=\"y\"\n",
    "                )\n",
    "\n",
    "        self._training = tf.placeholder_with_default(False, None, \n",
    "                                                     name=\"training\")\n",
    "        \n",
    "        logits = self._dnn_layout(X, n_labels)\n",
    "        y_proba = tf.nn.softmax(logits, name=\"y_proba\")\n",
    "        \n",
    "        with tf.name_scope(\"loss\"):\n",
    "            xentropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "                                labels=y, logits=logits, name=\"xentropy\")\n",
    "            base_loss = tf.reduce_mean(xentropy)\n",
    "            reg_loss = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "            loss = tf.add_n([base_loss]+reg_loss, name=\"loss\")\n",
    "            \n",
    "        with tf.name_scope(\"train\"):\n",
    "            batch_updates = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "            optimizer = self.optimizer_class(self.learning_rate)\n",
    "            training_op = optimizer.minimize(loss)\n",
    "            \n",
    "        with tf.name_scope(\"eval\"):\n",
    "            true_labels = tf.argmax(y, axis=1)\n",
    "            y_pred = tf.argmax(logits, axis=1)\n",
    "            all_acc, acc = tf.metrics.accuracy(labels=true_labels, \n",
    "                                               predictions=y_pred)\n",
    "            \n",
    "        time_now = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "        save_dir = os.path.join(self.base_dir, \"saves\", time_now)\n",
    "        log_dir = os.path.join(self.base_dir, \"logs\", time_now)\n",
    "#         if the path for save and log don't exist, create them.\n",
    "        if not os.path.isdir(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "        if not os.path.isdir(log_dir):\n",
    "            os.makedirs(log_dir)\n",
    "        with tf.name_scope(\"savNlog\"):\n",
    "            saver = tf.train.Saver(max_to_keep=20)\n",
    "            file_writer = tf.summary.FileWriter(log_dir)\n",
    "            train_acc_log = tf.summary.scalar(\"train_accuracy\", acc)\n",
    "            validation_acc_log = tf.summary.scalar(\"validation_accuracy\", acc)\n",
    "            train_loss_log = tf.summary.scalar(\"train_loss\", loss)\n",
    "            validation_loss_log = tf.summary.scalar(\"validation_loss\", loss)\n",
    "\n",
    "\n",
    "        with tf.name_scope(\"initializer\"):\n",
    "#             Bundle the two initializers together\n",
    "            inits = tf.group(tf.global_variables_initializer(),\n",
    "                                    tf.local_variables_initializer())\n",
    "            \n",
    "        \n",
    "        self._X = X\n",
    "        self._y = y\n",
    "        self._y_proba = y_proba\n",
    "        self._logits = logits\n",
    "        self._xentropy = xentropy\n",
    "        self._loss = loss\n",
    "        self._batch_updates = batch_updates\n",
    "        self._y_pred = y_pred\n",
    "        self._optimizer = optimizer\n",
    "        self._training_op = training_op\n",
    "        self._true_labels = true_labels\n",
    "        self._acc = acc\n",
    "        self._save_dir = save_dir\n",
    "        self._log_dir = log_dir\n",
    "        self._saver = saver\n",
    "        self._file_writer = file_writer\n",
    "        self._train_acc_log = train_acc_log\n",
    "        self._validation_acc_log = validation_acc_log\n",
    "        self._train_loss_log = train_loss_log\n",
    "        self._validation_loss_log = validation_loss_log\n",
    "        self._inits = inits\n",
    "\n",
    "    def close_session(self):\n",
    "        \"\"\"\n",
    "        Close the session\n",
    "        \"\"\"\n",
    "        if self._session:\n",
    "            self._session.close()\n",
    "            \n",
    "    def _get_batch(self, X, y, batch_size, epoch, batch):\n",
    "        \"\"\"\n",
    "        get a batch of training set\n",
    "        \"\"\"\n",
    "        num_inst = X.shape[0]\n",
    "        np.random.seed(epoch * batch * np.random.randint(100))\n",
    "        shuffle_index = np.random.randint(num_inst, size=batch_size)\n",
    "        return X[shuffle_index], y[shuffle_index]\n",
    "\n",
    "    def fit(self, X, y, X_val=None, y_val=None, model=\"loss\"):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        X:\n",
    "            Training sets' data of shape [n_example, n_feature]\n",
    "        y:\n",
    "            Training sets' labels of shape [n_example, n_labels]\n",
    "        X_val:\n",
    "            Valiadtion sets data used for early stopping\n",
    "        y_val:\n",
    "            Valiadtion sets labels used for early stopping\n",
    "        model:\n",
    "            string: \"loss\" or \"acc\"\n",
    "            in case of early stopping, restore the model with best\n",
    "            loss on validation set or model with best accuracy on \n",
    "            validation set\n",
    "            \n",
    "        Return: None\n",
    "        \"\"\"\n",
    "        self.close_session()\n",
    "        if self._graph:\n",
    "            print(\"Resetting Old Graph\")\n",
    "            tf.reset_default_graph()\n",
    "        \n",
    "        n_features = X.shape[1]\n",
    "        n_labels = y.shape[1]\n",
    "        self._build_graph(n_features, n_labels)\n",
    "        \n",
    "        training_size = X.shape[0]\n",
    "        self._file_writer.add_graph(tf.get_default_graph())\n",
    "        \n",
    "        if X_val is not None and y_val is not None:\n",
    "            max_val_loss = np.inf\n",
    "            count_drop_loss = 0\n",
    "            self.best_model_loss_ = None\n",
    "\n",
    "            max_val_acc = 0\n",
    "            count_drop_acc = 0\n",
    "            self.best_model_acc_ = None\n",
    "\n",
    "        sess = tf.Session()\n",
    "        self._session = sess\n",
    "        sess.as_default()\n",
    "        sess.run(self._inits)\n",
    "        for epoch in range(1, self.n_epochs+1):\n",
    "            for batch in range(1, np.ceil(training_size/\n",
    "                                          self.batch_size).astype(int)+1):\n",
    "                \n",
    "                X_batch, y_batch =  self._get_batch(X, y, self.batch_size, \n",
    "                                                    epoch, batch)\n",
    "\n",
    "                sess.run([self._training_op, self._batch_updates], \n",
    "                         feed_dict={self._X:X_batch, \n",
    "                                    self._y: y_batch, \n",
    "                                    self._training:True})\n",
    "            if not epoch % self.log_freq:\n",
    "                train_acc, train_loss, log_train_acc, log_train_loss = sess.run(\n",
    "                                                    [self._acc, \n",
    "                                                     self._loss, \n",
    "                                                     self._train_acc_log, \n",
    "                                                     self._train_loss_log], \n",
    "                                                    feed_dict={self._X: X, self._y: y})\n",
    "\n",
    "                self._file_writer.add_summary(log_train_acc, epoch)\n",
    "\n",
    "\n",
    "                save_file_path = self._saver.save(sess, \n",
    "                                    os.path.join(self._save_dir, \n",
    "                                        \"{}.ckpt\".format(epoch//\n",
    "                                                         self.log_freq)))\n",
    "                \n",
    "                self.last_epoch_model = save_file_path\n",
    "                \n",
    "                if X_val is not None and y_val is not None:\n",
    "                    val_acc, val_loss, log_val_acc, log_val_loss = sess.run(\n",
    "                                                    [self._acc,  \n",
    "                                                     self._loss,\n",
    "                                                     self._validation_acc_log,\n",
    "                                                     self._validation_loss_log],\n",
    "                                                    feed_dict={self._X: X_val, self._y: y_val})\n",
    "                    self._file_writer.add_summary(log_val_acc, epoch)\n",
    "\n",
    "                    if val_loss < max_val_loss:\n",
    "                        max_val_loss = val_loss\n",
    "                        self.best_model_loss_ = save_file_path\n",
    "                        count_drop_loss = 0\n",
    "                    else:\n",
    "                        count_drop_loss += 1\n",
    "\n",
    "\n",
    "                    if val_acc > max_val_acc:\n",
    "                        max_val_acc = val_acc\n",
    "                        self.best_model_acc_ = save_file_path\n",
    "                        count_drop_acc = 0\n",
    "                    else:\n",
    "                        count_drop_acc += 1\n",
    "\n",
    "                    print(\"epoch: {:<3}Training accuracy: {:<6.4f}, \\n\\\n",
    "              Training loss: {:<6.6f}\\n\\\n",
    "                                        Validation accuracy: {:<6.4f}\\n\\\n",
    "                                            Validation loss: {:<6.6f}\"\n",
    "                      .format(epoch, train_acc, train_loss, \n",
    "                              val_acc, val_loss))\n",
    "                    if count_drop_acc > self.max_stop and \\\n",
    "                                            count_drop_loss > self.max_stop:\n",
    "                        print(\"Early stopping!\\n    Count acc = %d\\n    Count loss = %d\" \\\n",
    "                              % (count_drop_acc, count_drop_loss))\n",
    "                        self._restore_params(model)\n",
    "                        break\n",
    "                else:\n",
    "                    print(\"epoch: {:<3}Training accuracy: {:<6.4f}, \\n\\\n",
    "                              Training loss: {:<6.6f}\\n\".\n",
    "                          format(epoch, train_acc, train_loss))\n",
    "\n",
    "        self._file_writer.close()\n",
    "        \n",
    "    \n",
    "    def _restore_params(self, model):\n",
    "        \"\"\"\n",
    "        Restore the parameter from save files\n",
    "        \n",
    "        Parameters:\n",
    "        model:\n",
    "            The path to the save file or string /\"acc/\" or /\"loss/\"\n",
    "            \n",
    "        Return: None\n",
    "        \"\"\"\n",
    "        with self._session.as_default() as sess:\n",
    "            if model == \"acc\":\n",
    "                self._saver.restore(sess, self.best_model_acc_)\n",
    "            elif model == \"loss\":\n",
    "                self._saver.restore(sess, self.best_model_loss_)\n",
    "            else:\n",
    "                try:\n",
    "                     self._saver.restore(sess, model)\n",
    "                except:\n",
    "                    raise AttributeError(\"Restore for '%s' is invalid\" \n",
    "                                         % str(model))\n",
    "\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Return the probability of labels of Data given\n",
    "        \n",
    "        Parameters:\n",
    "        X:\n",
    "            Dataset same features as X in fit function\n",
    "            \n",
    "        return: \n",
    "            probability matrix\n",
    "        \"\"\"\n",
    "        if self._session is None:\n",
    "            raise NotFittedError(\"This %s instamance is not fitted yet\" \n",
    "                                 % self.__class__.__name__)\n",
    "        with self._session.as_default() as sess:\n",
    "            return self._y_proba.eval(feed_dict={self._X: X})\n",
    "        \n",
    "    def predict(self, X, onehot=True):\n",
    "        \"\"\"\n",
    "        Return the predicted labels of Data given\n",
    "        \n",
    "        Parameters:\n",
    "        X:\n",
    "            Dataset same features as X in fit function\n",
    "        \n",
    "        onehot:\n",
    "            True or False, defaults to True\n",
    "            Whether the returned label is in onehot encoding or not.\n",
    "            \n",
    "        return: \n",
    "            Onehot prediction matrix or predicted clases number \n",
    "            (range 0 to n_labels)\n",
    "        \"\"\"\n",
    "        if self._session is None:\n",
    "            raise NotFittedError(\"This %s instance is not fitted yet\" \n",
    "                                 % self.__class__.__name__)\n",
    "        with self._session.as_default() as sess:\n",
    "            if onehot:\n",
    "                pred_proba = self._y_proba.eval(feed_dict={self._X: X})\n",
    "                return (pred_proba ==\n",
    "                        pred_proba.max(axis=1)[:,None]).astype(int)\n",
    "            return self._y_pred.eval(feed_dict={self._X: X})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-20T04:39:13.012604Z",
     "start_time": "2018-01-20T04:38:58.012Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "he_init = tf.contrib.layers.variance_scaling_initializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Randomized Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-20T04:28:04.892177Z",
     "start_time": "2018-01-20T04:28:04.076873Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-20T04:28:05.099903Z",
     "start_time": "2018-01-20T04:28:05.060737Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "param_dict = {\n",
    "    'n_hidden_layers':[4,5,6,7],\n",
    "    'n_neurons':[80 + 10 * i for i in range(6)],\n",
    "    'optimizer_class':[tf.train.AdamOptimizer, tf.train.GradientDescentOptimizer],\n",
    "    'learning_rate':[0.005, 0.01, 0.02, 0.04],\n",
    "    'initializer':[tf.contrib.layers.variance_scaling_initializer(), None],\n",
    "    'batch_norm':[True],\n",
    "    'n_epochs':[40],\n",
    "    'dropout_rate':[0.2 * i for i in range(2, 5)]\n",
    "}\n",
    "\n",
    "search_dnn_cv = RandomizedSearchCV(DNNClassifier(), param_dict, scoring=\"accuracy\", \n",
    "                                   n_jobs=1, n_iter=10, verbose=4, cv=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-20T02:21:24.971781Z",
     "start_time": "2018-01-20T02:19:40.762993Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 10 candidates, totalling 20 fits\n",
      "[CV] optimizer_class=<class 'tensorflow.python.training.gradient_descent.GradientDescentOptimizer'>, n_neurons=100, n_hidden_layers=5, n_epochs=40, learning_rate=0.005, initializer=<function variance_scaling_initializer.<locals>._initializer at 0x1a2b24f9d8>, dropout_rate=0.8, batch_norm=True \n",
      "epoch: 5  Training accuracy: 0.2159, \n",
      "              Training loss: 1.635947\n",
      "                                        Validation accuracy: 0.2166\n",
      "                                            Validation loss: 1.628693\n",
      "epoch: 10 Training accuracy: 0.2163, \n",
      "              Training loss: 1.647187\n",
      "                                        Validation accuracy: 0.2166\n",
      "                                            Validation loss: 1.637716\n",
      "epoch: 15 Training accuracy: 0.2164, \n",
      "              Training loss: 1.655976\n",
      "                                        Validation accuracy: 0.2166\n",
      "                                            Validation loss: 1.647135\n",
      "epoch: 20 Training accuracy: 0.2164, \n",
      "              Training loss: 1.664888\n",
      "                                        Validation accuracy: 0.2166\n",
      "                                            Validation loss: 1.653666\n",
      "epoch: 25 Training accuracy: 0.2165, \n",
      "              Training loss: 1.682756\n",
      "                                        Validation accuracy: 0.2166\n",
      "                                            Validation loss: 1.670144\n",
      "epoch: 30 Training accuracy: 0.2165, \n",
      "              Training loss: 1.685075\n",
      "                                        Validation accuracy: 0.2166\n",
      "                                            Validation loss: 1.672290\n",
      "epoch: 35 Training accuracy: 0.2165, \n",
      "              Training loss: 1.681194\n",
      "                                        Validation accuracy: 0.2166\n",
      "                                            Validation loss: 1.666327\n",
      "Early stopping!\n",
      "    Count acc = 6\n",
      "    Count loss = 6\n",
      "INFO:tensorflow:Restoring parameters from temp/4/saves/20180120131941/1.ckpt\n",
      "[CV]  optimizer_class=<class 'tensorflow.python.training.gradient_descent.GradientDescentOptimizer'>, n_neurons=100, n_hidden_layers=5, n_epochs=40, learning_rate=0.005, initializer=<function variance_scaling_initializer.<locals>._initializer at 0x1a2b24f9d8>, dropout_rate=0.8, batch_norm=True, score=0.22483772023682147, total= 1.3min\n",
      "[CV] optimizer_class=<class 'tensorflow.python.training.gradient_descent.GradientDescentOptimizer'>, n_neurons=100, n_hidden_layers=5, n_epochs=40, learning_rate=0.005, initializer=<function variance_scaling_initializer.<locals>._initializer at 0x1a2b24f9d8>, dropout_rate=0.8, batch_norm=True \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.3min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5  Training accuracy: 0.2248, \n",
      "              Training loss: 1.630842\n",
      "                                        Validation accuracy: 0.2241\n",
      "                                            Validation loss: 1.631867\n",
      "epoch: 10 Training accuracy: 0.2244, \n",
      "              Training loss: 1.658574\n",
      "                                        Validation accuracy: 0.2241\n",
      "                                            Validation loss: 1.661140\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-581-fdb898b94c4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msearch_dnn_cv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_04\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_04\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_valid_04\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_valid_04\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda/envs/ml_env/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    637\u001b[0m                                   error_score=self.error_score)\n\u001b[1;32m    638\u001b[0m           for parameters, (train, test) in product(candidate_params,\n\u001b[0;32m--> 639\u001b[0;31m                                                    cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0;31m# if one choose to see train score, \"out\" will contain train score info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/ml_env/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/ml_env/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/ml_env/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/ml_env/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/ml_env/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/ml_env/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/ml_env/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/ml_env/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, error_score)\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-569-c5d5e34eea15>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, X_val, y_val, model)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m                 sess.run([self._training_op, self._batch_updates], \n\u001b[0;32m--> 193\u001b[0;31m                          feed_dict={self._X:X_batch, self._y: y_batch, self._training:True})\n\u001b[0m\u001b[1;32m    194\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_freq\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                 train_acc, train_loss, log_train_acc, log_train_loss = sess.run(\n",
      "\u001b[0;32m/anaconda/envs/ml_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/ml_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/ml_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/ml_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/ml_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "search_dnn_cv.fit(X_train_04, y_train_04, X_val=X_valid_04, y_val=y_valid_04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-19T10:22:35.936330Z",
     "start_time": "2018-01-19T10:22:35.927366Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_norm': True,\n",
       " 'dropout_rate': 0.2,\n",
       " 'learning_rate': 0.01,\n",
       " 'n_epochs': 100,\n",
       " 'n_hidden_layers': 4,\n",
       " 'n_neurons': 120,\n",
       " 'optimizer_class': tensorflow.python.training.gradient_descent.GradientDescentOptimizer}"
      ]
     },
     "execution_count": 553,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_dnn_param = search_dnn_cv.best_params_\n",
    "best_dnn_param[\"n_epochs\"] = 100\n",
    "best_dnn_param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-20T04:39:14.288907Z",
     "start_time": "2018-01-20T04:39:14.278587Z"
    }
   },
   "outputs": [],
   "source": [
    "dnn_clf = DNNClassifier(n_hidden_layers=5, n_neurons=[150,140,130,120,110], \n",
    "                 optimizer_class=tf.train.AdamOptimizer,\n",
    "                 learning_rate=0.01, n_epochs=100, batch_size=500, \n",
    "                 activation=tf.nn.selu, initializer=he_init,\n",
    "                 batch_norm_momentum=0.98, dropout_rate=0.5, \n",
    "                 regularizer=tf.contrib.layers.l2_regularizer,\n",
    "                 reg_scale=0.001,\n",
    "                 base_dir=\"temp\", \n",
    "                 log_freq=5, max_stop=5,\n",
    "                 random_state=42\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-20T04:51:05.337230Z",
     "start_time": "2018-01-20T04:39:14.952771Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5  Training accuracy: 0.9783, \n",
      "              Training loss: 0.257082\n",
      "                                        Validation accuracy: 0.9781\n",
      "                                            Validation loss: 0.262478\n",
      "epoch: 10 Training accuracy: 0.9777, \n",
      "              Training loss: 0.282405\n",
      "                                        Validation accuracy: 0.9777\n",
      "                                            Validation loss: 0.283239\n",
      "epoch: 15 Training accuracy: 0.9769, \n",
      "              Training loss: 0.293770\n",
      "                                        Validation accuracy: 0.9769\n",
      "                                            Validation loss: 0.296809\n",
      "epoch: 20 Training accuracy: 0.9775, \n",
      "              Training loss: 0.282405\n",
      "                                        Validation accuracy: 0.9775\n",
      "                                            Validation loss: 0.287820\n",
      "epoch: 25 Training accuracy: 0.9780, \n",
      "              Training loss: 0.283787\n",
      "                                        Validation accuracy: 0.9781\n",
      "                                            Validation loss: 0.288669\n",
      "epoch: 30 Training accuracy: 0.9781, \n",
      "              Training loss: 0.296582\n",
      "                                        Validation accuracy: 0.9782\n",
      "                                            Validation loss: 0.295988\n",
      "epoch: 35 Training accuracy: 0.9778, \n",
      "              Training loss: 0.296733\n",
      "                                        Validation accuracy: 0.9778\n",
      "                                            Validation loss: 0.297681\n",
      "epoch: 40 Training accuracy: 0.9780, \n",
      "              Training loss: 0.289996\n",
      "                                        Validation accuracy: 0.9781\n",
      "                                            Validation loss: 0.286818\n",
      "epoch: 45 Training accuracy: 0.9781, \n",
      "              Training loss: 0.296662\n",
      "                                        Validation accuracy: 0.9781\n",
      "                                            Validation loss: 0.292915\n",
      "epoch: 50 Training accuracy: 0.9781, \n",
      "              Training loss: 0.290306\n",
      "                                        Validation accuracy: 0.9781\n",
      "                                            Validation loss: 0.289132\n",
      "epoch: 55 Training accuracy: 0.9782, \n",
      "              Training loss: 0.299865\n",
      "                                        Validation accuracy: 0.9782\n",
      "                                            Validation loss: 0.300062\n",
      "epoch: 60 Training accuracy: 0.9785, \n",
      "              Training loss: 0.292495\n",
      "                                        Validation accuracy: 0.9785\n",
      "                                            Validation loss: 0.296517\n",
      "epoch: 65 Training accuracy: 0.9786, \n",
      "              Training loss: 0.289565\n",
      "                                        Validation accuracy: 0.9786\n",
      "                                            Validation loss: 0.288034\n",
      "epoch: 70 Training accuracy: 0.9787, \n",
      "              Training loss: 0.277873\n",
      "                                        Validation accuracy: 0.9787\n",
      "                                            Validation loss: 0.279071\n",
      "epoch: 75 Training accuracy: 0.9787, \n",
      "              Training loss: 0.293804\n",
      "                                        Validation accuracy: 0.9787\n",
      "                                            Validation loss: 0.293091\n",
      "epoch: 80 Training accuracy: 0.9788, \n",
      "              Training loss: 0.283660\n",
      "                                        Validation accuracy: 0.9789\n",
      "                                            Validation loss: 0.285768\n",
      "epoch: 85 Training accuracy: 0.9790, \n",
      "              Training loss: 0.283032\n",
      "                                        Validation accuracy: 0.9790\n",
      "                                            Validation loss: 0.287410\n",
      "epoch: 90 Training accuracy: 0.9790, \n",
      "              Training loss: 0.289084\n",
      "                                        Validation accuracy: 0.9790\n",
      "                                            Validation loss: 0.293763\n",
      "epoch: 95 Training accuracy: 0.9788, \n",
      "              Training loss: 0.289634\n",
      "                                        Validation accuracy: 0.9788\n",
      "                                            Validation loss: 0.288912\n",
      "epoch: 100Training accuracy: 0.9790, \n",
      "              Training loss: 0.283338\n",
      "                                        Validation accuracy: 0.9790\n",
      "                                            Validation loss: 0.287134\n"
     ]
    }
   ],
   "source": [
    "dnn_clf.fit(X_train_04, y_train_04, X_valid_04, y_valid_04)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Test the result on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-20T05:10:29.712322Z",
     "start_time": "2018-01-20T05:10:29.705795Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-20T05:10:30.601894Z",
     "start_time": "2018-01-20T05:10:30.345356Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from temp/saves/20180120153919/20.ckpt\n",
      "The final model: Accuracy for test set is: 0.9831\n"
     ]
    }
   ],
   "source": [
    "dnn_clf._restore_params(dnn_clf.last_epoch_model)\n",
    "y_pred = dnn_clf.predict(X_test_04)\n",
    "acc_test = accuracy_score(y_test_04, y_pred)\n",
    "print(\"The final model: Accuracy for test set is: {:>4.4f}\".format(acc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-20T05:10:32.584414Z",
     "start_time": "2018-01-20T05:10:32.265776Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from temp/saves/20180120153919/17.ckpt\n",
      "The best validation acc model: Accuracy for test set is: 0.9856\n"
     ]
    }
   ],
   "source": [
    "dnn_clf._restore_params(\"acc\")\n",
    "y_pred = dnn_clf.predict(X_test_04)\n",
    "acc_test = accuracy_score(y_test_04, y_pred)\n",
    "print(\"The best validation acc model: Accuracy for test set is: {:>4.4f}\".format(acc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-20T05:10:33.846003Z",
     "start_time": "2018-01-20T05:10:33.528696Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from temp/saves/20180120153919/1.ckpt\n",
      "The best validation loss model: Accuracy for test set is: 0.9796\n"
     ]
    }
   ],
   "source": [
    "dnn_clf._restore_params(\"loss\")\n",
    "y_pred = dnn_clf.predict(X_test_04)\n",
    "acc_test = accuracy_score(y_test_04, y_pred)\n",
    "print(\"The best validation loss model: Accuracy for test set is: {:>4.4f}\".format(acc_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <i>Question9</i> --Transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare 5 to 9 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-20T02:47:16.160053Z",
     "start_time": "2018-01-20T02:47:15.489484Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ind_59_train = (np.argmax(y_train, axis=1) > 4)\n",
    "ind_59_test = (np.argmax(y_test, axis=1) > 4)\n",
    "ind_59_valid = (np.argmax(y_valid, axis=1) > 4)\n",
    "X_train_59 = X_train[ind_59_train]\n",
    "y_train_59 = np.delete(y_train[ind_59_train], [5,6,7,8,9], axis=1)\n",
    "X_test_59 = X_test[ind_59_test]\n",
    "y_test_59 = np.delete(y_test[ind_59_test], [5,6,7,8,9], axis=1)\n",
    "X_valid_59 = X_valid[ind_59_valid]\n",
    "y_valid_59 = np.delete(y_valid[ind_59_valid], [5,6,7,8,9], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus: Check memory usage of different variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-20T02:48:18.856004Z",
     "start_time": "2018-01-20T02:48:18.847534Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "172480112"
      ]
     },
     "execution_count": 597,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sys import getsizeof\n",
    "\n",
    "getsizeof(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the previous model's parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dnn_clf."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 483,
   "position": {
    "height": "505px",
    "left": "1542px",
    "right": "20px",
    "top": "67px",
    "width": "334px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
