{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <i>Question8</i> --Training for digit 0 to 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load framework and Other library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-30T02:20:33.426730Z",
     "start_time": "2018-01-30T02:20:33.420158Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from datetime import datetime\n",
    "import os\n",
    "from time import clock, time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-30T02:09:22.559152Z",
     "start_time": "2018-01-30T02:09:21.514766Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-30T02:09:29.280719Z",
     "start_time": "2018-01-30T02:09:22.777646Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "X_train = mnist.train.images\n",
    "y_train = mnist.train.labels\n",
    "X_test = mnist.test.images\n",
    "y_test = mnist.test.labels\n",
    "X_valid = mnist.validation.images\n",
    "y_valid = mnist.validation.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-30T02:09:29.778094Z",
     "start_time": "2018-01-30T02:09:29.402804Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ind_04_train = (np.argmax(y_train, axis=1) <= 4)\n",
    "ind_04_test = (np.argmax(y_test, axis=1) <= 4)\n",
    "ind_04_valid = (np.argmax(y_valid, axis=1) <= 4)\n",
    "X_train_04 = X_train[ind_04_train]\n",
    "y_train_04 = np.delete(y_train[ind_04_train], [5,6,7,8,9], axis=1)\n",
    "X_test_04 = X_test[ind_04_test]\n",
    "y_test_04 = np.delete(y_test[ind_04_test], [5,6,7,8,9], axis=1)\n",
    "X_valid_04 = X_valid[ind_04_valid]\n",
    "y_valid_04 = np.delete(y_valid[ind_04_valid], [5,6,7,8,9], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-30T02:09:29.937288Z",
     "start_time": "2018-01-30T02:09:29.906391Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55000\n",
      "10000\n",
      "5000\n",
      "(28038, 5)\n"
     ]
    }
   ],
   "source": [
    "print(len(mnist.train.images))\n",
    "print(len(mnist.test.images))\n",
    "print(len(mnist.validation.images))\n",
    "print(y_train_04.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load a image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-30T02:09:30.937332Z",
     "start_time": "2018-01-30T02:09:30.062130Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAADIZJREFUeJzt3X/oXfV9x/Hn25+RtKChxMXULV2J\nY0PQypcwcA5lRNwoxAaU+sfISFlKULAwdRIEFSnIMrsNhEJKgym02oI/EmSsLTqWDURMpFSr8wcl\na5Pvl2TGgNY/LJr3/viejG/1e8+9ub/OTd7PB4R773nfe86bS17fz7n3nHs+kZlIquecrhuQ1A3D\nLxVl+KWiDL9UlOGXijL8UlGGXyrK8EtFGX6pqPOmubGI8HRCacIyMwZ53kgjf0TcFBFvRMTbEXHv\nKOuSNF0x7Ln9EXEu8CawETgMvATclpmvtbzGkV+asGmM/BuAtzPzl5n5W+AJYNMI65M0RaOEfy3w\n6yWPDzfLfkdEbIuIAxFxYIRtSRqzUb7wW27X4lO79Zm5C9gF7vZLs2SUkf8wcPmSx58H5kdrR9K0\njBL+l4D1EfGFiLgA+CqwbzxtSZq0oXf7M/OjiLgD+DFwLrA7M38xts4kTdTQh/qG2pif+aWJm8pJ\nPpLOXIZfKsrwS0UZfqkowy8VZfilogy/VJThl4oy/FJRhl8qyvBLRRl+qSjDLxVl+KWiDL9UlOGX\nijL8UlGGXyrK8EtFGX6pKMMvFTXVKbql03HFFVe01l944YXW+okTJ3rWrrrqqtbXfvDBB631s4Ej\nv1SU4ZeKMvxSUYZfKsrwS0UZfqkowy8VNdJx/og4BLwPfAx8lJlz42hKAti+fXtr/eKLL26tb968\nuWft/PPPH6qns8k4TvK5ITPfGcN6JE2Ru/1SUaOGP4GfRMTBiNg2joYkTceou/3XZuZ8RKwGfhoR\n/52Z+5c+ofmj4B8GacaMNPJn5nxzewx4GtiwzHN2ZeacXwZKs2Xo8EfEyoj47Kn7wI3Aq+NqTNJk\njbLbfynwdEScWs8PMvPfxtKVpImLzJzexiKmtzHNvHvuuae1/uCDD7bWT5482VpfuXLlafd0NsjM\nGOR5HuqTijL8UlGGXyrK8EtFGX6pKMMvFeWluzVRq1ev7lm75ZZbWl97wQUXtNbvu+++oXrSIkd+\nqSjDLxVl+KWiDL9UlOGXijL8UlGGXyrK4/yaqNtvv71n7Zprrml97fz8fGv9scceG6YlNRz5paIM\nv1SU4ZeKMvxSUYZfKsrwS0UZfqkoj/NrJGvWrGmtb926tWftnHPax5633nqrtb6wsNBaVztHfqko\nwy8VZfilogy/VJThl4oy/FJRhl8qqu9x/ojYDXwZOJaZVzbLVgE/BNYBh4BbM/PE5NrUrGo7jg9w\n2WWX9az1m2J77969Q/WkwQwy8j8G3PSJZfcCz2XmeuC55rGkM0jf8GfmfuDdTyzeBOxp7u8Bbh5z\nX5ImbNjP/Jdm5gJAc9t7TiZJM2ni5/ZHxDZg26S3I+n0DDvyH42INQDN7bFeT8zMXZk5l5lzQ25L\n0gQMG/59wJbm/hbAr2WlM0zf8EfE48ALwB9FxOGI+BrwMLAxIt4CNjaPJZ1BIjOnt7GI6W1MY3HR\nRRe11o8fP95av/DCC3vWdu7c2fra+++/v7X+4YcfttaryswY5Hme4ScVZfilogy/VJThl4oy/FJR\nhl8qykt3q9WOHTta6ytWrGittx1KfuaZZ1pf66G8yXLkl4oy/FJRhl8qyvBLRRl+qSjDLxVl+KWi\nPM5f3IYNG1rrd91110jrf+SRR3rWDhw4MNK6NRpHfqkowy8VZfilogy/VJThl4oy/FJRhl8qykt3\nn+XOO6/9VI5nn322tX7jjTe21o8ePdpan5vrPVHTkSNHWl+r4XjpbkmtDL9UlOGXijL8UlGGXyrK\n8EtFGX6pqL6/54+I3cCXgWOZeWWz7AHgb4H/bZ62IzP/dVJNanhbt25trW/cuLG13u88kH7X9fdY\n/uwaZOR/DLhpmeX/lJlXN/8MvnSG6Rv+zNwPvDuFXiRN0Sif+e+IiJ9HxO6IuGRsHUmaimHD/23g\ni8DVwALQ80JtEbEtIg5EhBdsk2bIUOHPzKOZ+XFmngS+A/S8CmRm7srMuczs/QsPSVM3VPgjYs2S\nh18BXh1PO5KmZZBDfY8D1wOfi4jDwP3A9RFxNZDAIeDrE+xR0gT4e/6zwIoVK3rW3nzzzdbXrl27\ntrW+f//+1voNN9zQWtf0+Xt+Sa0Mv1SU4ZeKMvxSUYZfKsrwS0U5RfdZ4NFHH+1Z63co77XXXmut\nb9q0aaieNPsc+aWiDL9UlOGXijL8UlGGXyrK8EtFGX6pKI/znwFWrVrVWr/uuuuGXvfzzz/fWn/v\nvfeGXrdmmyO/VJThl4oy/FJRhl8qyvBLRRl+qSjDLxXlcf4zwPbt21vr69ev71l74403Wl975513\nDtWTznyO/FJRhl8qyvBLRRl+qSjDLxVl+KWiDL9UVN/j/BFxOfA94PeAk8CuzPyXiFgF/BBYBxwC\nbs3ME5Nrta6HHnqotd42zfoTTzwx7nZ0lhhk5P8I+LvM/GPgT4HbI+JPgHuB5zJzPfBc81jSGaJv\n+DNzITNfbu6/D7wOrAU2AXuap+0Bbp5Uk5LG77Q+80fEOuBLwIvApZm5AIt/IIDV425O0uQMfG5/\nRHwGeBL4Rma+FxGDvm4bsG249iRNykAjf0Scz2Lwv5+ZTzWLj0bEmqa+Bji23Gszc1dmzmXm3Dga\nljQefcMfi0P8d4HXM/NbS0r7gC3N/S3A3vG3J2lSBtntvxb4a+CViPhZs2wH8DDwo4j4GvAr4JbJ\ntHjmW7duXWt9586dI63/4MGDPWtt03ertr7hz8z/Anp9wP+L8bYjaVo8w08qyvBLRRl+qSjDLxVl\n+KWiDL9UlJfunoJ+x/k3b9480vrvvvvunrXjx4+PtG6dvRz5paIMv1SU4ZeKMvxSUYZfKsrwS0UZ\nfqkoj/NPwfz8fGv9xIn2K573u2TakSNHTrsnyZFfKsrwS0UZfqkowy8VZfilogy/VJThl4qKtumd\nx76xiOltTCoqMweaS8+RXyrK8EtFGX6pKMMvFWX4paIMv1SU4ZeK6hv+iLg8Iv49Il6PiF9ExJ3N\n8gci4khE/Kz591eTb1fSuPQ9ySci1gBrMvPliPgscBC4GbgV+E1m/uPAG/MkH2niBj3Jp++VfDJz\nAVho7r8fEa8Da0drT1LXTuszf0SsA74EvNgsuiMifh4RuyPikh6v2RYRByLiwEidShqrgc/tj4jP\nAP8BfDMzn4qIS4F3gAQeYvGjwdY+63C3X5qwQXf7Bwp/RJwPPAv8ODO/tUx9HfBsZl7ZZz2GX5qw\nsf2wJxYvHftd4PWlwW++CDzlK8Crp9ukpO4M8m3/nwH/CbwCnGwW7wBuA65mcbf/EPD15svBtnU5\n8ksTNtbd/nEx/NLk+Xt+Sa0Mv1SU4ZeKMvxSUYZfKsrwS0UZfqkowy8VZfilogy/VJThl4oy/FJR\nhl8qyvBLRfW9gOeYvQP8z5LHn2uWzaJZ7W1W+wJ7G9Y4e/uDQZ841d/zf2rjEQcyc66zBlrMam+z\n2hfY27C66s3dfqkowy8V1XX4d3W8/Taz2tus9gX2NqxOeuv0M7+k7nQ98kvqSCfhj4ibIuKNiHg7\nIu7toodeIuJQRLzSzDzc6RRjzTRoxyLi1SXLVkXETyPireZ22WnSOuptJmZubplZutP3btZmvJ76\nbn9EnAu8CWwEDgMvAbdl5mtTbaSHiDgEzGVm58eEI+LPgd8A3zs1G1JE/APwbmY+3PzhvCQz/35G\nenuA05y5eUK99ZpZ+m/o8L0b54zX49DFyL8BeDszf5mZvwWeADZ10MfMy8z9wLufWLwJ2NPc38Pi\nf56p69HbTMjMhcx8ubn/PnBqZulO37uWvjrRRfjXAr9e8vgwszXldwI/iYiDEbGt62aWcempmZGa\n29Ud9/NJfWdunqZPzCw9M+/dMDNej1sX4V9uNpFZOuRwbWZeA/wlcHuze6vBfBv4IovTuC0Aj3TZ\nTDOz9JPANzLzvS57WWqZvjp537oI/2Hg8iWPPw/Md9DHsjJzvrk9BjzN4seUWXL01CSpze2xjvv5\nf5l5NDM/zsyTwHfo8L1rZpZ+Evh+Zj7VLO78vVuur67ety7C/xKwPiK+EBEXAF8F9nXQx6dExMrm\nixgiYiVwI7M3+/A+YEtzfwuwt8NefseszNzca2ZpOn7vZm3G605O8mkOZfwzcC6wOzO/OfUmlhER\nf8jiaA+Lv3j8QZe9RcTjwPUs/urrKHA/8AzwI+D3gV8Bt2Tm1L9469Hb9ZzmzM0T6q3XzNIv0uF7\nN84Zr8fSj2f4STV5hp9UlOGXijL8UlGGXyrK8EtFGX6pKMMvFWX4paL+D/jSqtytRMHQAAAAAElF\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c1d165c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "some_index = 50000\n",
    "some_digit = mnist.train.images[some_index]\n",
    "\n",
    "plt.imshow(some_digit.reshape(28, 28), cmap=\"gray\")\n",
    "plt.show()\n",
    "print(np.argmax(mnist.train.labels[some_index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## \\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\n",
    "## First iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T14:33:16.796480Z",
     "start_time": "2018-01-18T14:33:16.790781Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "iter_ind=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### define some variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T14:33:17.063994Z",
     "start_time": "2018-01-18T14:33:17.054427Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_size = mnist.train.num_examples\n",
    "n_epoch = 50\n",
    "batch_size = 200\n",
    "\n",
    "n_feature = mnist.train.images.shape[1]\n",
    "n_neuron = {\"n_hidden{}\".format(i) : 100 for i in range(6)}\n",
    "n_label = mnist.train.labels.shape[1]\n",
    "n_label_04 = 5\n",
    "\n",
    "d_dtype=tf.float64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### first define some common functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T14:33:17.342050Z",
     "start_time": "2018-01-18T14:33:17.333967Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T14:33:17.772510Z",
     "start_time": "2018-01-18T14:33:17.767626Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def my_dnn_layer(activation=None, \n",
    "                 kernel_initializer=tf.contrib.layers.variance_scaling_initializer()):\n",
    "    return partial(tf.layers.dense, activation=activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T14:33:17.993383Z",
     "start_time": "2018-01-18T14:33:17.986808Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_batch(X, y, batch_size, epoch, batch):\n",
    "    num_inst = X.shape[0]\n",
    "    np.random.seed(epoch * batch * np.random.randint(100))\n",
    "    shuffle_index = np.random.randint(num_inst, size=batch_size)\n",
    "    return X[shuffle_index], y[shuffle_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Build DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T14:33:18.246072Z",
     "start_time": "2018-01-18T14:33:18.239220Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T14:33:18.704417Z",
     "start_time": "2018-01-18T14:33:18.697745Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_04 = tf.placeholder(dtype=d_dtype, shape=(None, n_feature), name=\"X_04\")\n",
    "y_04 = tf.placeholder(dtype=d_dtype, shape=(None, n_label_04), name=\"y_04\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T14:33:19.069045Z",
     "start_time": "2018-01-18T14:33:18.982240Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "selu_layer = my_dnn_layer(activation=tf.nn.selu)\n",
    "linear_layer = my_dnn_layer(activation=None)\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = selu_layer(X_04, n_neuron[\"n_hidden1\"], name=\"hidden1\")\n",
    "    hidden2 = selu_layer(hidden1, n_neuron[\"n_hidden2\"], name=\"hidden2\")\n",
    "    hidden3 = selu_layer(hidden2, n_neuron[\"n_hidden3\"], name=\"hidden3\")\n",
    "    hidden4 = selu_layer(hidden3, n_neuron[\"n_hidden4\"], name=\"hidden4\")\n",
    "    hidden5 = selu_layer(hidden4, n_neuron[\"n_hidden5\"], name=\"hidden5\")\n",
    "    logits = linear_layer(hidden5, n_label_04, name=\"logits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T14:33:19.607164Z",
     "start_time": "2018-01-18T14:33:19.345418Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.softmax_cross_entropy_with_logits(labels=y_04, logits=logits, name=\"xentropy\")\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T14:33:19.866325Z",
     "start_time": "2018-01-18T14:33:19.824221Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    true_labels = tf.argmax(y_04, axis=1)\n",
    "    y_pred = tf.argmax(logits, axis=1)\n",
    "    all_acc, acc = tf.metrics.accuracy(labels=true_labels, predictions=y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T14:33:20.265293Z",
     "start_time": "2018-01-18T14:33:20.084043Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"train\"):\n",
    "    adam_optimizer = tf.train.AdamOptimizer()\n",
    "    train_op = adam_optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T14:33:31.818495Z",
     "start_time": "2018-01-18T14:33:31.710795Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "time_now = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "base_dir = os.path.join(\"temp\", str(iter_ind))\n",
    "save_dir = os.path.join(base_dir, time_now, \"saves\")\n",
    "log_dir = os.path.join(base_dir, time_now, \"logs\")\n",
    "\n",
    "with tf.name_scope(\"savNlog\"):\n",
    "    saver = tf.train.Saver()\n",
    "    file_writer = tf.summary.FileWriter(log_dir) # Need to add the graph\n",
    "    train_acc_log = tf.summary.scalar(\"train_accuracy\", acc)\n",
    "    validation_acc_log = tf.summary.scalar(\"validation_accuracy\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T14:34:31.564922Z",
     "start_time": "2018-01-18T14:34:31.559173Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"initializer\"):\n",
    "    inits = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T14:37:12.030105Z",
     "start_time": "2018-01-18T14:34:31.948935Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5  Training accuracy: 0.9950, \n",
      "Validation accuracy: 0.9880\n",
      "epoch: 10 Training accuracy: 0.9888, \n",
      "Validation accuracy: 0.9889\n",
      "epoch: 15 Training accuracy: 0.9893, \n",
      "Validation accuracy: 0.9906\n",
      "epoch: 20 Training accuracy: 0.9908, \n",
      "Validation accuracy: 0.9915\n",
      "epoch: 25 Training accuracy: 0.9916, \n",
      "Validation accuracy: 0.9916\n",
      "epoch: 30 Training accuracy: 0.9917, \n",
      "Validation accuracy: 0.9917\n",
      "epoch: 35 Training accuracy: 0.9918, \n",
      "Validation accuracy: 0.9918\n",
      "epoch: 40 Training accuracy: 0.9919, \n",
      "Validation accuracy: 0.9919\n",
      "epoch: 45 Training accuracy: 0.9920, \n",
      "Validation accuracy: 0.9920\n",
      "epoch: 50 Training accuracy: 0.9921, \n",
      "Validation accuracy: 0.9921\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(inits)\n",
    "    file_writer.add_graph(tf.get_default_graph())\n",
    "    tf.train.start_queue_runners(sess=sess)\n",
    "    for epoch in range(1, n_epoch+1):\n",
    "        for batch in range(1, np.ceil(train_size/batch_size).astype(int)+1):\n",
    "            X_batch, y_batch =  get_batch(X_train_04, y_train_04, batch_size, epoch, batch)\n",
    "            sess.run(train_op, feed_dict={X_04: X_batch, y_04: y_batch})\n",
    "        if not epoch % 5:\n",
    "            train_acc, log_train = sess.run([acc, train_acc_log], \n",
    "                                            feed_dict={X_04: X_batch, y_04: y_batch})\n",
    "            val_acc, log_val = sess.run([acc, validation_acc_log],\n",
    "                                        feed_dict={X_04: X_valid_04, y_04: y_valid_04})\n",
    "            file_writer.add_summary(log_train, epoch)\n",
    "            file_writer.add_summary(log_val, epoch)\n",
    "            print(\"epoch: {:<3}Training accuracy: {:<6.4f}, \\nValidation accuracy: {:<6.4f}\"\n",
    "                  .format(epoch, train_acc, val_acc))\n",
    "            if not epoch % 10:\n",
    "                save_file_path = saver.save(sess, os.path.join(save_dir, \"{}.ckpt\".format(epoch//10)))\n",
    "                \n",
    "    file_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Test the result on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T14:37:12.678862Z",
     "start_time": "2018-01-18T14:37:12.406137Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from temp/1/20180119013331/saves/5.ckpt\n",
      "Accuracy for test set is: 0.9957\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    inits.run()\n",
    "    saver.restore(sess, save_file_path)\n",
    "    acc_test = sess.run(acc, feed_dict={X_04: X_test_04, y_04: y_test_04})\n",
    "    print(\"Accuracy for test set is: {:>4.4f}\".format(acc_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## \\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\n",
    "## Second iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T14:37:13.021453Z",
     "start_time": "2018-01-18T14:37:13.017125Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "iter_ind = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<Strong>Change</Strong>\n",
    "<ol>\n",
    "    <li>changed neuron per layer to 150\n",
    "    </li>\n",
    "    <li>changed activation function to elu for comparison\n",
    "    </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### define some variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T14:40:43.390644Z",
     "start_time": "2018-01-18T14:40:43.377638Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_size = mnist.train.num_examples\n",
    "n_epoch = 50\n",
    "batch_size = 200\n",
    "\n",
    "n_feature = mnist.train.images.shape[1]\n",
    "n_neuron = {\"n_hidden{}\".format(i) : 150 for i in range(6)}\n",
    "n_label = mnist.train.labels.shape[1]\n",
    "n_label_04 = 5\n",
    "\n",
    "d_dtype=tf.float64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### define some common functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T14:40:43.732313Z",
     "start_time": "2018-01-18T14:40:43.725749Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def my_dnn_layer(activation=None, \n",
    "                 kernel_initializer=tf.contrib.layers.variance_scaling_initializer()):\n",
    "    return partial(tf.layers.dense, activation=activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T14:40:44.141131Z",
     "start_time": "2018-01-18T14:40:44.133009Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_batch(X, y, batch_size, epoch, batch):\n",
    "    num_inst = X.shape[0]\n",
    "    np.random.seed(epoch * batch * np.random.randint(100))\n",
    "    shuffle_index = np.random.randint(num_inst, size=batch_size)\n",
    "    return X[shuffle_index], y[shuffle_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Build DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T14:40:45.095412Z",
     "start_time": "2018-01-18T14:40:45.091750Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T14:40:45.416421Z",
     "start_time": "2018-01-18T14:40:45.407148Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_04 = tf.placeholder(dtype=d_dtype, shape=(None, n_feature), name=\"X_04\")\n",
    "y_04 = tf.placeholder(dtype=d_dtype, shape=(None, n_label_04), name=\"y_04\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T14:40:45.802519Z",
     "start_time": "2018-01-18T14:40:45.720694Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "elu_layer = my_dnn_layer(activation=tf.nn.elu)\n",
    "linear_layer = my_dnn_layer(activation=None)\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = elu_layer(X_04, n_neuron[\"n_hidden1\"], name=\"hidden1\")\n",
    "    hidden2 = elu_layer(hidden1, n_neuron[\"n_hidden2\"], name=\"hidden2\")\n",
    "    hidden3 = elu_layer(hidden2, n_neuron[\"n_hidden3\"], name=\"hidden3\")\n",
    "    hidden4 = elu_layer(hidden3, n_neuron[\"n_hidden4\"], name=\"hidden4\")\n",
    "    hidden5 = elu_layer(hidden4, n_neuron[\"n_hidden5\"], name=\"hidden5\")\n",
    "    logits = linear_layer(hidden5, n_label_04, name=\"logits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T14:40:46.068366Z",
     "start_time": "2018-01-18T14:40:46.038493Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.softmax_cross_entropy_with_logits(labels=y_04, logits=logits, name=\"xentropy\")\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T14:40:46.289461Z",
     "start_time": "2018-01-18T14:40:46.266335Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    true_labels = tf.argmax(y_04, axis=1)\n",
    "    y_pred = tf.argmax(logits, axis=1)\n",
    "    all_acc, acc = tf.metrics.accuracy(labels=true_labels, predictions=y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T14:40:46.685628Z",
     "start_time": "2018-01-18T14:40:46.464915Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"train\"):\n",
    "    adam_optimizer = tf.train.AdamOptimizer()\n",
    "    train_op = adam_optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T14:41:00.737273Z",
     "start_time": "2018-01-18T14:41:00.478633Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "time_now = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "base_dir = os.path.join(\"temp\", str(iter_ind))\n",
    "save_dir = os.path.join(base_dir, time_now, \"saves\")\n",
    "log_dir = os.path.join(base_dir, time_now, \"logs\")\n",
    "\n",
    "with tf.name_scope(\"savNlog\"):\n",
    "    saver = tf.train.Saver()\n",
    "    file_writer = tf.summary.FileWriter(log_dir) # Need to add the graph\n",
    "    train_acc_log = tf.summary.scalar(\"train_accuracy\", acc)\n",
    "    validation_acc_log = tf.summary.scalar(\"validation_accuracy\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T14:41:00.999097Z",
     "start_time": "2018-01-18T14:41:00.992364Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"initializer\"):\n",
    "    inits = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T14:45:11.907733Z",
     "start_time": "2018-01-18T14:41:01.648679Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5  Training accuracy: 1.0000, \n",
      "Validation accuracy: 0.9840\n",
      "epoch: 10 Training accuracy: 0.9851, \n",
      "Validation accuracy: 0.9868\n",
      "epoch: 15 Training accuracy: 0.9872, \n",
      "Validation accuracy: 0.9888\n",
      "epoch: 20 Training accuracy: 0.9890, \n",
      "Validation accuracy: 0.9898\n",
      "epoch: 25 Training accuracy: 0.9900, \n",
      "Validation accuracy: 0.9906\n",
      "epoch: 30 Training accuracy: 0.9907, \n",
      "Validation accuracy: 0.9911\n",
      "epoch: 35 Training accuracy: 0.9912, \n",
      "Validation accuracy: 0.9915\n",
      "epoch: 40 Training accuracy: 0.9916, \n",
      "Validation accuracy: 0.9918\n",
      "epoch: 45 Training accuracy: 0.9919, \n",
      "Validation accuracy: 0.9920\n",
      "epoch: 50 Training accuracy: 0.9921, \n",
      "Validation accuracy: 0.9922\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(inits)\n",
    "    file_writer.add_graph(tf.get_default_graph())\n",
    "    tf.train.start_queue_runners(sess=sess)\n",
    "    for epoch in range(1, n_epoch+1):\n",
    "        for batch in range(1, np.ceil(train_size/batch_size).astype(int)+1):\n",
    "            X_batch, y_batch =  get_batch(X_train_04, y_train_04, batch_size, epoch, batch)\n",
    "            sess.run(train_op, feed_dict={X_04: X_batch, y_04: y_batch})\n",
    "        if not epoch % 5:\n",
    "            train_acc, log_train = sess.run([acc, train_acc_log], \n",
    "                                            feed_dict={X_04: X_batch, y_04: y_batch})\n",
    "            val_acc, log_val = sess.run([acc, validation_acc_log],\n",
    "                                        feed_dict={X_04: X_valid_04, y_04: y_valid_04})\n",
    "            file_writer.add_summary(log_train, epoch)\n",
    "            file_writer.add_summary(log_val, epoch)\n",
    "            print(\"epoch: {:<3}Training accuracy: {:<6.4f}, \\nValidation accuracy: {:<6.4f}\"\n",
    "                  .format(epoch, train_acc, val_acc))\n",
    "            if not epoch % 10:\n",
    "                save_file_path = saver.save(sess, os.path.join(save_dir, \"{}.ckpt\".format(epoch//10)))\n",
    "                \n",
    "    file_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Test the result on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T14:45:12.624793Z",
     "start_time": "2018-01-18T14:45:12.276399Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from temp/2/20180119014100/saves/5.ckpt\n",
      "Accuracy for test set is: 0.9953\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    inits.run()\n",
    "    saver.restore(sess, save_file_path)\n",
    "    acc_test = sess.run(acc, feed_dict={X_04: X_test_04, y_04: y_test_04})\n",
    "    print(\"Accuracy for test set is: {:>4.4f}\".format(acc_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## \\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\n",
    "## Third iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:27:22.613119Z",
     "start_time": "2018-01-18T15:27:22.609012Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "iter_ind = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<Strong>Change</Strong>\n",
    "<ol>\n",
    "    <li>Layer number back to 100 for faster training, same for selu\n",
    "    </li>\n",
    "    <li>Adding early stopping\n",
    "    </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### define some variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:27:23.348460Z",
     "start_time": "2018-01-18T15:27:23.337886Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_size = mnist.train.num_examples\n",
    "n_epoch = 50\n",
    "batch_size = 200\n",
    "max_drop = 5\n",
    "\n",
    "n_feature = mnist.train.images.shape[1]\n",
    "n_neuron = {\"n_hidden{}\".format(i) : 100 for i in range(6)}\n",
    "n_label = mnist.train.labels.shape[1]\n",
    "n_label_04 = 5\n",
    "\n",
    "d_dtype=tf.float64\n",
    "\n",
    "log_freq = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### define some common functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:27:24.064403Z",
     "start_time": "2018-01-18T15:27:24.059622Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def my_dnn_layer(activation=None, \n",
    "                 kernel_initializer=tf.contrib.layers.variance_scaling_initializer()):\n",
    "    return partial(tf.layers.dense, activation=activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:27:24.445745Z",
     "start_time": "2018-01-18T15:27:24.439675Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_batch(X, y, batch_size, epoch, batch):\n",
    "    num_inst = X.shape[0]\n",
    "    np.random.seed(epoch * batch * np.random.randint(100))\n",
    "    shuffle_index = np.random.randint(num_inst, size=batch_size)\n",
    "    return X[shuffle_index], y[shuffle_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Build DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:27:25.322846Z",
     "start_time": "2018-01-18T15:27:25.319169Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:27:25.680235Z",
     "start_time": "2018-01-18T15:27:25.665447Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_04 = tf.placeholder(dtype=d_dtype, shape=(None, n_feature), name=\"X_04\")\n",
    "y_04 = tf.placeholder(dtype=d_dtype, shape=(None, n_label_04), name=\"y_04\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:27:26.063400Z",
     "start_time": "2018-01-18T15:27:25.978162Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "selu_layer = my_dnn_layer(activation=tf.nn.selu)\n",
    "linear_layer = my_dnn_layer(activation=None)\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = selu_layer(X_04, n_neuron[\"n_hidden1\"], name=\"hidden1\")\n",
    "    hidden2 = selu_layer(hidden1, n_neuron[\"n_hidden2\"], name=\"hidden2\")\n",
    "    hidden3 = selu_layer(hidden2, n_neuron[\"n_hidden3\"], name=\"hidden3\")\n",
    "    hidden4 = selu_layer(hidden3, n_neuron[\"n_hidden4\"], name=\"hidden4\")\n",
    "    hidden5 = selu_layer(hidden4, n_neuron[\"n_hidden5\"], name=\"hidden5\")\n",
    "    logits = linear_layer(hidden5, n_label_04, name=\"logits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:27:26.351684Z",
     "start_time": "2018-01-18T15:27:26.309998Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.softmax_cross_entropy_with_logits(labels=y_04, logits=logits, name=\"xentropy\")\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:27:26.627796Z",
     "start_time": "2018-01-18T15:27:26.403645Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"train\"):\n",
    "    adam_optimizer = tf.train.AdamOptimizer()\n",
    "    train_op = adam_optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:27:27.059868Z",
     "start_time": "2018-01-18T15:27:27.033761Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    true_labels = tf.argmax(y_04, axis=1)\n",
    "    y_pred = tf.argmax(logits, axis=1)\n",
    "    all_acc, acc = tf.metrics.accuracy(labels=true_labels, predictions=y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:27:27.267655Z",
     "start_time": "2018-01-18T15:27:27.110576Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "time_now = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "base_dir = os.path.join(\"temp\", str(iter_ind))\n",
    "save_dir = os.path.join(base_dir, time_now, \"saves\")\n",
    "log_dir = os.path.join(base_dir, time_now, \"logs\")\n",
    "\n",
    "with tf.name_scope(\"savNlog\"):\n",
    "    saver = tf.train.Saver()\n",
    "    file_writer = tf.summary.FileWriter(log_dir) # Need to add the graph\n",
    "    train_acc_log = tf.summary.scalar(\"train_accuracy\", acc)\n",
    "    validation_acc_log = tf.summary.scalar(\"validation_accuracy\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:27:27.397440Z",
     "start_time": "2018-01-18T15:27:27.390490Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"initializer\"):\n",
    "    inits = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:30:06.097853Z",
     "start_time": "2018-01-18T15:27:27.762542Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5  Training accuracy: 1.0000, \n",
      "              Training loss: 0.002594\n",
      "                  Validation accuracy: 0.9880\n",
      "                      Validation loss: 0.047893\n",
      "0 0\n",
      "epoch: 10 Training accuracy: 0.9888, \n",
      "              Training loss: 0.000118\n",
      "                  Validation accuracy: 0.9898\n",
      "                      Validation loss: 0.054172\n",
      "0 1\n",
      "epoch: 15 Training accuracy: 0.9902, \n",
      "              Training loss: 0.001589\n",
      "                  Validation accuracy: 0.9911\n",
      "                      Validation loss: 0.044657\n",
      "0 0\n",
      "epoch: 20 Training accuracy: 0.9913, \n",
      "              Training loss: 0.000295\n",
      "                  Validation accuracy: 0.9912\n",
      "                      Validation loss: 0.054318\n",
      "0 1\n",
      "epoch: 25 Training accuracy: 0.9914, \n",
      "              Training loss: 0.000024\n",
      "                  Validation accuracy: 0.9910\n",
      "                      Validation loss: 0.055939\n",
      "1 2\n",
      "epoch: 30 Training accuracy: 0.9911, \n",
      "              Training loss: 0.005227\n",
      "                  Validation accuracy: 0.9907\n",
      "                      Validation loss: 0.083740\n",
      "2 3\n",
      "epoch: 35 Training accuracy: 0.9908, \n",
      "              Training loss: 0.000007\n",
      "                  Validation accuracy: 0.9910\n",
      "                      Validation loss: 0.042891\n",
      "3 0\n",
      "epoch: 40 Training accuracy: 0.9911, \n",
      "              Training loss: 0.000000\n",
      "                  Validation accuracy: 0.9912\n",
      "                      Validation loss: 0.043432\n",
      "4 1\n",
      "epoch: 45 Training accuracy: 0.9913, \n",
      "              Training loss: 0.000004\n",
      "                  Validation accuracy: 0.9914\n",
      "                      Validation loss: 0.044455\n",
      "0 2\n",
      "epoch: 50 Training accuracy: 0.9915, \n",
      "              Training loss: 0.000000\n",
      "                  Validation accuracy: 0.9916\n",
      "                      Validation loss: 0.045661\n",
      "0 3\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(inits)\n",
    "    \n",
    "    file_writer.add_graph(tf.get_default_graph())\n",
    "    \n",
    "    max_val_loss = np.inf\n",
    "    count_drop_loss = 0\n",
    "    best_model_loss = None\n",
    "    \n",
    "    max_val_acc = 0\n",
    "    count_drop_acc = 0\n",
    "    best_model_acc = None\n",
    "    \n",
    "    for epoch in range(1, n_epoch+1):\n",
    "        for batch in range(1, np.ceil(train_size/batch_size).astype(int)+1):\n",
    "            \n",
    "            X_batch, y_batch =  get_batch(X_train_04, y_train_04, batch_size, epoch, batch)\n",
    "            \n",
    "            sess.run(train_op, feed_dict={X_04: X_batch, y_04: y_batch})\n",
    "        if not epoch % log_freq:\n",
    "            train_acc, log_train, train_loss = sess.run([acc, train_acc_log, loss], \n",
    "                                            feed_dict={X_04: X_batch, y_04: y_batch})\n",
    "            val_acc, log_val, val_loss = sess.run([acc, validation_acc_log, loss],\n",
    "                                        feed_dict={X_04: X_valid_04, y_04: y_valid_04})\n",
    "            file_writer.add_summary(log_train, epoch)\n",
    "            file_writer.add_summary(log_val, epoch)\n",
    "            \n",
    "            print(\"epoch: {:<3}Training accuracy: {:<6.4f}, \\n              Training loss: {:<6.6f}\\n\\\n",
    "                  Validation accuracy: {:<6.4f}\\n                      Validation loss: {:<6.6f}\"\n",
    "                  .format(epoch, train_acc, train_loss, val_acc, val_loss))\n",
    "\n",
    "            save_file_path = saver.save(sess, os.path.join(save_dir, \"{}.ckpt\".format(epoch//log_freq)))\n",
    "            \n",
    "            if val_loss < max_val_loss:\n",
    "                max_val_loss = val_loss\n",
    "                best_model_loss = save_file_path\n",
    "                count_drop_loss = 0\n",
    "            else:\n",
    "                count_drop_loss += 1\n",
    "\n",
    "\n",
    "            if val_acc > max_val_acc:\n",
    "                max_val_acc = val_acc\n",
    "                best_model_acc = save_file_path\n",
    "                count_drop_acc = 0\n",
    "            else:\n",
    "                count_drop_acc += 1\n",
    "            if count_drop_acc > max_drop and count_drop_loss > max_drop:\n",
    "                break\n",
    "            print(count_drop_acc, count_drop_loss)\n",
    "                \n",
    "    file_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Test the result on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:30:06.909601Z",
     "start_time": "2018-01-18T15:30:06.589187Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from temp/3/20180119022727/saves/10.ckpt\n",
      "Accuracy for test set is: 0.9940\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    inits.run()\n",
    "    saver.restore(sess, best_model_acc)\n",
    "    acc_test = sess.run(acc, feed_dict={X_04: X_test_04, y_04: y_test_04})\n",
    "    print(\"Accuracy for test set is: {:>4.4f}\".format(acc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:30:07.888343Z",
     "start_time": "2018-01-18T15:30:07.658117Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from temp/3/20180119022727/saves/7.ckpt\n",
      "Accuracy for test set is: 0.9942\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    inits.run()\n",
    "    saver.restore(sess, best_model_loss)\n",
    "    acc_test = sess.run(acc, feed_dict={X_04: X_test_04, y_04: y_test_04})\n",
    "    print(\"Accuracy for test set is: {:>4.4f}\".format(acc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:30:07.888343Z",
     "start_time": "2018-01-18T15:30:07.658117Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from temp/3/20180119022727/saves/10.ckpt\n",
      "Accuracy for test set is: 0.9940\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    inits.run()\n",
    "    saver.restore(sess, save_file_path)\n",
    "    acc_test = sess.run(acc, feed_dict={X_04: X_test_04, y_04: y_test_04})\n",
    "    print(\"Accuracy for test set is: {:>4.4f}\".format(acc_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## \\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\n",
    "## Fourth iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:45:37.127395Z",
     "start_time": "2018-01-18T15:45:37.123729Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "iter_ind = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<Strong>Change</Strong>\n",
    "<ol>\n",
    "    <li>Adding BN\n",
    "    </li>\n",
    "<!--     <li>Adding early stopping\n",
    "    </li> -->\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### define some variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:52:41.883654Z",
     "start_time": "2018-01-18T15:52:41.873550Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_size = mnist.train.num_examples\n",
    "n_epoch = 100\n",
    "batch_size = 200\n",
    "max_drop = 5\n",
    "\n",
    "n_feature = mnist.train.images.shape[1]\n",
    "n_neuron = {\"n_hidden{}\".format(i) : 100 for i in range(6)}\n",
    "n_label = mnist.train.labels.shape[1]\n",
    "n_label_04 = 5\n",
    "\n",
    "d_dtype=tf.float64\n",
    "\n",
    "log_freq = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### define some common functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:52:42.457709Z",
     "start_time": "2018-01-18T15:52:42.453116Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def my_dnn_layer(**key_args):\n",
    "    return partial(tf.layers.dense, **key_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:52:42.788473Z",
     "start_time": "2018-01-18T15:52:42.783378Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def my_batch_norm(**key_args):\n",
    "    return partial(tf.layers.batch_normalization, **key_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:52:42.980388Z",
     "start_time": "2018-01-18T15:52:42.974047Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_batch(X, y, batch_size, epoch, batch):\n",
    "    num_inst = X.shape[0]\n",
    "    np.random.seed(epoch * batch * np.random.randint(100))\n",
    "    shuffle_index = np.random.randint(num_inst, size=batch_size)\n",
    "    return X[shuffle_index], y[shuffle_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Build DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:52:43.583972Z",
     "start_time": "2018-01-18T15:52:43.579232Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:52:43.600903Z",
     "start_time": "2018-01-18T15:52:43.592565Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_04 = tf.placeholder(dtype=d_dtype, shape=(None, n_feature), name=\"X_04\")\n",
    "y_04 = tf.placeholder(dtype=d_dtype, shape=(None, n_label_04), name=\"y_04\")\n",
    "training = tf.placeholder_with_default(False, None, name=\"training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:52:44.524207Z",
     "start_time": "2018-01-18T15:52:44.084202Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "selu_layer = my_dnn_layer(activation=tf.nn.selu)\n",
    "linear_layer = my_dnn_layer(activation=None)\n",
    "\n",
    "batch_layer = my_batch_norm(training=training)\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = selu_layer(X_04, n_neuron[\"n_hidden1\"], name=\"hidden1\")\n",
    "    bn1 = batch_layer(hidden1, name=\"bn1\")\n",
    "    hidden2 = selu_layer(bn1, n_neuron[\"n_hidden2\"], name=\"hidden2\")\n",
    "    bn2 = batch_layer(hidden2, name=\"bn2\")\n",
    "    hidden3 = selu_layer(bn2, n_neuron[\"n_hidden3\"], name=\"hidden3\")\n",
    "    bn3 = batch_layer(hidden3, name=\"bn3\")\n",
    "    hidden4 = selu_layer(bn3, n_neuron[\"n_hidden4\"], name=\"hidden4\")\n",
    "    bn4 = batch_layer(hidden4, name=\"bn4\")\n",
    "    hidden5 = selu_layer(bn4, n_neuron[\"n_hidden5\"], name=\"hidden5\")\n",
    "    bn5 = batch_layer(hidden5, name=\"bn5\")\n",
    "    logits = linear_layer(bn5, n_label_04, name=\"logits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:52:44.683806Z",
     "start_time": "2018-01-18T15:52:44.653379Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.softmax_cross_entropy_with_logits(labels=y_04, logits=logits, name=\"xentropy\")\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:52:45.885184Z",
     "start_time": "2018-01-18T15:52:45.165646Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"train\"):\n",
    "    batch_updates = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    adam_optimizer = tf.train.AdamOptimizer()\n",
    "    train_op = adam_optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:52:46.151431Z",
     "start_time": "2018-01-18T15:52:46.128725Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    true_labels = tf.argmax(y_04, axis=1)\n",
    "    y_pred = tf.argmax(logits, axis=1)\n",
    "    all_acc, acc = tf.metrics.accuracy(labels=true_labels, predictions=y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:52:46.602934Z",
     "start_time": "2018-01-18T15:52:46.413196Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "time_now = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "base_dir = os.path.join(\"temp\", str(iter_ind))\n",
    "save_dir = os.path.join(base_dir, time_now, \"saves\")\n",
    "log_dir = os.path.join(base_dir, time_now, \"logs\")\n",
    "\n",
    "with tf.name_scope(\"savNlog\"):\n",
    "    saver = tf.train.Saver(max_to_keep=20)\n",
    "    file_writer = tf.summary.FileWriter(log_dir) # Need to add the graph\n",
    "    train_acc_log = tf.summary.scalar(\"train_accuracy\", acc)\n",
    "    validation_acc_log = tf.summary.scalar(\"validation_accuracy\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:52:46.884447Z",
     "start_time": "2018-01-18T15:52:46.876134Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"initializer\"):\n",
    "    inits = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T16:03:03.640986Z",
     "start_time": "2018-01-18T15:52:47.128070Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5  Training accuracy: 0.9950, \n",
      "              Training loss: 0.005974\n",
      "                  Validation accuracy: 0.9902\n",
      "                      Validation loss: 0.033956\n",
      "0 0\n",
      "epoch: 10 Training accuracy: 0.9909, \n",
      "              Training loss: 0.004205\n",
      "                  Validation accuracy: 0.9898\n",
      "                      Validation loss: 0.036859\n",
      "1 1\n",
      "epoch: 15 Training accuracy: 0.9902, \n",
      "              Training loss: 0.001278\n",
      "                  Validation accuracy: 0.9908\n",
      "                      Validation loss: 0.045906\n",
      "0 2\n",
      "epoch: 20 Training accuracy: 0.9910, \n",
      "              Training loss: 0.000021\n",
      "                  Validation accuracy: 0.9913\n",
      "                      Validation loss: 0.041380\n",
      "0 3\n",
      "epoch: 25 Training accuracy: 0.9915, \n",
      "              Training loss: 0.000008\n",
      "                  Validation accuracy: 0.9918\n",
      "                      Validation loss: 0.036323\n",
      "0 4\n",
      "epoch: 30 Training accuracy: 0.9919, \n",
      "              Training loss: 0.000004\n",
      "                  Validation accuracy: 0.9922\n",
      "                      Validation loss: 0.037636\n",
      "0 5\n",
      "epoch: 35 Training accuracy: 0.9923, \n",
      "              Training loss: 0.000001\n",
      "                  Validation accuracy: 0.9925\n",
      "                      Validation loss: 0.038828\n",
      "0 6\n",
      "epoch: 40 Training accuracy: 0.9926, \n",
      "              Training loss: 0.000002\n",
      "                  Validation accuracy: 0.9927\n",
      "                      Validation loss: 0.040459\n",
      "0 7\n",
      "epoch: 45 Training accuracy: 0.9928, \n",
      "              Training loss: 0.000001\n",
      "                  Validation accuracy: 0.9929\n",
      "                      Validation loss: 0.041929\n",
      "0 8\n",
      "epoch: 50 Training accuracy: 0.9929, \n",
      "              Training loss: 0.000000\n",
      "                  Validation accuracy: 0.9930\n",
      "                      Validation loss: 0.044197\n",
      "0 9\n",
      "epoch: 55 Training accuracy: 0.9930, \n",
      "              Training loss: 0.000000\n",
      "                  Validation accuracy: 0.9930\n",
      "                      Validation loss: 0.045809\n",
      "0 10\n",
      "epoch: 60 Training accuracy: 0.9931, \n",
      "              Training loss: 0.000000\n",
      "                  Validation accuracy: 0.9931\n",
      "                      Validation loss: 0.048417\n",
      "0 11\n",
      "epoch: 65 Training accuracy: 0.9932, \n",
      "              Training loss: 0.000000\n",
      "                  Validation accuracy: 0.9932\n",
      "                      Validation loss: 0.050597\n",
      "0 12\n",
      "epoch: 70 Training accuracy: 0.9932, \n",
      "              Training loss: 0.000000\n",
      "                  Validation accuracy: 0.9932\n",
      "                      Validation loss: 0.052825\n",
      "0 13\n",
      "epoch: 75 Training accuracy: 0.9932, \n",
      "              Training loss: 0.000000\n",
      "                  Validation accuracy: 0.9933\n",
      "                      Validation loss: 0.055474\n",
      "0 14\n",
      "epoch: 80 Training accuracy: 0.9933, \n",
      "              Training loss: 0.000000\n",
      "                  Validation accuracy: 0.9933\n",
      "                      Validation loss: 0.058049\n",
      "0 15\n",
      "epoch: 85 Training accuracy: 0.9933, \n",
      "              Training loss: 0.000000\n",
      "                  Validation accuracy: 0.9934\n",
      "                      Validation loss: 0.060739\n",
      "0 16\n",
      "epoch: 90 Training accuracy: 0.9934, \n",
      "              Training loss: 0.000000\n",
      "                  Validation accuracy: 0.9934\n",
      "                      Validation loss: 0.062802\n",
      "0 17\n",
      "epoch: 95 Training accuracy: 0.9935, \n",
      "              Training loss: 0.000000\n",
      "                  Validation accuracy: 0.9935\n",
      "                      Validation loss: 0.065059\n",
      "0 18\n",
      "epoch: 100Training accuracy: 0.9935, \n",
      "              Training loss: 0.000000\n",
      "                  Validation accuracy: 0.9935\n",
      "                      Validation loss: 0.067404\n",
      "0 19\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(inits)\n",
    "    \n",
    "    file_writer.add_graph(tf.get_default_graph())\n",
    "    \n",
    "    max_val_loss = np.inf\n",
    "    count_drop_loss = 0\n",
    "    best_model_loss = None\n",
    "    \n",
    "    max_val_acc = 0\n",
    "    count_drop_acc = 0\n",
    "    best_model_acc = None\n",
    "    \n",
    "    for epoch in range(1, n_epoch+1):\n",
    "        for batch in range(1, np.ceil(train_size/batch_size).astype(int)+1):\n",
    "            \n",
    "            X_batch, y_batch =  get_batch(X_train_04, y_train_04, batch_size, epoch, batch)\n",
    "            \n",
    "            sess.run([train_op, batch_updates], feed_dict={X_04: X_batch, y_04: y_batch})\n",
    "        if not epoch % log_freq:\n",
    "            train_acc, log_train, train_loss = sess.run([acc, train_acc_log, loss], \n",
    "                                            feed_dict={X_04: X_batch, y_04: y_batch})\n",
    "            val_acc, log_val, val_loss = sess.run([acc, validation_acc_log, loss],\n",
    "                                        feed_dict={X_04: X_valid_04, y_04: y_valid_04})\n",
    "            file_writer.add_summary(log_train, epoch)\n",
    "            file_writer.add_summary(log_val, epoch)\n",
    "            \n",
    "            print(\"epoch: {:<3}Training accuracy: {:<6.4f}, \\n              Training loss: {:<6.6f}\\n\\\n",
    "                  Validation accuracy: {:<6.4f}\\n                      Validation loss: {:<6.6f}\"\n",
    "                  .format(epoch, train_acc, train_loss, val_acc, val_loss))\n",
    "\n",
    "            save_file_path = saver.save(sess, os.path.join(save_dir, \"{}.ckpt\".format(epoch//log_freq)))\n",
    "            \n",
    "            if val_loss < max_val_loss:\n",
    "                max_val_loss = val_loss\n",
    "                best_model_loss = save_file_path\n",
    "                count_drop_loss = 0\n",
    "            else:\n",
    "                count_drop_loss += 1\n",
    "\n",
    "\n",
    "            if val_acc > max_val_acc:\n",
    "                max_val_acc = val_acc\n",
    "                best_model_acc = save_file_path\n",
    "                count_drop_acc = 0\n",
    "            else:\n",
    "                count_drop_acc += 1\n",
    "            if count_drop_acc > max_drop and count_drop_loss > max_drop:\n",
    "                break\n",
    "            print(count_drop_acc, count_drop_loss)\n",
    "                \n",
    "    file_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Test the result on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T16:03:04.722625Z",
     "start_time": "2018-01-18T16:03:04.010731Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from temp/4/20180119025246/saves/20.ckpt\n",
      "Accuracy for test set is: 0.9946\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    inits.run()\n",
    "    saver.restore(sess, best_model_acc)\n",
    "    acc_test = sess.run(acc, feed_dict={X_04: X_test_04, y_04: y_test_04})\n",
    "    print(\"Accuracy for test set is: {:>4.4f}\".format(acc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T16:06:27.486133Z",
     "start_time": "2018-01-18T16:06:26.995012Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/apple/AnacondaProjects/handson-ml/chapter_11_exer/temp/4/20180119025246/saves/16.ckpt\n",
      "Accuracy for test set is: 0.9944\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    inits.run()\n",
    "    saver.restore(sess, best_model_loss)\n",
    "    acc_test = sess.run(acc, feed_dict={X_04: X_test_04, y_04: y_test_04})\n",
    "    print(\"Accuracy for test set is: {:>4.4f}\".format(acc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T16:06:28.940866Z",
     "start_time": "2018-01-18T16:06:28.446732Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from temp/4/20180119025246/saves/20.ckpt\n",
      "Accuracy for test set is: 0.9946\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    inits.run()\n",
    "    saver.restore(sess, save_file_path)\n",
    "    acc_test = sess.run(acc, feed_dict={X_04: X_test_04, y_04: y_test_04})\n",
    "    print(\"Accuracy for test set is: {:>4.4f}\".format(acc_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## \\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\n",
    "## Fifth iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:45:37.127395Z",
     "start_time": "2018-01-18T15:45:37.123729Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "iter_ind = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<Strong>Change</Strong>\n",
    "<ol>\n",
    "    <li>Parsing the structure into a sklearn func\n",
    "    </li>\n",
    "    <li>Changing the structure of saving file\n",
    "    </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-19T08:46:56.006848Z",
     "start_time": "2018-01-19T08:46:54.266798Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.exceptions import NotFittedError\n",
    "\n",
    "class DNNClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, n_hidden_layers=5, n_neurons=100, optimizer_class=tf.train.AdamOptimizer,\n",
    "                 learning_rate=0.01, n_epochs=100, batch_size=50, \n",
    "                 activation=tf.nn.selu, initializer=None,\n",
    "                 batch_norm=None, dropout_rate=None, \n",
    "                 regularizer=None, base_dir=\"temp\", \n",
    "                 log_freq=5, max_stop=5,\n",
    "                 random_state=None):\n",
    "        self.n_hidden_layers = n_hidden_layers\n",
    "        self.n_neurons = n_neurons\n",
    "        self.optimizer_class = optimizer_class\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_epochs = n_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.activation = activation\n",
    "        self.initializer= initializer\n",
    "        self.batch_norm = batch_norm\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.regularizer = regularizer\n",
    "        self.base_dir = base_dir\n",
    "        self.log_freq = log_freq\n",
    "        self.max_stop = max_stop\n",
    "        self.random_state = random_state\n",
    "        self._session = None\n",
    "        self._graph = None\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "    def _dnn_layout(self, inputs, n_labels):\n",
    "        \n",
    "        if type(self.n_neurons) is int:\n",
    "            self._n_neurons = (self.n_neurons for i in range(self.n_hidden_layers))\n",
    "        elif type(self.n_neurons) is list:\n",
    "            self._n_neurons = iter(self.n_neurons)\n",
    "        else:\n",
    "            raise TypeError(\"The type of n_neurons can't be %s\" % str(type(self.n_neurons)))\n",
    "        with tf.name_scope(\"dnn\"):\n",
    "            for lay in range(1, self.n_hidden_layers+1):\n",
    "                if self.dropout_rate:\n",
    "                    inputs = tf.layers.dropout(\n",
    "                                      inputs, \n",
    "                                      rate=self.dropout_rate, \n",
    "                                      training=self._training, \n",
    "                                      name=\"dropout%d\" % lay\n",
    "                                    )\n",
    "                inputs = tf.layers.dense(\n",
    "                                    inputs,\n",
    "                                    next(self._n_neurons),\n",
    "                                    activation=self.activation,\n",
    "                                    kernel_initializer=self.initializer,\n",
    "                                    kernel_regularizer=self.regularizer,\n",
    "                                    trainable=True,\n",
    "                                    name=\"hidden%d\" % lay\n",
    "                                )\n",
    "                if self.batch_norm:\n",
    "                    inputs = tf.layers.batch_normalization(\n",
    "                                        inputs,\n",
    "                                        training=self._training,\n",
    "                                        name=\"bn%d\" % lay\n",
    "                                    )\n",
    "            logits = tf.layers.dense(\n",
    "                                    inputs,\n",
    "                                    n_labels,\n",
    "                                    name=\"logits\"\n",
    "                                )\n",
    "            return logits\n",
    "\n",
    "    def _build_graph(self, n_features, n_labels):\n",
    "        \"\"\"Build the same model as earlier\"\"\"\n",
    "        if self.random_state is not None:\n",
    "            tf.set_random_seed(self.random_state)\n",
    "            np.random.seed(self.random_state)\n",
    "        self._graph = tf.Graph()\n",
    "        self._graph.as_default()\n",
    "        \n",
    "        X = tf.placeholder(\n",
    "                    tf.float32,\n",
    "                    shape=(None, n_features),\n",
    "                    name=\"X\"\n",
    "                )\n",
    "        y = tf.placeholder(\n",
    "                    tf.float32,\n",
    "                    shape=(None, n_labels),\n",
    "                    name=\"y\"\n",
    "                )\n",
    "\n",
    "        self._training = tf.placeholder_with_default(False, None, name=\"training\")\n",
    "        \n",
    "        logits = self._dnn_layout(X, n_labels)\n",
    "        y_proba = tf.nn.softmax(logits, name=\"y_proba\")\n",
    "        \n",
    "        with tf.name_scope(\"loss\"):\n",
    "            xentropy = tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=logits, name=\"xentropy\")\n",
    "            loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "            \n",
    "        with tf.name_scope(\"train\"):\n",
    "            batch_updates = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "            optimizer = self.optimizer_class()\n",
    "            training_op = optimizer.minimize(loss)\n",
    "            \n",
    "        with tf.name_scope(\"eval\"):\n",
    "            true_labels = tf.argmax(y, axis=1)\n",
    "            y_pred = tf.argmax(logits, axis=1)\n",
    "            all_acc, acc = tf.metrics.accuracy(labels=true_labels, predictions=y_pred)\n",
    "            \n",
    "        time_now = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "        save_dir = os.path.join(base_dir, \"saves\", time_now)\n",
    "        log_dir = os.path.join(base_dir, \"logs\", time_now)\n",
    "        if not os.path.isdir(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "        if not os.path.isdir(log_dir):\n",
    "            os.makedirs(log_dir)\n",
    "        with tf.name_scope(\"savNlog\"):\n",
    "            saver = tf.train.Saver(max_to_keep=20)\n",
    "            file_writer = tf.summary.FileWriter(log_dir) # Need to add the graph\n",
    "            train_acc_log = tf.summary.scalar(\"train_accuracy\", acc)\n",
    "            validation_acc_log = tf.summary.scalar(\"validation_accuracy\", acc)\n",
    "\n",
    "            \n",
    "        with tf.name_scope(\"initializer\"):\n",
    "            inits = tf.group(tf.global_variables_initializer(),\n",
    "                                    tf.local_variables_initializer())\n",
    "            \n",
    "        \n",
    "        self._X = X\n",
    "        self._y = y\n",
    "        self._y_proba = y_proba\n",
    "        self._logits = logits\n",
    "        self._xentropy = xentropy\n",
    "        self._loss = loss\n",
    "        self._batch_updates = batch_updates\n",
    "        self._y_pred = y_pred\n",
    "        self._optimizer = optimizer\n",
    "        self._training_op = training_op\n",
    "        self._true_labels = true_labels\n",
    "        self._acc = acc\n",
    "        self._save_dir = save_dir\n",
    "        self._log_dir = log_dir\n",
    "        self._saver = saver\n",
    "        self._file_writer = file_writer\n",
    "        self._train_acc_log = train_acc_log\n",
    "        self._validation_acc_log = validation_acc_log\n",
    "        self._inits = inits\n",
    "\n",
    "    def close_session(self):\n",
    "        if self._session:\n",
    "            self._session.close()\n",
    "            \n",
    "    def _get_batch(self, X, y, batch_size, epoch, batch):\n",
    "        num_inst = X.shape[0]\n",
    "        np.random.seed(epoch * batch * np.random.randint(100))\n",
    "        shuffle_index = np.random.randint(num_inst, size=batch_size)\n",
    "        return X[shuffle_index], y[shuffle_index]\n",
    "\n",
    "    def fit(self, X, y, X_val=None, y_val=None, model=\"loss\"):\n",
    "        self.close_session()\n",
    "        if self._graph:\n",
    "            print(\"Resetting Old Graph\")\n",
    "            tf.reset_default_graph()\n",
    "        \n",
    "        n_features = X.shape[1]\n",
    "        n_labels = y.shape[1]\n",
    "        self._build_graph(n_features, n_labels)\n",
    "        \n",
    "        training_size = X.shape[0]\n",
    "        self._file_writer.add_graph(tf.get_default_graph())\n",
    "        \n",
    "        if X_val is not None and y_val is not None:\n",
    "            max_val_loss = np.inf\n",
    "            count_drop_loss = 0\n",
    "            self.best_model_loss_ = None\n",
    "\n",
    "            max_val_acc = 0\n",
    "            count_drop_acc = 0\n",
    "            self.best_model_acc_ = None\n",
    "        \n",
    "\n",
    "        sess = tf.Session()\n",
    "        self._session = sess\n",
    "        sess.as_default()\n",
    "        \n",
    "        sess.run(self._inits)\n",
    "        \n",
    "        for epoch in range(1, self.n_epochs+1):\n",
    "            for batch in range(1, np.ceil(training_size/self.batch_size).astype(int)+1):\n",
    "                X_batch, y_batch =  self._get_batch(X, y, self.batch_size, \n",
    "                                                    epoch, batch)\n",
    "\n",
    "                sess.run([self._training_op, self._batch_updates], \n",
    "                         feed_dict={self._X:X_batch, self._y: y_batch, self._training:True})\n",
    "            if not epoch % self.log_freq:\n",
    "                train_acc, log_train, train_loss = sess.run(\n",
    "                                                    [self._acc, self._train_acc_log, self._loss], \n",
    "                                                    feed_dict={self._X: X_batch, self._y: y_batch})\n",
    "\n",
    "                self._file_writer.add_summary(log_train, epoch)\n",
    "\n",
    "\n",
    "                save_file_path = self._saver.save(sess, os.path.join(self._save_dir, \n",
    "                                                                \"{}.ckpt\".format(epoch//log_freq)))\n",
    "                \n",
    "                self.last_epoch_model = save_file_path\n",
    "                \n",
    "                if X_val is not None and y_val is not None:\n",
    "                    val_acc, log_val, val_loss = sess.run(\n",
    "                                                    [self._acc, self._validation_acc_log, self._loss],\n",
    "                                                    feed_dict={self._X: X_val, self._y: y_val})\n",
    "                    self._file_writer.add_summary(log_val, epoch)\n",
    "\n",
    "                    if val_loss < max_val_loss:\n",
    "                        max_val_loss = val_loss\n",
    "                        self.best_model_loss_ = save_file_path\n",
    "                        count_drop_loss = 0\n",
    "                    else:\n",
    "                        count_drop_loss += 1\n",
    "\n",
    "\n",
    "                    if val_acc > max_val_acc:\n",
    "                        max_val_acc = val_acc\n",
    "                        self.best_model_acc_ = save_file_path\n",
    "                        count_drop_acc = 0\n",
    "                    else:\n",
    "                        count_drop_acc += 1\n",
    "\n",
    "                    print(\"epoch: {:<3}Training accuracy: {:<6.4f}, \\n\\\n",
    "              Training loss: {:<6.6f}\\n\\\n",
    "                                        Validation accuracy: {:<6.4f}\\n\\\n",
    "                                            Validation loss: {:<6.6f}\"\n",
    "                      .format(epoch, train_acc, train_loss, val_acc, val_loss))\n",
    "                    if count_drop_acc > self.max_stop and count_drop_loss > self.max_stop:\n",
    "                        print(\"Early stopping!\\n    Count acc = %d\\n    Count loss = %d\" \\\n",
    "                              % (count_drop_acc, count_drop_loss))\n",
    "                        self._restore_params(model)\n",
    "                        break\n",
    "                else:\n",
    "                    print(\"epoch: {:<3}Training accuracy: {:<6.4f}, \\n\\\n",
    "                              Training loss: {:<6.6f}\\n\".format(epoch, train_acc, train_loss))\n",
    "\n",
    "        self._file_writer.close()\n",
    "        \n",
    "    \n",
    "    def _restore_params(self, model):\n",
    "        with self._session.as_default() as sess:\n",
    "            if model == \"acc\":\n",
    "                self._saver.restore(sess, self.best_model_acc_)\n",
    "            elif model == \"loss\":\n",
    "                self._saver.restore(sess, self.best_model_loss_)\n",
    "            else:\n",
    "                try:\n",
    "                     self._saver.restore(sess, model)\n",
    "                except:\n",
    "                    raise AttributeError(\"Restore for '%s' is invalid\" % str(model))\n",
    "\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        if self._session is None:\n",
    "            raise NotFittedError(\"This %s instamance is not fitted yet\" % self.__class__.__name__)\n",
    "        with self._session.as_default() as sess:\n",
    "            return self._y_proba.eval(feed_dict={self._X: X})\n",
    "        \n",
    "    def predict(self, X, onehot=True):\n",
    "        if self._session is None:\n",
    "            raise NotFittedError(\"This %s instance is not fitted yet\" % self.__class__.__name__)\n",
    "        with self._session.as_default() as sess:\n",
    "            if onehot:\n",
    "                pred_proba = self._y_proba.eval(feed_dict={self._X: X})\n",
    "                return (pred_proba == pred_proba.max(axis=1)[:,None]).astype(int)\n",
    "            return self._y_pred.eval(feed_dict={self._X: X})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-19T08:46:56.127377Z",
     "start_time": "2018-01-19T08:46:56.120256Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "he_init = tf.contrib.layers.variance_scaling_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-19T08:46:57.128663Z",
     "start_time": "2018-01-19T08:46:57.121548Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dnn_clf = DNNClassifier(n_hidden_layers=6, n_neurons=120, optimizer_class=tf.train.AdamOptimizer,\n",
    "                 learning_rate=0.05, n_epochs=10, batch_size=100, \n",
    "                 activation=tf.nn.selu, initializer=None,\n",
    "                 batch_norm=True, dropout_rate=0.5, \n",
    "                 regularizer=None, base_dir=\"temp\", \n",
    "                 log_freq=5, max_stop = 5,\n",
    "                 random_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-19T08:47:37.535062Z",
     "start_time": "2018-01-19T08:46:58.840015Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5  Training accuracy: 1.0000, \n",
      "              Training loss: 0.008145\n",
      "                                        Validation accuracy: 0.9872\n",
      "                                            Validation loss: 0.045174\n",
      "epoch: 10 Training accuracy: 0.9873, \n",
      "              Training loss: 0.038048\n",
      "                                        Validation accuracy: 0.9870\n",
      "                                            Validation loss: 0.057140\n"
     ]
    }
   ],
   "source": [
    "dnn_clf.fit(X_train_04, y_train_04, X_valid_04, y_valid_04)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Test the result on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-19T08:47:37.679203Z",
     "start_time": "2018-01-19T08:47:37.670633Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-19T08:47:38.221764Z",
     "start_time": "2018-01-19T08:47:37.848593Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from temp/4/saves/20180119194700/2.ckpt\n",
      "The final model: Accuracy for test set is: 0.9879\n"
     ]
    }
   ],
   "source": [
    "dnn_clf._restore_params(dnn_clf.last_epoch_model)\n",
    "y_pred = dnn_clf.predict(X_test_04)\n",
    "acc_test = accuracy_score(y_test_04, y_pred)\n",
    "print(\"The final model: Accuracy for test set is: {:>4.4f}\".format(acc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-19T08:47:38.538545Z",
     "start_time": "2018-01-19T08:47:38.325760Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from temp/4/saves/20180119194700/1.ckpt\n",
      "The best validation acc model: Accuracy for test set is: 0.9887\n"
     ]
    }
   ],
   "source": [
    "dnn_clf._restore_params(\"acc\")\n",
    "y_pred = dnn_clf.predict(X_test_04)\n",
    "acc_test = accuracy_score(y_test_04, y_pred)\n",
    "print(\"The best validation acc model: Accuracy for test set is: {:>4.4f}\".format(acc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-19T08:47:38.872946Z",
     "start_time": "2018-01-19T08:47:38.690009Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from temp/4/saves/20180119194700/1.ckpt\n",
      "The best validation loss model: Accuracy for test set is: 0.9887\n"
     ]
    }
   ],
   "source": [
    "dnn_clf._restore_params(\"loss\")\n",
    "y_pred = dnn_clf.predict(X_test_04)\n",
    "acc_test = accuracy_score(y_test_04, y_pred)\n",
    "print(\"The best validation loss model: Accuracy for test set is: {:>4.4f}\"\n",
    "      .format(acc_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\n",
    "## Sixth iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-20T04:20:54.374812Z",
     "start_time": "2018-01-20T04:20:54.370945Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iter_ind = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Strong>Change</Strong>\n",
    "<ol>\n",
    "    <li>Adding randomized grid search\n",
    "    </li>\n",
    "    <li>Fixed bugs with accuracy and graph logging\n",
    "    </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-30T02:10:27.801757Z",
     "start_time": "2018-01-30T02:10:21.065121Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.exceptions import NotFittedError\n",
    "from datetime import datetime\n",
    "\n",
    "class DNNClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"\n",
    "    Brief: This class implements the methods with sklearn compatible \n",
    "           structure,and is aimed to enable training with sklearn \n",
    "           RandomizedSearchCV class of sklearn.\n",
    "           Available for one-hot encoded labels.\n",
    "           \n",
    "           You need call close_session()\n",
    "    \n",
    "    Authorship: Calvin Zhuoqun Huang, 19 Jan 2018\n",
    "    \n",
    "    Requirements: tensorflow 1.4 and above\n",
    "                  numpy 1.13 and above\n",
    "                  sklearn 0.19 and above\n",
    "                  \n",
    "    Known issues: A lot :D\n",
    "                  Accuracy implementation is not entirely correct\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_hidden_layers=5, n_neurons=100, \n",
    "                 optimizer_class=tf.train.AdamOptimizer,\n",
    "                 learning_rate=0.01, n_epochs=100, batch_size=50, \n",
    "                 activation=tf.nn.selu, initializer=None,\n",
    "                 batch_norm_momentum=None, dropout_rate=None, \n",
    "                 regularizer=None, reg_scale=0.001, base_dir=\"temp\", \n",
    "                 log_freq=5, max_stop=5,\n",
    "                 random_state=None):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        n_hidden_layers: \n",
    "            Type: int, default to 5\n",
    "            Number of neural network layers you wish to use\n",
    "\n",
    "        n_neurons: \n",
    "            Type: int or sequence type containing int, default to 100.\n",
    "            Number of neurons you wish to use in each layer.\n",
    "            If integer is passed, then implements constant number of\n",
    "            neuron each layer\n",
    "\n",
    "\n",
    "        optimizer_class: \n",
    "            Type: default to tf.train.AdamOptimizer\n",
    "            Optimizer class from tensorflow train optimizer or any\n",
    "            other compatible optimizer\n",
    "\n",
    "\n",
    "        learning_rate: \n",
    "            Type: float, default to 0.01\n",
    "            Learning rate of the neural network.\n",
    "        \n",
    "        n_epochs:\n",
    "            Type: int, defaults to 100\n",
    "            The number of full epoches get used to train the model\n",
    "        \n",
    "        batch_size:\n",
    "            Type: int, defaults to 50\n",
    "            The number of examples in each batch\n",
    "            \n",
    "        activation:\n",
    "            Type: function, defaults to tf.nn.selu\n",
    "            The activation functions used in hidden layers\n",
    "            \n",
    "        batch_norm_momentum:\n",
    "            Type: 0 < float < 1, defaults to None\n",
    "            When given, applies batch normalization to the model\n",
    "            \n",
    "        dropout_rate:\n",
    "            Type: 0 < float < 1, defaults to None\n",
    "            When given, applies dropout regularization to the model\n",
    "        \n",
    "        regularizer:\n",
    "            Type: defaults to None\n",
    "            The type of regularization applied to hidden layer.\n",
    "            Supports tf.contrib.layers.l2/l1regularizer.\n",
    "        \n",
    "        reg_scale:\n",
    "            Type: float, defaults to 0.001\n",
    "            paramter beta for regularization.\n",
    "            \n",
    "        base_dir:\n",
    "            Type: str or os.path type\n",
    "            The directory you wantted to store the log file and session\n",
    "            record(maxima 20). With unique timestamp on the folder and\n",
    "            Record number on the files.\n",
    "            \n",
    "        log_freq:\n",
    "            Type: int, defaults to 5\n",
    "            The interval of epochs for storing logs and session parameters\n",
    "            \n",
    "        max_stop:\n",
    "            Type: int, defaults to 5\n",
    "            Specify the number of intervals before early stopping is \n",
    "            happens if both the accuracy and loss on\n",
    "            validation set drops belows the best mark for that long interval\n",
    "            (i.e. max_stop * log_freq)\n",
    "            (Only triggers if during fit stage the validation set X and y\n",
    "            are given)\n",
    "            \n",
    "        random_state:\n",
    "            Type: int, defaults to None\n",
    "            Random state for tensorflow random and numpy random.\n",
    "            \n",
    "            \n",
    "        Return: None\n",
    "        \"\"\"\n",
    "        self.n_hidden_layers = n_hidden_layers\n",
    "        self.n_neurons = n_neurons\n",
    "        self.optimizer_class = optimizer_class\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_epochs = n_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.activation = activation\n",
    "        self.initializer= initializer\n",
    "        self.batch_norm_momentum = batch_norm_momentum\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.regularizer = regularizer\n",
    "        self.reg_scale = reg_scale\n",
    "        self.base_dir = base_dir\n",
    "        self.log_freq = log_freq\n",
    "        self.max_stop = max_stop\n",
    "        self.random_state = random_state\n",
    "        self._session = None\n",
    "        self._graph = None\n",
    "        \n",
    "    def _dnn_layout(self, inputs):\n",
    "        \"\"\"\n",
    "        Constructs the deep neural network part of the model\n",
    "        \n",
    "        Parameters:\n",
    "        inputs: \n",
    "            The inputs tensor for deep NN\n",
    "\n",
    "\n",
    "        Return: output from the last hidden layer or batch norm\n",
    "        \n",
    "        TensorName:\n",
    "            dropout1...n, hidden1...n, bn1...n \n",
    "        \"\"\"\n",
    "        if type(self.n_neurons) is int:\n",
    "#             convert the number of neurons to a generator\n",
    "            self._n_neurons = (self.n_neurons for i in \n",
    "                               range(self.n_hidden_layers))\n",
    "        elif type(self.n_neurons) is list:\n",
    "            self._n_neurons = iter(self.n_neurons)\n",
    "        else:\n",
    "            raise TypeError(\"The type of n_neurons can't be %s\" \n",
    "                            % str(type(self.n_neurons)))\n",
    "        with self._graph.name_scope(\"dnn\"):\n",
    "            for lay in range(1, self.n_hidden_layers+1):\n",
    "                if self.dropout_rate:\n",
    "#                     Dropout layer\n",
    "                    inputs = tf.layers.dropout(\n",
    "                                      inputs, \n",
    "                                      rate=self.dropout_rate, \n",
    "                                      training=self._training, \n",
    "                                      name=\"dropout%d\" % lay\n",
    "                                    )\n",
    "                    self._graph.add_to_collections(\"dnn_ops\", inputs)\n",
    "#                 Hidden layer\n",
    "                inputs = tf.layers.dense(\n",
    "                                    inputs,\n",
    "                                    next(self._n_neurons),\n",
    "                                    activation=self.activation,\n",
    "                                    kernel_initializer=\n",
    "                                        None if not self.initializer else \n",
    "                                        self.initializer(),\n",
    "                                    kernel_regularizer=\n",
    "                                        None if not self.regularizer else \n",
    "                                        self.regularizer(self.reg_scale),\n",
    "                                    trainable=True,\n",
    "                                    name=\"hidden%d\" % lay\n",
    "                                )\n",
    "                self._graph.add_to_collections(\"dnn_ops\", inputs)\n",
    "                if self.batch_norm_momentum:\n",
    "#                     BN layer\n",
    "                    inputs = tf.layers.batch_normalization(\n",
    "                                        inputs,\n",
    "                                        momentum=self.batch_norm_momentum,\n",
    "                                        training=self._training,\n",
    "                                        name=\"bn%d\" % lay\n",
    "                                    )\n",
    "                    self._graph.add_to_collections(\"dnn_ops\", inputs)\n",
    "            return inputs\n",
    "\n",
    "    def _build_graph(self, n_features, n_labels):\n",
    "        \"\"\"Build the full model with all units\"\"\"\n",
    "        if self.random_state is not None:\n",
    "            tf.set_random_seed(self.random_state)\n",
    "            np.random.seed(self.random_state)\n",
    "\n",
    "        if self._graph is None:\n",
    "            self._graph = tf.Graph()\n",
    "\n",
    "        with self._graph.as_default() as graph:\n",
    "            X = tf.placeholder(\n",
    "                        tf.float32,\n",
    "                        shape=(None, n_features),\n",
    "                        name=\"X\"\n",
    "                    )\n",
    "            y = tf.placeholder(\n",
    "                        tf.float32,\n",
    "                        shape=(None, n_labels),\n",
    "                        name=\"y\"\n",
    "                    )\n",
    "\n",
    "            self._training = tf.placeholder_with_default(False, None, \n",
    "                                                         name=\"training\")\n",
    "\n",
    "\n",
    "            inputs = self._dnn_layout(X)\n",
    "            with self._graph.name_scope(\"outputs\"):\n",
    "                logits = tf.layers.dense(\n",
    "                            inputs,\n",
    "                            n_labels,\n",
    "                            name=\"logits\"\n",
    "                        )\n",
    "                y_proba = tf.nn.softmax(logits, name=\"y_proba\")\n",
    "                y_pred = tf.argmax(logits, axis=1, name=\"y_pred\")\n",
    "                self._graph.add_to_collections(\"output_ops\", logits)\n",
    "                self._graph.add_to_collections(\"output_ops\", y_pred)\n",
    "                self._graph.add_to_collections(\"output_ops\", y_proba)\n",
    "\n",
    "            with self._graph.name_scope(\"loss\"):\n",
    "                xentropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "                                    labels=y, logits=logits, name=\"xentropy\")\n",
    "                base_loss = tf.reduce_mean(xentropy, name=\"base_loss\")\n",
    "                reg_loss = tf.get_collection(\n",
    "                    tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "                loss = tf.add_n([base_loss]+reg_loss, name=\"loss\")\n",
    "                self._graph.add_to_collections(\"loss_ops\", base_loss)\n",
    "                self._graph.add_to_collections(\"loss_ops\", loss)\n",
    "                self._graph.add_to_collections(\"loss_ops\", xentropy)\n",
    "\n",
    "            with self._graph.name_scope(\"train\"):\n",
    "                optimizer = self.optimizer_class(self.learning_rate, \n",
    "                                                 name=\"optimizer\")\n",
    "                update_ops = self._graph.get_collection(\n",
    "                    tf.GraphKeys.UPDATE_OPS)\n",
    "                training_op = optimizer.minimize(loss, \n",
    "                                                 name=\"training_op\")\n",
    "                self._graph.add_to_collections(\"train_ops\", training_op)\n",
    "\n",
    "            with self._graph.name_scope(\"eval\"):\n",
    "                true_labels = tf.argmax(y, axis=1, name=\"true_labels\")\n",
    "                all_acc, acc = tf.metrics.accuracy(labels=true_labels, \n",
    "                                                   predictions=y_pred,\n",
    "                                                   name=\"accuracy\")\n",
    "                self._graph.add_to_collections(\"eval_ops\", acc)\n",
    "                self._graph.add_to_collections(\"eval_ops\", true_labels)\n",
    "\n",
    "            time_now = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "            save_dir = os.path.join(self.base_dir, \"saves\", time_now)\n",
    "            log_dir = os.path.join(self.base_dir, \"logs\", time_now)\n",
    "    #         if the path for save and log don't exist, create them.\n",
    "            if not os.path.isdir(save_dir):\n",
    "                os.makedirs(save_dir)\n",
    "            if not os.path.isdir(log_dir):\n",
    "                os.makedirs(log_dir)\n",
    "            with self._graph.name_scope(\"savNlog\"):\n",
    "                saver = tf.train.Saver(max_to_keep=20)\n",
    "                file_writer = tf.summary.FileWriter(log_dir)\n",
    "                train_acc_log = tf.summary.scalar(\"train_accuracy\", acc)\n",
    "                validation_acc_log = tf.summary.scalar(\"validation_accuracy\",\n",
    "                                                       acc)\n",
    "                train_loss_log = tf.summary.scalar(\"train_loss\", loss)\n",
    "                validation_loss_log = tf.summary.scalar(\"validation_loss\",\n",
    "                                                        loss)\n",
    "\n",
    "\n",
    "            with self._graph.name_scope(\"initializer\"):\n",
    "    #             Bundle the two initializers together\n",
    "                inits = tf.group(tf.global_variables_initializer(),\n",
    "                                        tf.local_variables_initializer())\n",
    "\n",
    "            self._X = X\n",
    "            self._y = y\n",
    "            self._y_proba = y_proba\n",
    "            self._logits = logits\n",
    "            self._xentropy = xentropy\n",
    "            self._loss = loss\n",
    "            self._y_pred = y_pred\n",
    "            self._update_ops = update_ops\n",
    "            self._optimizer = optimizer\n",
    "            self._training_op = training_op\n",
    "            self._true_labels = true_labels\n",
    "            self._acc = acc\n",
    "            self._save_dir = save_dir\n",
    "            self._log_dir = log_dir\n",
    "            self._saver = saver\n",
    "            self._file_writer = file_writer\n",
    "            self._train_acc_log = train_acc_log\n",
    "            self._validation_acc_log = validation_acc_log\n",
    "            self._train_loss_log = train_loss_log\n",
    "            self._validation_loss_log = validation_loss_log\n",
    "            self._inits = inits\n",
    "\n",
    "    def close_session(self):\n",
    "        \"\"\"\n",
    "        Close the session\n",
    "        \"\"\"\n",
    "        if self._session:\n",
    "            self._session.close()\n",
    "    \n",
    "    @staticmethod\n",
    "    def _get_batch(X, y, batch_size, epoch, batch):\n",
    "        \"\"\"\n",
    "        get a batch of training set\n",
    "        \"\"\"\n",
    "        num_inst = X.shape[0]\n",
    "        np.random.seed(epoch * batch * np.random.randint(100))\n",
    "        shuffle_index = np.random.randint(num_inst, size=batch_size)\n",
    "        return X[shuffle_index], y[shuffle_index]\n",
    "\n",
    "    def fit(self, X, y, X_val=None, y_val=None, model=\"loss\"):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        X:\n",
    "            Training sets' data of shape [n_example, n_feature]\n",
    "        y:\n",
    "            Training sets' labels of shape [n_example, n_labels]\n",
    "        X_val:\n",
    "            Valiadtion sets data used for early stopping\n",
    "        y_val:\n",
    "            Valiadtion sets labels used for early stopping\n",
    "        model:\n",
    "            string: \"loss\" or \"acc\"\n",
    "            in case of early stopping, restore the model with best\n",
    "            loss on validation set or model with best accuracy on \n",
    "            validation set\n",
    "            \n",
    "        Return: None\n",
    "        \"\"\"\n",
    "        self.close_session()\n",
    "        \n",
    "#         Setting up used dimension variable\n",
    "        n_features = X.shape[1]\n",
    "        n_labels = y.shape[1]\n",
    "#         Building up the graph\n",
    "        self._build_graph(n_features, n_labels)\n",
    "        \n",
    "        training_size = X.shape[0]\n",
    "        self._file_writer.add_graph(self._graph)\n",
    "        \n",
    "        if X_val is not None and y_val is not None:\n",
    "            max_val_loss = np.inf\n",
    "            count_drop_loss = 0\n",
    "            self.best_model_loss_ = None\n",
    "\n",
    "            max_val_acc = 0\n",
    "            count_drop_acc = 0\n",
    "            self.best_model_acc_ = None\n",
    "\n",
    "        self._session = tf.Session(graph=self._graph)\n",
    "        with self._session.as_default() as sess:\n",
    "            sess.run(self._inits)\n",
    "            for epoch in range(1, self.n_epochs+1):\n",
    "                for batch in range(1, np.ceil(training_size/\n",
    "                                          self.batch_size).astype(int)+1):\n",
    "\n",
    "                    X_batch, y_batch =  self._get_batch(X, y, \n",
    "                                                        self.batch_size, \n",
    "                                                        epoch, batch)\n",
    "\n",
    "                    sess.run([self._training_op, self._update_ops], \n",
    "                             feed_dict={self._X:X_batch, \n",
    "                                        self._y: y_batch, \n",
    "                                        self._training:True})\n",
    "                if not epoch % self.log_freq:\n",
    "                    train_acc, train_loss, log_train_acc, log_train_loss = \\\n",
    "                                        sess.run(\n",
    "                                            [self._acc, \n",
    "                                             self._loss, \n",
    "                                             self._train_acc_log, \n",
    "                                             self._train_loss_log], \n",
    "                                            feed_dict={self._X: X, \n",
    "                                                       self._y: y})\n",
    "\n",
    "                    self._file_writer.add_summary(log_train_acc, epoch)\n",
    "\n",
    "\n",
    "                    save_file_path = self._saver.save(sess, \n",
    "                                        os.path.join(self._save_dir, \n",
    "                                            \"{}.ckpt\".format(epoch//\n",
    "                                                             self.log_freq)))\n",
    "\n",
    "                    self.last_epoch_model = save_file_path\n",
    "\n",
    "                    if X_val is not None and y_val is not None:\n",
    "                        val_acc, val_loss, log_val_acc, log_val_loss = \\\n",
    "                                            sess.run(\n",
    "                                                [self._acc,  \n",
    "                                                 self._loss,\n",
    "                                                 self._validation_acc_log,\n",
    "                                                 self._validation_loss_log],\n",
    "                                                feed_dict={self._X: X_val, \n",
    "                                                           self._y: y_val})\n",
    "                        self._file_writer.add_summary(log_val_acc, epoch)\n",
    "\n",
    "                        if val_loss < max_val_loss:\n",
    "                            max_val_loss = val_loss\n",
    "                            self.best_model_loss_ = save_file_path\n",
    "                            count_drop_loss = 0\n",
    "                        else:\n",
    "                            count_drop_loss += 1\n",
    "\n",
    "\n",
    "                        if val_acc > max_val_acc:\n",
    "                            max_val_acc = val_acc\n",
    "                            self.best_model_acc_ = save_file_path\n",
    "                            count_drop_acc = 0\n",
    "                        else:\n",
    "                            count_drop_acc += 1\n",
    "\n",
    "                        print(\"epoch: {:<3}Training accuracy: {:<6.4f}, \\n\\\n",
    "                  Training loss: {:<6.6f}\\n\\\n",
    "                                            Validation accuracy: {:<6.4f}\\n\\\n",
    "                                                Validation loss: {:<6.6f}\"\n",
    "                          .format(epoch, train_acc, train_loss, \n",
    "                                  val_acc, val_loss))\n",
    "                        if count_drop_acc > self.max_stop and \\\n",
    "                                            count_drop_loss > self.max_stop:\n",
    "                            print(\"Early stopping!\\n    Count acc = %d\\n\\\n",
    "                            Count loss = %d\" \\\n",
    "                                  % (count_drop_acc, count_drop_loss))\n",
    "                            self._restore_session(model)\n",
    "                            break\n",
    "                    else:\n",
    "                        print(\"epoch: {:<3}Training accuracy: {:<6.4f}, \\n\\\n",
    "                                  Training loss: {:<6.6f}\\n\".\n",
    "                              format(epoch, train_acc, train_loss))\n",
    "\n",
    "            self._file_writer.close()\n",
    "\n",
    "        \n",
    "    def restore_session(self, model):\n",
    "        assert self._session\n",
    "#       If there are session already, then just restore session\n",
    "#       since the graph will also exist\n",
    "        self._restore_session(model)\n",
    "            \n",
    "    \n",
    "    def _restore_session(self, model):\n",
    "        \"\"\"\n",
    "        Restore the session from save files\n",
    "        \n",
    "        Parameters:\n",
    "        model:\n",
    "            The path to the save file or string /\"acc/\" or /\"loss/\"\n",
    "            \n",
    "        Return: None\n",
    "        \"\"\"\n",
    "        with self._session.as_default() as sess:\n",
    "            if model == \"acc\":\n",
    "                self._saver.restore(sess, self.best_model_acc_)\n",
    "            elif model == \"loss\":\n",
    "                self._saver.restore(sess, self.best_model_loss_)\n",
    "            else:\n",
    "                try:\n",
    "                     self._saver.restore(sess, model)\n",
    "                except:\n",
    "                    raise AttributeError(\"Restore for '%s' is invalid\" \n",
    "                                         % str(model))\n",
    "\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Return the probability of labels of Data given\n",
    "        \n",
    "        Parameters:\n",
    "        X:\n",
    "            Dataset same features as X in fit function\n",
    "            \n",
    "        return: \n",
    "            probability matrix\n",
    "        \"\"\"\n",
    "        if self._session is None:\n",
    "            raise NotFittedError(\"This %s instamance is not fitted yet\" \n",
    "                                 % self.__class__.__name__)\n",
    "        with self._session.as_default() as sess:\n",
    "            return self._y_proba.eval(feed_dict={self._X: X})\n",
    "        \n",
    "    def predict(self, X, onehot=True):\n",
    "        \"\"\"\n",
    "        Return the predicted labels of Data given\n",
    "        \n",
    "        Parameters:\n",
    "        X:\n",
    "            Dataset same features as X in fit function\n",
    "        \n",
    "        onehot:\n",
    "            True or False, defaults to True\n",
    "            Whether the returned label is in onehot encoding or not.\n",
    "            \n",
    "        return: \n",
    "            Onehot prediction matrix or predicted clases number \n",
    "            (range 0 to n_labels)\n",
    "        \"\"\"\n",
    "        if self._session is None:\n",
    "            raise NotFittedError(\"This %s instance is not fitted yet\" \n",
    "                                 % self.__class__.__name__)\n",
    "        with self._session.as_default() as sess:\n",
    "            if onehot:\n",
    "                pred_proba = self._y_proba.eval(feed_dict={self._X: X})\n",
    "                return (pred_proba ==\n",
    "                        pred_proba.max(axis=1)[:,None]).astype(int)\n",
    "            return self._y_pred.eval(feed_dict={self._X: X})\n",
    "        \n",
    "    def get_session(self):\n",
    "        if self._session:\n",
    "            return self._session\n",
    "    \n",
    "    def get_graph(self):\n",
    "        if self._graph:\n",
    "            return self._graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-21T05:25:07.381758Z",
     "start_time": "2018-01-21T05:25:07.367328Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "he_init = tf.contrib.layers.variance_scaling_initializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Randomized Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-20T04:28:04.892177Z",
     "start_time": "2018-01-20T04:28:04.076873Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-20T04:28:05.099903Z",
     "start_time": "2018-01-20T04:28:05.060737Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "param_dict = {\n",
    "    'n_hidden_layers':[4,5,6,7],\n",
    "    'n_neurons':[80 + 10 * i for i in range(6)],\n",
    "    'optimizer_class':[tf.train.AdamOptimizer, tf.train.GradientDescentOptimizer],\n",
    "    'learning_rate':[0.005, 0.01, 0.02, 0.04],\n",
    "    'initializer':[tf.contrib.layers.variance_scaling_initializer(), None],\n",
    "    'batch_norm':[True],\n",
    "    'n_epochs':[40],\n",
    "    'dropout_rate':[0.2 * i for i in range(2, 5)]\n",
    "}\n",
    "\n",
    "search_dnn_cv = RandomizedSearchCV(DNNClassifier(), param_dict, scoring=\"accuracy\", \n",
    "                                   n_jobs=1, n_iter=10, verbose=4, cv=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-20T02:21:24.971781Z",
     "start_time": "2018-01-20T02:19:40.762993Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 10 candidates, totalling 20 fits\n",
      "[CV] optimizer_class=<class 'tensorflow.python.training.gradient_descent.GradientDescentOptimizer'>, n_neurons=100, n_hidden_layers=5, n_epochs=40, learning_rate=0.005, initializer=<function variance_scaling_initializer.<locals>._initializer at 0x1a2b24f9d8>, dropout_rate=0.8, batch_norm=True \n",
      "epoch: 5  Training accuracy: 0.2159, \n",
      "              Training loss: 1.635947\n",
      "                                        Validation accuracy: 0.2166\n",
      "                                            Validation loss: 1.628693\n",
      "epoch: 10 Training accuracy: 0.2163, \n",
      "              Training loss: 1.647187\n",
      "                                        Validation accuracy: 0.2166\n",
      "                                            Validation loss: 1.637716\n",
      "epoch: 15 Training accuracy: 0.2164, \n",
      "              Training loss: 1.655976\n",
      "                                        Validation accuracy: 0.2166\n",
      "                                            Validation loss: 1.647135\n",
      "epoch: 20 Training accuracy: 0.2164, \n",
      "              Training loss: 1.664888\n",
      "                                        Validation accuracy: 0.2166\n",
      "                                            Validation loss: 1.653666\n",
      "epoch: 25 Training accuracy: 0.2165, \n",
      "              Training loss: 1.682756\n",
      "                                        Validation accuracy: 0.2166\n",
      "                                            Validation loss: 1.670144\n",
      "epoch: 30 Training accuracy: 0.2165, \n",
      "              Training loss: 1.685075\n",
      "                                        Validation accuracy: 0.2166\n",
      "                                            Validation loss: 1.672290\n",
      "epoch: 35 Training accuracy: 0.2165, \n",
      "              Training loss: 1.681194\n",
      "                                        Validation accuracy: 0.2166\n",
      "                                            Validation loss: 1.666327\n",
      "Early stopping!\n",
      "    Count acc = 6\n",
      "    Count loss = 6\n",
      "INFO:tensorflow:Restoring parameters from temp/4/saves/20180120131941/1.ckpt\n",
      "[CV]  optimizer_class=<class 'tensorflow.python.training.gradient_descent.GradientDescentOptimizer'>, n_neurons=100, n_hidden_layers=5, n_epochs=40, learning_rate=0.005, initializer=<function variance_scaling_initializer.<locals>._initializer at 0x1a2b24f9d8>, dropout_rate=0.8, batch_norm=True, score=0.22483772023682147, total= 1.3min\n",
      "[CV] optimizer_class=<class 'tensorflow.python.training.gradient_descent.GradientDescentOptimizer'>, n_neurons=100, n_hidden_layers=5, n_epochs=40, learning_rate=0.005, initializer=<function variance_scaling_initializer.<locals>._initializer at 0x1a2b24f9d8>, dropout_rate=0.8, batch_norm=True \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.3min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5  Training accuracy: 0.2248, \n",
      "              Training loss: 1.630842\n",
      "                                        Validation accuracy: 0.2241\n",
      "                                            Validation loss: 1.631867\n",
      "epoch: 10 Training accuracy: 0.2244, \n",
      "              Training loss: 1.658574\n",
      "                                        Validation accuracy: 0.2241\n",
      "                                            Validation loss: 1.661140\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-581-fdb898b94c4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msearch_dnn_cv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_04\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_04\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_valid_04\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_valid_04\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda/envs/ml_env/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    637\u001b[0m                                   error_score=self.error_score)\n\u001b[1;32m    638\u001b[0m           for parameters, (train, test) in product(candidate_params,\n\u001b[0;32m--> 639\u001b[0;31m                                                    cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0;31m# if one choose to see train score, \"out\" will contain train score info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/ml_env/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/ml_env/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/ml_env/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/ml_env/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/ml_env/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/ml_env/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/ml_env/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/ml_env/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, error_score)\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-569-c5d5e34eea15>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, X_val, y_val, model)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m                 sess.run([self._training_op, self._batch_updates], \n\u001b[0;32m--> 193\u001b[0;31m                          feed_dict={self._X:X_batch, self._y: y_batch, self._training:True})\n\u001b[0m\u001b[1;32m    194\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_freq\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                 train_acc, train_loss, log_train_acc, log_train_loss = sess.run(\n",
      "\u001b[0;32m/anaconda/envs/ml_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/ml_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/ml_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/ml_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/ml_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "search_dnn_cv.fit(X_train_04, y_train_04, X_val=X_valid_04, y_val=y_valid_04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-19T10:22:35.936330Z",
     "start_time": "2018-01-19T10:22:35.927366Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_norm': True,\n",
       " 'dropout_rate': 0.2,\n",
       " 'learning_rate': 0.01,\n",
       " 'n_epochs': 100,\n",
       " 'n_hidden_layers': 4,\n",
       " 'n_neurons': 120,\n",
       " 'optimizer_class': tensorflow.python.training.gradient_descent.GradientDescentOptimizer}"
      ]
     },
     "execution_count": 553,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_dnn_param = search_dnn_cv.best_params_\n",
    "best_dnn_param[\"n_epochs\"] = 100\n",
    "best_dnn_param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-30T02:10:37.212803Z",
     "start_time": "2018-01-30T02:10:37.192576Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dnn_clf = DNNClassifier(n_hidden_layers=5, n_neurons=[160,140,120,100,80], \n",
    "                 optimizer_class=tf.train.AdamOptimizer,\n",
    "                 learning_rate=0.01, n_epochs=30, batch_size=500, \n",
    "                 activation=tf.nn.selu, initializer=None,\n",
    "                 batch_norm_momentum=None, dropout_rate=None, \n",
    "                 regularizer=None,\n",
    "                 reg_scale=None,\n",
    "                 base_dir=\"temp\", \n",
    "                 log_freq=2, max_stop=2,\n",
    "                 random_state=42\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-30T02:11:50.027092Z",
     "start_time": "2018-01-30T02:10:38.261305Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-8-8198264ecb76>:232: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "epoch: 2  Training accuracy: 0.9778, \n",
      "                  Training loss: 0.071206\n",
      "                                            Validation accuracy: 0.9780\n",
      "                                                Validation loss: 0.068557\n",
      "epoch: 4  Training accuracy: 0.9772, \n",
      "                  Training loss: 0.075632\n",
      "                                            Validation accuracy: 0.9773\n",
      "                                                Validation loss: 0.076035\n",
      "epoch: 6  Training accuracy: 0.9804, \n",
      "                  Training loss: 0.041005\n",
      "                                            Validation accuracy: 0.9806\n",
      "                                                Validation loss: 0.053752\n",
      "epoch: 8  Training accuracy: 0.9837, \n",
      "                  Training loss: 0.020525\n",
      "                                            Validation accuracy: 0.9838\n",
      "                                                Validation loss: 0.039328\n",
      "epoch: 10 Training accuracy: 0.9858, \n",
      "                  Training loss: 0.016955\n",
      "                                            Validation accuracy: 0.9858\n",
      "                                                Validation loss: 0.044035\n",
      "epoch: 12 Training accuracy: 0.9873, \n",
      "                  Training loss: 0.014163\n",
      "                                            Validation accuracy: 0.9874\n",
      "                                                Validation loss: 0.051045\n",
      "epoch: 14 Training accuracy: 0.9883, \n",
      "                  Training loss: 0.017878\n",
      "                                            Validation accuracy: 0.9883\n",
      "                                                Validation loss: 0.070181\n",
      "epoch: 16 Training accuracy: 0.9892, \n",
      "                  Training loss: 0.009975\n",
      "                                            Validation accuracy: 0.9892\n",
      "                                                Validation loss: 0.039843\n",
      "epoch: 18 Training accuracy: 0.9901, \n",
      "                  Training loss: 0.007426\n",
      "                                            Validation accuracy: 0.9901\n",
      "                                                Validation loss: 0.028921\n",
      "epoch: 20 Training accuracy: 0.9907, \n",
      "                  Training loss: 0.011069\n",
      "                                            Validation accuracy: 0.9907\n",
      "                                                Validation loss: 0.059977\n",
      "epoch: 22 Training accuracy: 0.9910, \n",
      "                  Training loss: 0.016651\n",
      "                                            Validation accuracy: 0.9910\n",
      "                                                Validation loss: 0.055194\n",
      "epoch: 24 Training accuracy: 0.9915, \n",
      "                  Training loss: 0.005970\n",
      "                                            Validation accuracy: 0.9915\n",
      "                                                Validation loss: 0.040346\n",
      "epoch: 26 Training accuracy: 0.9918, \n",
      "                  Training loss: 0.014700\n",
      "                                            Validation accuracy: 0.9918\n",
      "                                                Validation loss: 0.048626\n",
      "epoch: 28 Training accuracy: 0.9918, \n",
      "                  Training loss: 0.035367\n",
      "                                            Validation accuracy: 0.9917\n",
      "                                                Validation loss: 0.077755\n",
      "epoch: 30 Training accuracy: 0.9922, \n",
      "                  Training loss: 0.003420\n",
      "                                            Validation accuracy: 0.9922\n",
      "                                                Validation loss: 0.036014\n"
     ]
    }
   ],
   "source": [
    "dnn_clf.fit(X_train_04, y_train_04, X_valid_04, y_valid_04)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Test the result on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-30T02:27:28.048047Z",
     "start_time": "2018-01-30T02:24:19.684Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-21T05:44:07.049584Z",
     "start_time": "2018-01-21T05:44:06.583218Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from temp/saves/20180121162950/5.ckpt\n",
      "The final model: Accuracy for test set is: 0.9903\n"
     ]
    }
   ],
   "source": [
    "dnn_clf.restore_session(dnn_clf.last_epoch_model)\n",
    "y_pred = dnn_clf.predict(X_test_04)\n",
    "acc_test = accuracy_score(y_test_04, y_pred)\n",
    "print(\"The final model: Accuracy for test set is: {:>4.4f}\".format(acc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-21T05:44:07.433987Z",
     "start_time": "2018-01-21T05:44:07.141040Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from temp/saves/20180121162950/5.ckpt\n",
      "The best validation acc model: Accuracy for test set is: 0.9903\n"
     ]
    }
   ],
   "source": [
    "dnn_clf.restore_session(dnn_clf.best_model_acc_)\n",
    "y_pred = dnn_clf.predict(X_test_04)\n",
    "acc_test = accuracy_score(y_test_04, y_pred)\n",
    "print(\"The best validation acc model: Accuracy for test set is: {:>4.4f}\".format(acc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-21T05:44:07.751823Z",
     "start_time": "2018-01-21T05:44:07.575979Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from temp/saves/20180121162950/5.ckpt\n",
      "The best validation loss model: Accuracy for test set is: 0.9903\n"
     ]
    }
   ],
   "source": [
    "dnn_clf.restore_session(dnn_clf.best_model_loss_)\n",
    "y_pred = dnn_clf.predict(X_test_04)\n",
    "acc_test = accuracy_score(y_test_04, y_pred)\n",
    "print(\"The best validation loss model: Accuracy for test set is: {:>4.4f}\".format(acc_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# <i>Question9</i> --Transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Prepare 5 to 9 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-30T03:14:10.407973Z",
     "start_time": "2018-01-30T03:14:09.800216Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/ml_env/lib/python3.6/site-packages/sklearn/model_selection/_split.py:1639: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "ind_59_train = (np.argmax(y_train, axis=1) > 4)\n",
    "ind_59_test = (np.argmax(y_test, axis=1) > 4)\n",
    "ind_59_valid = (np.argmax(y_valid, axis=1) > 4)\n",
    "X_train_59_all = X_train[ind_59_train]\n",
    "y_train_59_all = np.delete(y_train[ind_59_train], [0,1,2,3,4], axis=1)\n",
    "X_test_59 = X_test[ind_59_test]\n",
    "y_test_59 = np.delete(y_test[ind_59_test], [0,1,2,3,4], axis=1)\n",
    "X_valid_59 = X_valid[ind_59_valid]\n",
    "y_valid_59 = np.delete(y_valid[ind_59_valid], [0,1,2,3,4], axis=1)\n",
    "sss = StratifiedShuffleSplit(n_splits=1, train_size=500)\n",
    "for train_index, test_index in sss.split(X_train_59_all, y_train_59_all):\n",
    "    X_train_59, _ = X_train_59_all[train_index], X_train_59_all[test_index]\n",
    "    y_train_59, _ = y_train_59_all[train_index], y_train_59_all[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-30T03:13:15.589376Z",
     "start_time": "2018-01-30T03:13:15.577925Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "[[0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " ...\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/ml_env/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "_ = y_train[ind_59_train]\n",
    "_ == y_train_59_all\n",
    "print(_)\n",
    "print(y_train_59_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Check data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T03:58:18.190072Z",
     "start_time": "2018-01-23T03:58:18.171712Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 107848\n",
      "1: 26962\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(y_train_59, return_counts=True)\n",
    "for i, j in zip(unique, counts):\n",
    "    print(\"%d: %d\" %(i, j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T03:58:34.109425Z",
     "start_time": "2018-01-23T03:58:34.101287Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " ...\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(y_train_59)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Bonus: Check memory usage of different variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-21T02:57:57.242337Z",
     "start_time": "2018-01-21T02:57:57.235736Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "172480112"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sys import getsizeof\n",
    "\n",
    "getsizeof(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## a,b:Load the previous model's parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Create graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-30T03:29:54.110959Z",
     "start_time": "2018-01-30T03:29:53.604792Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    del graph\n",
    "except:\n",
    "    pass\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    res_saver = tf.train.import_meta_graph(dnn_clf.last_epoch_model+\".meta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-30T03:29:54.413528Z",
     "start_time": "2018-01-30T03:29:54.369618Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dnn_ops\n",
      "eval_ops\n",
      "local_variables\n",
      "loss_ops\n",
      "metric_variables\n",
      "output_ops\n",
      "summaries\n",
      "train_op\n",
      "train_ops\n",
      "trainable_variables\n",
      "variables\n"
     ]
    }
   ],
   "source": [
    "for key in graph.get_all_collection_keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "constructing new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-30T03:29:56.508392Z",
     "start_time": "2018-01-30T03:29:56.371767Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    X = graph.get_tensor_by_name(\"X:0\")\n",
    "    y = graph.get_tensor_by_name(\"y:0\")\n",
    "    training = graph.get_tensor_by_name(\"training:0\")\n",
    "    hidden5 = graph.get_collection('dnn_ops', scope='.*hidden5.*')[0]\n",
    "    logits = graph.get_collection(\"output_ops\", scope=\".*logits.*\")[0]\n",
    "    y_pred = graph.get_collection(\"output_ops\", scope=\".*y_pred.*\")[0]\n",
    "    true_labels = graph.get_collection(\"eval_ops\", scope=\".*true_labels.*\")[0]\n",
    "    train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
    "                               scope=\"logits\")\n",
    "    loss = graph.get_collection(\"loss_ops\", scope=\"loss/loss\")[0]\n",
    "    training_op = tf.train.AdamOptimizer(0.01).minimize(loss, var_list=train_vars)\n",
    "    acc = graph.get_collection(\"eval_ops\")[0]\n",
    "    inits = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Training new layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-30T03:18:25.064676Z",
     "start_time": "2018-01-30T03:18:18.695168Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from temp/saves/20180130131041/15.ckpt\n",
      "1 The accuracy of training set is 0.4400\n",
      "1 The accuracy of val set is 0.4717\n",
      "2 The accuracy of training set is 0.5860\n",
      "2 The accuracy of val set is 0.6032\n",
      "3 The accuracy of training set is 0.6700\n",
      "3 The accuracy of val set is 0.6744\n",
      "4 The accuracy of training set is 0.7000\n",
      "4 The accuracy of val set is 0.7039\n",
      "5 The accuracy of training set is 0.7080\n",
      "5 The accuracy of val set is 0.6929\n",
      "6 The accuracy of training set is 0.7320\n",
      "6 The accuracy of val set is 0.7379\n",
      "7 The accuracy of training set is 0.7560\n",
      "7 The accuracy of val set is 0.7391\n",
      "8 The accuracy of training set is 0.7780\n",
      "8 The accuracy of val set is 0.7535\n",
      "9 The accuracy of training set is 0.7740\n",
      "9 The accuracy of val set is 0.7654\n",
      "10 The accuracy of training set is 0.7820\n",
      "10 The accuracy of val set is 0.7719\n",
      "11 The accuracy of training set is 0.7840\n",
      "11 The accuracy of val set is 0.7699\n",
      "12 The accuracy of training set is 0.7740\n",
      "12 The accuracy of val set is 0.7465\n",
      "13 The accuracy of training set is 0.7840\n",
      "13 The accuracy of val set is 0.7563\n",
      "14 The accuracy of training set is 0.7740\n",
      "14 The accuracy of val set is 0.7486\n",
      "15 The accuracy of training set is 0.7920\n",
      "15 The accuracy of val set is 0.7568\n",
      "Early Stopping!\n",
      "The total time cost for this to train is: 6.17s\n",
      "Time per epoch is: 0.41s\n"
     ]
    }
   ],
   "source": [
    "batch_size=50\n",
    "training_size=X_train_59.shape[0]\n",
    "time_now = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "if not os.path.isdir(\"temp/freezed/%s\"%(time_now)):\n",
    "    os.makedirs(\"temp/freezed/%s\" % time_now)\n",
    "\n",
    "\n",
    "    \n",
    "c_max = 5\n",
    "c = 0\n",
    "max_acc = -np.inf\n",
    "    \n",
    "tstart = time()\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    with graph.as_default():\n",
    "        inits.run()\n",
    "        res_saver.restore(sess,dnn_clf.last_epoch_model)\n",
    "        for epoch in range(1, 50):\n",
    "            for batch in range(1, np.ceil(training_size/\n",
    "                                      batch_size).astype(int)+1):\n",
    "\n",
    "                X_batch, y_batch =  dnn_clf._get_batch(X_train_59, \n",
    "                                                       y_train_59, \n",
    "                                                       batch_size, \n",
    "                                                       epoch, batch)\n",
    "\n",
    "                sess.run(training_op, \n",
    "                         feed_dict={X:X_batch, \n",
    "                                    y:y_batch, \n",
    "                                    training:True})\n",
    "\n",
    "\n",
    "            y_pred_train, true_label_train = sess.run([y_pred, true_labels],\n",
    "                                         feed_dict={X:X_train_59, \n",
    "                                                    y:y_train_59})\n",
    "            \n",
    "            \n",
    "            print(epoch, \"The accuracy of training set is %.4f\" % \n",
    "                  accuracy_score(true_label_train, y_pred_train))\n",
    "            y_pred_val, true_label_val= sess.run([y_pred, true_labels],\n",
    "                                                  feed_dict={X:X_valid_59, \n",
    "                                                  y:y_valid_59})\n",
    "            acc_val = accuracy_score(true_label_val, y_pred_val)\n",
    "            print(epoch, \"The accuracy of val set is %.4f\" % acc_val)\n",
    "                \n",
    "            save_path = res_saver.save(sess, \n",
    "                                \"temp/freezed/%s/%d.ckpt\"%(time_now, epoch))\n",
    "            \n",
    "            if acc_val > max_acc:\n",
    "                c = 0\n",
    "                max_acc = acc_val\n",
    "                final_save_path = save_path\n",
    "            else:\n",
    "                c += 1\n",
    "            if c >= 5:\n",
    "                print(\"Early Stopping!\")\n",
    "                break\n",
    "\n",
    "tend = time()\n",
    "\n",
    "print(\"The total time cost for this to train is: %.2fs\" % (tend-tstart))\n",
    "print(\"Time per epoch is: %.2fs\" % ((tend-tstart)/epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Check accuracy on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-30T02:54:44.974566Z",
     "start_time": "2018-01-30T02:54:44.560397Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from temp/freezed/20180130132316/99.ckpt\n",
      "0.6780497839950628\n",
      "[2 0 0 ... 4 0 1]\n",
      "[2 4 0 ... 4 0 1]\n",
      "0.6780498\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(inits)\n",
    "    res_saver.restore(sess, save_path)\n",
    "    y_pred_val, true_label_val, acc_val = sess.run([y_pred, true_labels, acc],\n",
    "                                          feed_dict={X:X_test_59, \n",
    "                                          y:y_test_59})\n",
    "    print(accuracy_score(true_label_val, y_pred_val))\n",
    "    print(y_pred_val)\n",
    "    print(true_label_val)\n",
    "    print(acc_val)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "source": [
    "## c. Caching and training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Get a handle on the hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-30T03:24:25.992643Z",
     "start_time": "2018-01-30T03:24:25.987933Z"
    },
    "hidden": true
   },
   "source": [
    "with graph.as_default():\n",
    "    hidden5 = graph.get_collection('dnn_ops', scope='.*hidden5.*')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Train the model with validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-30T03:24:31.414611Z",
     "start_time": "2018-01-30T03:24:26.840834Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from temp/saves/20180130131041/15.ckpt\n",
      "1 The accuracy of training set is 0.4140\n",
      "1 The accuracy of val set is 0.4505\n",
      "2 The accuracy of training set is 0.5820\n",
      "2 The accuracy of val set is 0.6335\n",
      "3 The accuracy of training set is 0.6880\n",
      "3 The accuracy of val set is 0.7121\n",
      "4 The accuracy of training set is 0.6920\n",
      "4 The accuracy of val set is 0.6933\n",
      "5 The accuracy of training set is 0.7280\n",
      "5 The accuracy of val set is 0.7043\n",
      "6 The accuracy of training set is 0.7240\n",
      "6 The accuracy of val set is 0.7387\n",
      "7 The accuracy of training set is 0.7560\n",
      "7 The accuracy of val set is 0.7477\n",
      "8 The accuracy of training set is 0.7660\n",
      "8 The accuracy of val set is 0.7551\n",
      "9 The accuracy of training set is 0.7900\n",
      "9 The accuracy of val set is 0.7789\n",
      "10 The accuracy of training set is 0.7960\n",
      "10 The accuracy of val set is 0.7789\n",
      "11 The accuracy of training set is 0.7780\n",
      "11 The accuracy of val set is 0.7654\n",
      "12 The accuracy of training set is 0.7680\n",
      "12 The accuracy of val set is 0.7359\n",
      "13 The accuracy of training set is 0.7840\n",
      "13 The accuracy of val set is 0.7539\n",
      "14 The accuracy of training set is 0.7800\n",
      "14 The accuracy of val set is 0.7518\n",
      "Early Stopping!\n",
      "The total time cost for this to train is: 4.43s\n",
      "The first half time cost for this to train is: 0.37s\n",
      "Time per epoch is: 0.29s\n"
     ]
    }
   ],
   "source": [
    "batch_size=50\n",
    "training_size=X_train_59.shape[0]\n",
    "time_now = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "if not os.path.isdir(\"temp/freezed/%s\"%(time_now)):\n",
    "    os.makedirs(\"temp/freezed/%s\" % time_now)\n",
    "\n",
    "\n",
    "    \n",
    "c_max = 5\n",
    "c = 0\n",
    "max_acc = -np.inf\n",
    "    \n",
    "tstart = time()\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    with graph.as_default():\n",
    "        inits.run()\n",
    "        res_saver.restore(sess,dnn_clf.last_epoch_model)\n",
    "        \n",
    "        hidden5_cache_train = sess.run(hidden5, feed_dict={X:X_train_59,\n",
    "                                                        y:y_train_59})\n",
    "        hidden5_cache_val = sess.run(hidden5, feed_dict={X:X_valid_59,\n",
    "                                                        y:y_valid_59})\n",
    "        \n",
    "        tmid = time()\n",
    "        \n",
    "        for epoch in range(1, 50):\n",
    "            for batch in range(1, np.ceil(training_size/\n",
    "                                      batch_size).astype(int)+1):\n",
    "\n",
    "                X_batch, y_batch =  dnn_clf._get_batch(hidden5_cache_train, \n",
    "                                                       y_train_59, \n",
    "                                                       batch_size, \n",
    "                                                       epoch, batch)\n",
    "\n",
    "                sess.run(training_op, \n",
    "                         feed_dict={hidden5:X_batch, \n",
    "                                    y:y_batch, \n",
    "                                    training:True})\n",
    "\n",
    "\n",
    "            y_pred_train, true_label_train = sess.run([y_pred, true_labels],\n",
    "                                         feed_dict={hidden5:hidden5_cache_train, \n",
    "                                                    y:y_train_59})\n",
    "            \n",
    "            \n",
    "            print(epoch, \"The accuracy of training set is %.4f\" % \n",
    "                  accuracy_score(true_label_train, y_pred_train))\n",
    "            \n",
    "            \n",
    "            \n",
    "            y_pred_val, true_label_val= sess.run([y_pred, true_labels],\n",
    "                                                  feed_dict={hidden5:hidden5_cache_val, \n",
    "                                                  y:y_valid_59})\n",
    "            acc_val = accuracy_score(true_label_val, y_pred_val)\n",
    "            print(epoch, \"The accuracy of val set is %.4f\" % acc_val)\n",
    "                \n",
    "            save_path = res_saver.save(sess, \n",
    "                                \"temp/freezed/%s/%d.ckpt\"%(time_now, epoch))\n",
    "            \n",
    "            if acc_val > max_acc:\n",
    "                c = 0\n",
    "                max_acc = acc_val\n",
    "                final_save_path = save_path\n",
    "            else:\n",
    "                c += 1\n",
    "            if c >= 5:\n",
    "                print(\"Early Stopping!\")\n",
    "                break\n",
    "\n",
    "tend = time()\n",
    "\n",
    "print(\"The total time cost for this to train is: %.2fs\" % (tend-tstart))\n",
    "print(\"The first half time cost for this to train is: %.2fs\" % (tmid-tstart))\n",
    "print(\"Time per epoch is: %.2fs\" % ((tend-tmid)/epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Check without validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Reset graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    del graph\n",
    "except:\n",
    "    pass\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    res_saver = tf.train.import_meta_graph(dnn_clf.last_epoch_model+\".meta\")\n",
    "    \n",
    "with graph.as_default():\n",
    "    X = graph.get_tensor_by_name(\"X:0\")\n",
    "    y = graph.get_tensor_by_name(\"y:0\")\n",
    "    training = graph.get_tensor_by_name(\"training:0\")\n",
    "    hidden5 = graph.get_collection('dnn_ops', scope='.*hidden5.*')[0]\n",
    "    logits = graph.get_collection(\"output_ops\", scope=\".*logits.*\")[0]\n",
    "    y_pred = graph.get_collection(\"output_ops\", scope=\".*y_pred.*\")[0]\n",
    "    true_labels = graph.get_collection(\"eval_ops\", scope=\".*true_labels.*\")[0]\n",
    "    train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
    "                               scope=\"logits\")\n",
    "    loss = graph.get_collection(\"loss_ops\", scope=\"loss/loss\")[0]\n",
    "    training_op = tf.train.AdamOptimizer(0.01).minimize(loss, var_list=train_vars)\n",
    "    acc = graph.get_collection(\"eval_ops\")[0]\n",
    "    inits = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### The non-cached version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-30T03:29:32.479755Z",
     "start_time": "2018-01-30T03:29:28.238017Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from temp/saves/20180130131041/15.ckpt\n",
      "1 The accuracy of training set is 0.4160\n",
      "2 The accuracy of training set is 0.5780\n",
      "3 The accuracy of training set is 0.6520\n",
      "4 The accuracy of training set is 0.7080\n",
      "5 The accuracy of training set is 0.7220\n",
      "6 The accuracy of training set is 0.7440\n",
      "7 The accuracy of training set is 0.7460\n",
      "8 The accuracy of training set is 0.7500\n",
      "9 The accuracy of training set is 0.7900\n",
      "10 The accuracy of training set is 0.7760\n",
      "11 The accuracy of training set is 0.7780\n",
      "12 The accuracy of training set is 0.7840\n",
      "13 The accuracy of training set is 0.8000\n",
      "14 The accuracy of training set is 0.8060\n",
      "15 The accuracy of training set is 0.8080\n",
      "16 The accuracy of training set is 0.7900\n",
      "17 The accuracy of training set is 0.8020\n",
      "18 The accuracy of training set is 0.8180\n",
      "19 The accuracy of training set is 0.7920\n",
      "20 The accuracy of training set is 0.8080\n",
      "21 The accuracy of training set is 0.8040\n",
      "22 The accuracy of training set is 0.8140\n",
      "23 The accuracy of training set is 0.8060\n",
      "24 The accuracy of training set is 0.8240\n",
      "25 The accuracy of training set is 0.8140\n",
      "26 The accuracy of training set is 0.8300\n",
      "27 The accuracy of training set is 0.8220\n",
      "28 The accuracy of training set is 0.8540\n",
      "29 The accuracy of training set is 0.8300\n",
      "30 The accuracy of training set is 0.8280\n",
      "31 The accuracy of training set is 0.8380\n",
      "32 The accuracy of training set is 0.8320\n",
      "33 The accuracy of training set is 0.8240\n",
      "34 The accuracy of training set is 0.8460\n",
      "35 The accuracy of training set is 0.8620\n",
      "36 The accuracy of training set is 0.8380\n",
      "37 The accuracy of training set is 0.8500\n",
      "38 The accuracy of training set is 0.8340\n",
      "39 The accuracy of training set is 0.8520\n",
      "40 The accuracy of training set is 0.8600\n",
      "41 The accuracy of training set is 0.8400\n",
      "42 The accuracy of training set is 0.8280\n",
      "43 The accuracy of training set is 0.8400\n",
      "44 The accuracy of training set is 0.8540\n",
      "45 The accuracy of training set is 0.8480\n",
      "46 The accuracy of training set is 0.8300\n",
      "47 The accuracy of training set is 0.8400\n",
      "48 The accuracy of training set is 0.8480\n",
      "49 The accuracy of training set is 0.8480\n",
      "The total time cost for this to train is: 4.13s\n",
      "Time per epoch is: 0.08s\n"
     ]
    }
   ],
   "source": [
    "batch_size=50\n",
    "training_size=X_train_59.shape[0]\n",
    "time_now = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "if not os.path.isdir(\"temp/freezed/%s\"%(time_now)):\n",
    "    os.makedirs(\"temp/freezed/%s\" % time_now)\n",
    "\n",
    "\n",
    "    \n",
    "c_max = 5\n",
    "c = 0\n",
    "max_acc = -np.inf\n",
    "    \n",
    "tstart = time()\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    with graph.as_default():\n",
    "        inits.run()\n",
    "        res_saver.restore(sess,dnn_clf.last_epoch_model)\n",
    "        for epoch in range(1, 50):\n",
    "            for batch in range(1, np.ceil(training_size/\n",
    "                                      batch_size).astype(int)+1):\n",
    "\n",
    "                X_batch, y_batch =  dnn_clf._get_batch(X_train_59, \n",
    "                                                       y_train_59, \n",
    "                                                       batch_size, \n",
    "                                                       epoch, batch)\n",
    "\n",
    "                sess.run(training_op, \n",
    "                         feed_dict={X:X_batch, \n",
    "                                    y:y_batch, \n",
    "                                    training:True})\n",
    "\n",
    "\n",
    "            y_pred_train, true_label_train = sess.run([y_pred, true_labels],\n",
    "                                         feed_dict={X:X_train_59, \n",
    "                                                    y:y_train_59})\n",
    "            \n",
    "            \n",
    "            print(epoch, \"The accuracy of training set is %.4f\" % \n",
    "                  accuracy_score(true_label_train, y_pred_train))\n",
    "#             y_pred_val, true_label_val= sess.run([y_pred, true_labels],\n",
    "#                                                   feed_dict={X:X_valid_59, \n",
    "#                                                   y:y_valid_59})\n",
    "#             acc_val = accuracy_score(true_label_val, y_pred_val)\n",
    "#             print(epoch, \"The accuracy of val set is %.4f\" % acc_val)\n",
    "                \n",
    "#             save_path = res_saver.save(sess, \n",
    "#                                 \"temp/freezed/%s/%d.ckpt\"%(time_now, epoch))\n",
    "            \n",
    "#             if acc_val > max_acc:\n",
    "#                 c = 0\n",
    "#                 max_acc = acc_val\n",
    "#                 final_save_path = save_path\n",
    "#             else:\n",
    "#                 c += 1\n",
    "#             if c >= 5:\n",
    "#                 print(\"Early Stopping!\")\n",
    "#                 break\n",
    "\n",
    "tend = time()\n",
    "\n",
    "print(\"The total time cost for this to train is: %.2fs\" % (tend-tstart))\n",
    "print(\"Time per epoch is: %.2fs\" % ((tend-tstart)/epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### The cached version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-30T03:30:08.042055Z",
     "start_time": "2018-01-30T03:30:05.841226Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from temp/saves/20180130131041/15.ckpt\n",
      "1 The accuracy of training set is 0.4160\n",
      "2 The accuracy of training set is 0.5780\n",
      "3 The accuracy of training set is 0.6520\n",
      "4 The accuracy of training set is 0.7080\n",
      "5 The accuracy of training set is 0.7220\n",
      "6 The accuracy of training set is 0.7440\n",
      "7 The accuracy of training set is 0.7460\n",
      "8 The accuracy of training set is 0.7500\n",
      "9 The accuracy of training set is 0.7900\n",
      "10 The accuracy of training set is 0.7760\n",
      "11 The accuracy of training set is 0.7780\n",
      "12 The accuracy of training set is 0.7840\n",
      "13 The accuracy of training set is 0.8000\n",
      "14 The accuracy of training set is 0.8060\n",
      "15 The accuracy of training set is 0.8080\n",
      "16 The accuracy of training set is 0.7900\n",
      "17 The accuracy of training set is 0.8020\n",
      "18 The accuracy of training set is 0.8180\n",
      "19 The accuracy of training set is 0.7920\n",
      "20 The accuracy of training set is 0.8080\n",
      "21 The accuracy of training set is 0.8040\n",
      "22 The accuracy of training set is 0.8140\n",
      "23 The accuracy of training set is 0.8060\n",
      "24 The accuracy of training set is 0.8240\n",
      "25 The accuracy of training set is 0.8140\n",
      "26 The accuracy of training set is 0.8300\n",
      "27 The accuracy of training set is 0.8220\n",
      "28 The accuracy of training set is 0.8540\n",
      "29 The accuracy of training set is 0.8300\n",
      "30 The accuracy of training set is 0.8280\n",
      "31 The accuracy of training set is 0.8380\n",
      "32 The accuracy of training set is 0.8320\n",
      "33 The accuracy of training set is 0.8240\n",
      "34 The accuracy of training set is 0.8460\n",
      "35 The accuracy of training set is 0.8620\n",
      "36 The accuracy of training set is 0.8380\n",
      "37 The accuracy of training set is 0.8500\n",
      "38 The accuracy of training set is 0.8340\n",
      "39 The accuracy of training set is 0.8520\n",
      "40 The accuracy of training set is 0.8600\n",
      "41 The accuracy of training set is 0.8400\n",
      "42 The accuracy of training set is 0.8280\n",
      "43 The accuracy of training set is 0.8400\n",
      "44 The accuracy of training set is 0.8540\n",
      "45 The accuracy of training set is 0.8480\n",
      "46 The accuracy of training set is 0.8300\n",
      "47 The accuracy of training set is 0.8400\n",
      "48 The accuracy of training set is 0.8480\n",
      "49 The accuracy of training set is 0.8480\n",
      "The total time cost for this to train is: 2.02s\n",
      "The first half time cost for this to train is: 0.31s\n",
      "Time per epoch is: 0.03s\n"
     ]
    }
   ],
   "source": [
    "with graph.as_default():\n",
    "    hidden5 = graph.get_collection('dnn_ops', scope='.*hidden5.*')[0]\n",
    "\n",
    "batch_size=50\n",
    "training_size=X_train_59.shape[0]\n",
    "time_now = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "if not os.path.isdir(\"temp/freezed/%s\"%(time_now)):\n",
    "    os.makedirs(\"temp/freezed/%s\" % time_now)\n",
    "\n",
    "\n",
    "    \n",
    "c_max = 5\n",
    "c = 0\n",
    "max_acc = -np.inf\n",
    "    \n",
    "tstart = time()\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    with graph.as_default():\n",
    "        inits.run()\n",
    "        res_saver.restore(sess,dnn_clf.last_epoch_model)\n",
    "        \n",
    "        hidden5_cache_train = sess.run(hidden5, feed_dict={X:X_train_59,\n",
    "                                                        y:y_train_59})\n",
    "#         hidden5_cache_val = sess.run(hidden5, feed_dict={X:X_valid_59,\n",
    "#                                                         y:y_valid_59})\n",
    "        \n",
    "        tmid = time()\n",
    "        \n",
    "        for epoch in range(1, 50):\n",
    "            for batch in range(1, np.ceil(training_size/\n",
    "                                      batch_size).astype(int)+1):\n",
    "\n",
    "                X_batch, y_batch =  dnn_clf._get_batch(hidden5_cache_train, \n",
    "                                                       y_train_59, \n",
    "                                                       batch_size, \n",
    "                                                       epoch, batch)\n",
    "\n",
    "                sess.run(training_op, \n",
    "                         feed_dict={hidden5:X_batch, \n",
    "                                    y:y_batch, \n",
    "                                    training:True})\n",
    "\n",
    "\n",
    "            y_pred_train, true_label_train = sess.run([y_pred, true_labels],\n",
    "                                         feed_dict={hidden5:hidden5_cache_train, \n",
    "                                                    y:y_train_59})\n",
    "            \n",
    "            \n",
    "            print(epoch, \"The accuracy of training set is %.4f\" % \n",
    "                  accuracy_score(true_label_train, y_pred_train))\n",
    "            \n",
    "            \n",
    "            \n",
    "#             y_pred_val, true_label_val= sess.run([y_pred, true_labels],\n",
    "#                                                   feed_dict={hidden5:hidden5_cache_val, \n",
    "#                                                   y:y_valid_59})\n",
    "#             acc_val = accuracy_score(true_label_val, y_pred_val)\n",
    "#             print(epoch, \"The accuracy of val set is %.4f\" % acc_val)\n",
    "                \n",
    "#             save_path = res_saver.save(sess, \n",
    "#                                 \"temp/freezed/%s/%d.ckpt\"%(time_now, epoch))\n",
    "            \n",
    "#             if acc_val > max_acc:\n",
    "#                 c = 0\n",
    "#                 max_acc = acc_val\n",
    "#                 final_save_path = save_path\n",
    "#             else:\n",
    "#                 c += 1\n",
    "#             if c >= 5:\n",
    "#                 print(\"Early Stopping!\")\n",
    "#                 break\n",
    "\n",
    "tend = time()\n",
    "\n",
    "print(\"The total time cost for this to train is: %.2fs\" % (tend-tstart))\n",
    "print(\"The first half time cost for this to train is: %.2fs\" % (tmid-tstart))\n",
    "print(\"Time per epoch is: %.2fs\" % ((tend-tmid)/epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "That is motherfxxker lot faster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## d,e.Unlock some more layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Set graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-30T04:02:23.102723Z",
     "start_time": "2018-01-30T04:02:16.955123Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    del graph\n",
    "except:\n",
    "    pass\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    res_saver = tf.train.import_meta_graph(dnn_clf.last_epoch_model+\".meta\")\n",
    "    \n",
    "with graph.as_default():\n",
    "    X = graph.get_tensor_by_name(\"X:0\")\n",
    "    y = graph.get_tensor_by_name(\"y:0\")\n",
    "    training = graph.get_tensor_by_name(\"training:0\")\n",
    "    hidden1 = graph.get_collection('dnn_ops', scope='.*hidden1.*')[0]\n",
    "    hidden2 = graph.get_collection('dnn_ops', scope='.*hidden2.*')[0]\n",
    "    hidden3 = graph.get_collection('dnn_ops', scope='.*hidden3.*')[0]\n",
    "    hidden4 = graph.get_collection('dnn_ops', scope='.*hidden4.*')[0]\n",
    "    hidden5 = graph.get_collection('dnn_ops', scope='.*hidden5.*')[0]\n",
    "    logits = graph.get_collection(\"output_ops\", scope=\".*logits.*\")[0]\n",
    "    y_pred = graph.get_collection(\"output_ops\", scope=\".*y_pred.*\")[0]\n",
    "    true_labels = graph.get_collection(\"eval_ops\", scope=\".*true_labels.*\")[0]\n",
    "    \n",
    "    \n",
    "    train_vars_5 = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
    "                               scope=\".*logits.*\")\n",
    "    train_vars_4 = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
    "                               scope=\".*logits|hidden[5].*\")\n",
    "    train_vars_3 = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
    "                               scope=\".*logits|hidden[4-5].*\")\n",
    "    train_vars_2 = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
    "                               scope=\".*logits|hidden[3-5].*\")\n",
    "    train_vars_1 = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
    "                               scope=\".*logits|hidden[2-5].*\")\n",
    "    train_vars_0 = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
    "                               scope=\".*logits|hidden[1-5].*\")\n",
    "    \n",
    "\n",
    "    loss = graph.get_collection(\"loss_ops\", scope=\"loss/loss\")[0]\n",
    "    \n",
    "\n",
    "    training_op_5 = tf.train.AdamOptimizer(0.01).minimize(loss, var_list=train_vars_5)\n",
    "    training_op_4 = tf.train.AdamOptimizer(0.01).minimize(loss, var_list=train_vars_4)\n",
    "    training_op_3 = tf.train.AdamOptimizer(0.01).minimize(loss, var_list=train_vars_3)\n",
    "    training_op_2 = tf.train.AdamOptimizer(0.01).minimize(loss, var_list=train_vars_2)\n",
    "    training_op_1 = tf.train.AdamOptimizer(0.01).minimize(loss, var_list=train_vars_1)\n",
    "    training_op_0 = tf.train.AdamOptimizer(0.01).minimize(loss, var_list=train_vars_0)\n",
    "    \n",
    "    \n",
    "    acc = graph.get_collection(\"eval_ops\")[0]\n",
    "    inits = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### op_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-30T04:02:39.519549Z",
     "start_time": "2018-01-30T04:02:29.294720Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from temp/saves/20180130131041/15.ckpt\n",
      "1 The accuracy of training set is 0.4360\n",
      "1 The accuracy of val set is 0.4717\n",
      "2 The accuracy of training set is 0.5880\n",
      "2 The accuracy of val set is 0.6208\n",
      "3 The accuracy of training set is 0.6820\n",
      "3 The accuracy of val set is 0.6916\n",
      "4 The accuracy of training set is 0.7040\n",
      "4 The accuracy of val set is 0.7093\n",
      "5 The accuracy of training set is 0.7240\n",
      "5 The accuracy of val set is 0.7166\n",
      "6 The accuracy of training set is 0.7340\n",
      "6 The accuracy of val set is 0.7346\n",
      "7 The accuracy of training set is 0.7640\n",
      "7 The accuracy of val set is 0.7568\n",
      "8 The accuracy of training set is 0.7640\n",
      "8 The accuracy of val set is 0.7568\n",
      "9 The accuracy of training set is 0.7860\n",
      "9 The accuracy of val set is 0.7695\n",
      "10 The accuracy of training set is 0.7880\n",
      "10 The accuracy of val set is 0.7719\n",
      "11 The accuracy of training set is 0.7800\n",
      "11 The accuracy of val set is 0.7645\n",
      "12 The accuracy of training set is 0.7740\n",
      "12 The accuracy of val set is 0.7408\n",
      "13 The accuracy of training set is 0.7720\n",
      "13 The accuracy of val set is 0.7547\n",
      "14 The accuracy of training set is 0.7720\n",
      "14 The accuracy of val set is 0.7551\n",
      "15 The accuracy of training set is 0.7880\n",
      "15 The accuracy of val set is 0.7592\n",
      "Early Stopping!\n",
      "------------------------------\n",
      "The final accuracy of test set is 0.7519\n",
      "------------------------------\n",
      "The total time cost for this to train is: 9.98s\n",
      "Time per epoch is: 0.67s\n"
     ]
    }
   ],
   "source": [
    "batch_size=50\n",
    "training_size=X_train_59.shape[0]\n",
    "time_now = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "if not os.path.isdir(\"temp/more_layer/%s\"%(time_now)):\n",
    "    os.makedirs(\"temp/more_layer/%s\" % time_now)\n",
    "\n",
    "\n",
    "    \n",
    "c_max = 5\n",
    "c = 0\n",
    "max_acc = -np.inf\n",
    "    \n",
    "tstart = time()\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    with graph.as_default():\n",
    "        inits.run()\n",
    "        res_saver.restore(sess,dnn_clf.last_epoch_model)\n",
    "        for epoch in range(1, 50):\n",
    "            for batch in range(1, np.ceil(training_size/\n",
    "                                      batch_size).astype(int)+1):\n",
    "\n",
    "                X_batch, y_batch =  dnn_clf._get_batch(X_train_59, \n",
    "                                                       y_train_59, \n",
    "                                                       batch_size, \n",
    "                                                       epoch, batch)\n",
    "\n",
    "                sess.run(training_op_5, \n",
    "                         feed_dict={X:X_batch, \n",
    "                                    y:y_batch, \n",
    "                                    training:True})\n",
    "\n",
    "\n",
    "            y_pred_train, true_label_train = sess.run([y_pred, true_labels],\n",
    "                                         feed_dict={X:X_train_59, \n",
    "                                                    y:y_train_59})\n",
    "            \n",
    "            \n",
    "            print(epoch, \"The accuracy of training set is %.4f\" % \n",
    "                  accuracy_score(true_label_train, y_pred_train))\n",
    "            y_pred_val, true_label_val= sess.run([y_pred, true_labels],\n",
    "                                                  feed_dict={X:X_valid_59, \n",
    "                                                  y:y_valid_59})\n",
    "            acc_val = accuracy_score(true_label_val, y_pred_val)\n",
    "            print(epoch, \"The accuracy of val set is %.4f\" % acc_val)\n",
    "                \n",
    "            save_path = res_saver.save(sess, \n",
    "                                \"temp/freezed/%s/%d.ckpt\"%(time_now, epoch))\n",
    "            \n",
    "            if acc_val > max_acc:\n",
    "                c = 0\n",
    "                max_acc = acc_val\n",
    "                final_save_path = save_path\n",
    "            else:\n",
    "                c += 1\n",
    "            if c >= 5:\n",
    "                print(\"Early Stopping!\")\n",
    "                break\n",
    "\n",
    "    y_pred_test, true_label_test= sess.run([y_pred, true_labels],\n",
    "                                          feed_dict={X:X_test_59, \n",
    "                                          y:y_test_59})\n",
    "    acc_test_5 = accuracy_score(true_label_test, y_pred_test)\n",
    "    print(\"------------------------------\\n\\\n",
    "The final accuracy of test set is %.4f\\n\\\n",
    "------------------------------\" % acc_test_5)\n",
    "\n",
    "tend = time()\n",
    "\n",
    "print(\"The total time cost for this to train is: %.2fs\" % (tend-tstart))\n",
    "print(\"Time per epoch is: %.2fs\" % ((tend-tstart)/epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### op_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-30T04:05:18.100848Z",
     "start_time": "2018-01-30T04:05:09.412478Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from temp/saves/20180130131041/15.ckpt\n",
      "1 The accuracy of training set is 0.6820\n",
      "1 The accuracy of val set is 0.6581\n",
      "2 The accuracy of training set is 0.7820\n",
      "2 The accuracy of val set is 0.7686\n",
      "3 The accuracy of training set is 0.8060\n",
      "3 The accuracy of val set is 0.7723\n",
      "4 The accuracy of training set is 0.8220\n",
      "4 The accuracy of val set is 0.7817\n",
      "5 The accuracy of training set is 0.8460\n",
      "5 The accuracy of val set is 0.7916\n",
      "6 The accuracy of training set is 0.8740\n",
      "6 The accuracy of val set is 0.8157\n",
      "7 The accuracy of training set is 0.9060\n",
      "7 The accuracy of val set is 0.8063\n",
      "8 The accuracy of training set is 0.8600\n",
      "8 The accuracy of val set is 0.7973\n",
      "9 The accuracy of training set is 0.8800\n",
      "9 The accuracy of val set is 0.7940\n",
      "10 The accuracy of training set is 0.9220\n",
      "10 The accuracy of val set is 0.8088\n",
      "11 The accuracy of training set is 0.9160\n",
      "11 The accuracy of val set is 0.8149\n",
      "Early Stopping!\n",
      "------------------------------\n",
      "The final accuracy of test set is 0.8101\n",
      "------------------------------\n",
      "The total time cost for this to train is: 9.10s\n",
      "Time per epoch is: 0.83s\n"
     ]
    }
   ],
   "source": [
    "batch_size=50\n",
    "training_size=X_train_59.shape[0]\n",
    "time_now = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "if not os.path.isdir(\"temp/more_layer/%s\"%(time_now)):\n",
    "    os.makedirs(\"temp/more_layer/%s\" % time_now)\n",
    "\n",
    "\n",
    "    \n",
    "c_max = 5\n",
    "c = 0\n",
    "max_acc = -np.inf\n",
    "    \n",
    "tstart = time()\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    with graph.as_default():\n",
    "        inits.run()\n",
    "        res_saver.restore(sess,dnn_clf.last_epoch_model)\n",
    "        for epoch in range(1, 50):\n",
    "            for batch in range(1, np.ceil(training_size/\n",
    "                                      batch_size).astype(int)+1):\n",
    "\n",
    "                X_batch, y_batch =  dnn_clf._get_batch(X_train_59, \n",
    "                                                       y_train_59, \n",
    "                                                       batch_size, \n",
    "                                                       epoch, batch)\n",
    "\n",
    "                sess.run(training_op_4, \n",
    "                         feed_dict={X:X_batch, \n",
    "                                    y:y_batch, \n",
    "                                    training:True})\n",
    "\n",
    "\n",
    "            y_pred_train, true_label_train = sess.run([y_pred, true_labels],\n",
    "                                         feed_dict={X:X_train_59, \n",
    "                                                    y:y_train_59})\n",
    "            \n",
    "            \n",
    "            print(epoch, \"The accuracy of training set is %.4f\" % \n",
    "                  accuracy_score(true_label_train, y_pred_train))\n",
    "            y_pred_val, true_label_val= sess.run([y_pred, true_labels],\n",
    "                                                  feed_dict={X:X_valid_59, \n",
    "                                                  y:y_valid_59})\n",
    "            acc_val = accuracy_score(true_label_val, y_pred_val)\n",
    "            print(epoch, \"The accuracy of val set is %.4f\" % acc_val)\n",
    "                \n",
    "            save_path = res_saver.save(sess, \n",
    "                                \"temp/freezed/%s/%d.ckpt\"%(time_now, epoch))\n",
    "            \n",
    "            if acc_val > max_acc:\n",
    "                c = 0\n",
    "                max_acc = acc_val\n",
    "                final_save_path = save_path\n",
    "            else:\n",
    "                c += 1\n",
    "            if c >= 5:\n",
    "                print(\"Early Stopping!\")\n",
    "                break\n",
    "                \n",
    "    y_pred_test, true_label_test= sess.run([y_pred, true_labels],\n",
    "                                          feed_dict={X:X_test_59, \n",
    "                                          y:y_test_59})\n",
    "    acc_test_4 = accuracy_score(true_label_test, y_pred_test)\n",
    "    print(\"------------------------------\\n\\\n",
    "The final accuracy of test set is %.4f\\n\\\n",
    "------------------------------\" % acc_test_4)\n",
    "\n",
    "tend = time()\n",
    "\n",
    "print(\"The total time cost for this to train is: %.2fs\" % (tend-tstart))\n",
    "print(\"Time per epoch is: %.2fs\" % ((tend-tstart)/epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### op_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-30T04:05:18.100848Z",
     "start_time": "2018-01-30T04:05:09.412478Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from temp/saves/20180130131041/15.ckpt\n",
      "1 The accuracy of training set is 0.7280\n",
      "1 The accuracy of val set is 0.6945\n",
      "2 The accuracy of training set is 0.8140\n",
      "2 The accuracy of val set is 0.7764\n",
      "3 The accuracy of training set is 0.8760\n",
      "3 The accuracy of val set is 0.8280\n",
      "4 The accuracy of training set is 0.8900\n",
      "4 The accuracy of val set is 0.8174\n",
      "5 The accuracy of training set is 0.8740\n",
      "5 The accuracy of val set is 0.8088\n",
      "6 The accuracy of training set is 0.9200\n",
      "6 The accuracy of val set is 0.8579\n",
      "7 The accuracy of training set is 0.9220\n",
      "7 The accuracy of val set is 0.8256\n",
      "8 The accuracy of training set is 0.9180\n",
      "8 The accuracy of val set is 0.8272\n",
      "9 The accuracy of training set is 0.9120\n",
      "9 The accuracy of val set is 0.8182\n",
      "10 The accuracy of training set is 0.9600\n",
      "10 The accuracy of val set is 0.8489\n",
      "11 The accuracy of training set is 0.9680\n",
      "11 The accuracy of val set is 0.8579\n",
      "Early Stopping!\n",
      "------------------------------\n",
      "The final accuracy of test set is 0.8504\n",
      "------------------------------\n",
      "The total time cost for this to train is: 6.78s\n",
      "Time per epoch is: 0.62s\n"
     ]
    }
   ],
   "source": [
    "batch_size=50\n",
    "training_size=X_train_59.shape[0]\n",
    "time_now = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "if not os.path.isdir(\"temp/more_layer/%s\"%(time_now)):\n",
    "    os.makedirs(\"temp/more_layer/%s\" % time_now)\n",
    "\n",
    "\n",
    "    \n",
    "c_max = 5\n",
    "c = 0\n",
    "max_acc = -np.inf\n",
    "    \n",
    "tstart = time()\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    with graph.as_default():\n",
    "        inits.run()\n",
    "        res_saver.restore(sess,dnn_clf.last_epoch_model)\n",
    "        for epoch in range(1, 50):\n",
    "            for batch in range(1, np.ceil(training_size/\n",
    "                                      batch_size).astype(int)+1):\n",
    "\n",
    "                X_batch, y_batch =  dnn_clf._get_batch(X_train_59, \n",
    "                                                       y_train_59, \n",
    "                                                       batch_size, \n",
    "                                                       epoch, batch)\n",
    "\n",
    "                sess.run(training_op_3, \n",
    "                         feed_dict={X:X_batch, \n",
    "                                    y:y_batch, \n",
    "                                    training:True})\n",
    "\n",
    "\n",
    "            y_pred_train, true_label_train = sess.run([y_pred, true_labels],\n",
    "                                         feed_dict={X:X_train_59, \n",
    "                                                    y:y_train_59})\n",
    "            \n",
    "            \n",
    "            print(epoch, \"The accuracy of training set is %.4f\" % \n",
    "                  accuracy_score(true_label_train, y_pred_train))\n",
    "            y_pred_val, true_label_val= sess.run([y_pred, true_labels],\n",
    "                                                  feed_dict={X:X_valid_59, \n",
    "                                                  y:y_valid_59})\n",
    "            acc_val = accuracy_score(true_label_val, y_pred_val)\n",
    "            print(epoch, \"The accuracy of val set is %.4f\" % acc_val)\n",
    "                \n",
    "            save_path = res_saver.save(sess, \n",
    "                                \"temp/freezed/%s/%d.ckpt\"%(time_now, epoch))\n",
    "            \n",
    "            if acc_val > max_acc:\n",
    "                c = 0\n",
    "                max_acc = acc_val\n",
    "                final_save_path = save_path\n",
    "            else:\n",
    "                c += 1\n",
    "            if c >= c_max:\n",
    "                print(\"Early Stopping!\")\n",
    "                break\n",
    "                \n",
    "    y_pred_test, true_label_test= sess.run([y_pred, true_labels],\n",
    "                                          feed_dict={X:X_test_59, \n",
    "                                          y:y_test_59})\n",
    "    acc_test_3 = accuracy_score(true_label_test, y_pred_test)\n",
    "    print(\"------------------------------\\n\\\n",
    "The final accuracy of test set is %.4f\\n\\\n",
    "------------------------------\" % acc_test_3)\n",
    "\n",
    "tend = time()\n",
    "\n",
    "print(\"The total time cost for this to train is: %.2fs\" % (tend-tstart))\n",
    "print(\"Time per epoch is: %.2fs\" % ((tend-tstart)/epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### op_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-30T04:05:18.100848Z",
     "start_time": "2018-01-30T04:05:09.412478Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from temp/saves/20180130131041/15.ckpt\n",
      "1 The accuracy of training set is 0.7160\n",
      "1 The accuracy of val set is 0.6794\n",
      "2 The accuracy of training set is 0.8560\n",
      "2 The accuracy of val set is 0.8219\n",
      "3 The accuracy of training set is 0.9060\n",
      "3 The accuracy of val set is 0.8518\n",
      "4 The accuracy of training set is 0.9200\n",
      "4 The accuracy of val set is 0.8522\n",
      "5 The accuracy of training set is 0.9260\n",
      "5 The accuracy of val set is 0.8616\n",
      "6 The accuracy of training set is 0.9080\n",
      "6 The accuracy of val set is 0.8481\n",
      "7 The accuracy of training set is 0.9380\n",
      "7 The accuracy of val set is 0.8616\n",
      "8 The accuracy of training set is 0.9560\n",
      "8 The accuracy of val set is 0.8747\n",
      "9 The accuracy of training set is 0.9540\n",
      "9 The accuracy of val set is 0.8923\n",
      "10 The accuracy of training set is 0.9500\n",
      "10 The accuracy of val set is 0.8710\n",
      "11 The accuracy of training set is 0.9640\n",
      "11 The accuracy of val set is 0.8784\n",
      "12 The accuracy of training set is 0.9520\n",
      "12 The accuracy of val set is 0.8669\n",
      "13 The accuracy of training set is 0.9740\n",
      "13 The accuracy of val set is 0.8706\n",
      "14 The accuracy of training set is 0.9720\n",
      "14 The accuracy of val set is 0.8661\n",
      "Early Stopping!\n",
      "------------------------------\n",
      "The final accuracy of test set is 0.8653\n",
      "------------------------------\n",
      "The total time cost for this to train is: 8.08s\n",
      "Time per epoch is: 0.58s\n"
     ]
    }
   ],
   "source": [
    "batch_size=50\n",
    "training_size=X_train_59.shape[0]\n",
    "time_now = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "if not os.path.isdir(\"temp/more_layer/%s\"%(time_now)):\n",
    "    os.makedirs(\"temp/more_layer/%s\" % time_now)\n",
    "\n",
    "\n",
    "    \n",
    "c_max = 5\n",
    "c = 0\n",
    "max_acc = -np.inf\n",
    "    \n",
    "tstart = time()\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    with graph.as_default():\n",
    "        inits.run()\n",
    "        res_saver.restore(sess,dnn_clf.last_epoch_model)\n",
    "        for epoch in range(1, 50):\n",
    "            for batch in range(1, np.ceil(training_size/\n",
    "                                      batch_size).astype(int)+1):\n",
    "\n",
    "                X_batch, y_batch =  dnn_clf._get_batch(X_train_59, \n",
    "                                                       y_train_59, \n",
    "                                                       batch_size, \n",
    "                                                       epoch, batch)\n",
    "\n",
    "                sess.run(training_op_2, \n",
    "                         feed_dict={X:X_batch, \n",
    "                                    y:y_batch, \n",
    "                                    training:True})\n",
    "\n",
    "\n",
    "            y_pred_train, true_label_train = sess.run([y_pred, true_labels],\n",
    "                                         feed_dict={X:X_train_59, \n",
    "                                                    y:y_train_59})\n",
    "            \n",
    "            \n",
    "            print(epoch, \"The accuracy of training set is %.4f\" % \n",
    "                  accuracy_score(true_label_train, y_pred_train))\n",
    "            y_pred_val, true_label_val= sess.run([y_pred, true_labels],\n",
    "                                                  feed_dict={X:X_valid_59, \n",
    "                                                  y:y_valid_59})\n",
    "            acc_val = accuracy_score(true_label_val, y_pred_val)\n",
    "            print(epoch, \"The accuracy of val set is %.4f\" % acc_val)\n",
    "                \n",
    "            save_path = res_saver.save(sess, \n",
    "                                \"temp/freezed/%s/%d.ckpt\"%(time_now, epoch))\n",
    "            \n",
    "            if acc_val > max_acc:\n",
    "                c = 0\n",
    "                max_acc = acc_val\n",
    "                final_save_path = save_path\n",
    "            else:\n",
    "                c += 1\n",
    "            if c >= 5:\n",
    "                print(\"Early Stopping!\")\n",
    "                break\n",
    "                \n",
    "    y_pred_test, true_label_test= sess.run([y_pred, true_labels],\n",
    "                                          feed_dict={X:X_test_59, \n",
    "                                          y:y_test_59})\n",
    "    acc_test_2 = accuracy_score(true_label_test, y_pred_test)\n",
    "    print(\"------------------------------\\n\\\n",
    "The final accuracy of test set is %.4f\\n\\\n",
    "------------------------------\" % acc_test_2)\n",
    "\n",
    "tend = time()\n",
    "\n",
    "print(\"The total time cost for this to train is: %.2fs\" % (tend-tstart))\n",
    "print(\"Time per epoch is: %.2fs\" % ((tend-tstart)/epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### op_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-30T04:05:18.100848Z",
     "start_time": "2018-01-30T04:05:09.412478Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from temp/saves/20180130131041/15.ckpt\n",
      "1 The accuracy of training set is 0.7480\n",
      "1 The accuracy of val set is 0.7609\n",
      "2 The accuracy of training set is 0.8500\n",
      "2 The accuracy of val set is 0.8104\n",
      "3 The accuracy of training set is 0.8980\n",
      "3 The accuracy of val set is 0.8473\n",
      "4 The accuracy of training set is 0.8920\n",
      "4 The accuracy of val set is 0.8575\n",
      "5 The accuracy of training set is 0.9120\n",
      "5 The accuracy of val set is 0.8710\n",
      "6 The accuracy of training set is 0.9260\n",
      "6 The accuracy of val set is 0.8477\n",
      "7 The accuracy of training set is 0.9620\n",
      "7 The accuracy of val set is 0.9025\n",
      "8 The accuracy of training set is 0.9320\n",
      "8 The accuracy of val set is 0.8743\n",
      "9 The accuracy of training set is 0.9700\n",
      "9 The accuracy of val set is 0.9025\n",
      "10 The accuracy of training set is 0.9700\n",
      "10 The accuracy of val set is 0.9066\n",
      "11 The accuracy of training set is 0.9800\n",
      "11 The accuracy of val set is 0.9140\n",
      "12 The accuracy of training set is 0.9780\n",
      "12 The accuracy of val set is 0.9050\n",
      "13 The accuracy of training set is 0.9660\n",
      "13 The accuracy of val set is 0.8948\n",
      "14 The accuracy of training set is 0.9840\n",
      "14 The accuracy of val set is 0.8976\n",
      "15 The accuracy of training set is 0.9820\n",
      "15 The accuracy of val set is 0.9021\n",
      "16 The accuracy of training set is 0.9400\n",
      "16 The accuracy of val set is 0.8907\n",
      "Early Stopping!\n",
      "------------------------------\n",
      "The final accuracy of test set is 0.8817\n",
      "------------------------------\n",
      "The total time cost for this to train is: 8.51s\n",
      "Time per epoch is: 0.53s\n"
     ]
    }
   ],
   "source": [
    "batch_size=50\n",
    "training_size=X_train_59.shape[0]\n",
    "time_now = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "if not os.path.isdir(\"temp/more_layer/%s\"%(time_now)):\n",
    "    os.makedirs(\"temp/more_layer/%s\" % time_now)\n",
    "\n",
    "\n",
    "    \n",
    "c_max = 5\n",
    "c = 0\n",
    "max_acc = -np.inf\n",
    "    \n",
    "tstart = time()\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    with graph.as_default():\n",
    "        inits.run()\n",
    "        res_saver.restore(sess,dnn_clf.last_epoch_model)\n",
    "        for epoch in range(1, 50):\n",
    "            for batch in range(1, np.ceil(training_size/\n",
    "                                      batch_size).astype(int)+1):\n",
    "\n",
    "                X_batch, y_batch =  dnn_clf._get_batch(X_train_59, \n",
    "                                                       y_train_59, \n",
    "                                                       batch_size, \n",
    "                                                       epoch, batch)\n",
    "\n",
    "                sess.run(training_op_1, \n",
    "                         feed_dict={X:X_batch, \n",
    "                                    y:y_batch, \n",
    "                                    training:True})\n",
    "\n",
    "\n",
    "            y_pred_train, true_label_train = sess.run([y_pred, true_labels],\n",
    "                                         feed_dict={X:X_train_59, \n",
    "                                                    y:y_train_59})\n",
    "            \n",
    "            \n",
    "            print(epoch, \"The accuracy of training set is %.4f\" % \n",
    "                  accuracy_score(true_label_train, y_pred_train))\n",
    "            y_pred_val, true_label_val= sess.run([y_pred, true_labels],\n",
    "                                                  feed_dict={X:X_valid_59, \n",
    "                                                  y:y_valid_59})\n",
    "            acc_val = accuracy_score(true_label_val, y_pred_val)\n",
    "            print(epoch, \"The accuracy of val set is %.4f\" % acc_val)\n",
    "                \n",
    "            save_path = res_saver.save(sess, \n",
    "                                \"temp/freezed/%s/%d.ckpt\"%(time_now, epoch))\n",
    "            \n",
    "            if acc_val > max_acc:\n",
    "                c = 0\n",
    "                max_acc = acc_val\n",
    "                final_save_path = save_path\n",
    "            else:\n",
    "                c += 1\n",
    "            if c >= 5:\n",
    "                print(\"Early Stopping!\")\n",
    "                break\n",
    "\n",
    "    y_pred_test, true_label_test= sess.run([y_pred, true_labels],\n",
    "                                          feed_dict={X:X_test_59, \n",
    "                                          y:y_test_59})\n",
    "    acc_test_1 = accuracy_score(true_label_test, y_pred_test)\n",
    "    print(\"------------------------------\\n\\\n",
    "The final accuracy of test set is %.4f\\n\\\n",
    "------------------------------\" % acc_test_1)\n",
    "                \n",
    "tend = time()\n",
    "\n",
    "print(\"The total time cost for this to train is: %.2fs\" % (tend-tstart))\n",
    "print(\"Time per epoch is: %.2fs\" % ((tend-tstart)/epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### op_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-30T04:05:31.361631Z",
     "start_time": "2018-01-30T04:05:25.520930Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from temp/saves/20180130131041/15.ckpt\n",
      "1 The accuracy of training set is 0.5360\n",
      "1 The accuracy of val set is 0.5246\n",
      "2 The accuracy of training set is 0.7860\n",
      "2 The accuracy of val set is 0.7432\n",
      "3 The accuracy of training set is 0.7060\n",
      "3 The accuracy of val set is 0.6941\n",
      "4 The accuracy of training set is 0.8380\n",
      "4 The accuracy of val set is 0.8092\n",
      "5 The accuracy of training set is 0.9460\n",
      "5 The accuracy of val set is 0.9050\n",
      "6 The accuracy of training set is 0.9500\n",
      "6 The accuracy of val set is 0.8927\n",
      "7 The accuracy of training set is 0.9520\n",
      "7 The accuracy of val set is 0.8755\n",
      "8 The accuracy of training set is 0.9360\n",
      "8 The accuracy of val set is 0.8911\n",
      "9 The accuracy of training set is 0.9560\n",
      "9 The accuracy of val set is 0.8943\n",
      "10 The accuracy of training set is 0.8860\n",
      "10 The accuracy of val set is 0.8538\n",
      "Early Stopping!\n",
      "------------------------------\n",
      "The final accuracy of test set is 0.8428\n",
      "------------------------------\n",
      "The total time cost for this to train is: 5.70s\n",
      "Time per epoch is: 0.57s\n"
     ]
    }
   ],
   "source": [
    "batch_size=50\n",
    "training_size=X_train_59.shape[0]\n",
    "time_now = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "if not os.path.isdir(\"temp/more_layer/%s\"%(time_now)):\n",
    "    os.makedirs(\"temp/more_layer/%s\" % time_now)\n",
    "\n",
    "\n",
    "    \n",
    "c_max = 5\n",
    "c = 0\n",
    "max_acc = -np.inf\n",
    "    \n",
    "tstart = time()\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    with graph.as_default():\n",
    "        inits.run()\n",
    "        res_saver.restore(sess,dnn_clf.last_epoch_model)\n",
    "        for epoch in range(1, 50):\n",
    "            for batch in range(1, np.ceil(training_size/\n",
    "                                      batch_size).astype(int)+1):\n",
    "\n",
    "                X_batch, y_batch =  dnn_clf._get_batch(X_train_59, \n",
    "                                                       y_train_59, \n",
    "                                                       batch_size, \n",
    "                                                       epoch, batch)\n",
    "\n",
    "                sess.run(training_op_0, \n",
    "                         feed_dict={X:X_batch, \n",
    "                                    y:y_batch, \n",
    "                                    training:True})\n",
    "\n",
    "\n",
    "            y_pred_train, true_label_train = sess.run([y_pred, true_labels],\n",
    "                                         feed_dict={X:X_train_59, \n",
    "                                                    y:y_train_59})\n",
    "            \n",
    "            \n",
    "            print(epoch, \"The accuracy of training set is %.4f\" % \n",
    "                  accuracy_score(true_label_train, y_pred_train))\n",
    "            y_pred_val, true_label_val= sess.run([y_pred, true_labels],\n",
    "                                                  feed_dict={X:X_valid_59, \n",
    "                                                  y:y_valid_59})\n",
    "            acc_val = accuracy_score(true_label_val, y_pred_val)\n",
    "            print(epoch, \"The accuracy of val set is %.4f\" % acc_val)\n",
    "                \n",
    "            save_path = res_saver.save(sess, \n",
    "                                \"temp/freezed/%s/%d.ckpt\"%(time_now, epoch))\n",
    "            \n",
    "            if acc_val > max_acc:\n",
    "                c = 0\n",
    "                max_acc = acc_val\n",
    "                final_save_path = save_path\n",
    "            else:\n",
    "                c += 1\n",
    "            if c >= 5:\n",
    "                print(\"Early Stopping!\")\n",
    "                break\n",
    "                \n",
    "    y_pred_test, true_label_test= sess.run([y_pred, true_labels],\n",
    "                                          feed_dict={X:X_test_59, \n",
    "                                          y:y_test_59})\n",
    "    acc_test_0 = accuracy_score(true_label_test, y_pred_test)\n",
    "    print(\"------------------------------\\n\\\n",
    "The final accuracy of test set is %.4f\\n\\\n",
    "------------------------------\" % acc_test_0)\n",
    "\n",
    "tend = time()\n",
    "\n",
    "print(\"The total time cost for this to train is: %.2fs\" % (tend-tstart))\n",
    "print(\"Time per epoch is: %.2fs\" % ((tend-tstart)/epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "variables": {
     "acc_test_0": "0.8428306932729891",
     "acc_test_1": "0.8817115819790167",
     "acc_test_2": "0.8652540629500103",
     "acc_test_3": "0.8504422958239045",
     "acc_test_4": "0.8101213742028389",
     "acc_test_5": "0.7519029006377289"
    }
   },
   "source": [
    "The accuracy of freezing different layers are:\n",
    "<ol>\n",
    "<li><pre>only logits     {{acc_test_5}}</pre></li>\n",
    "<li><pre>logits +  5     {{acc_test_4}}</pre></li>\n",
    "<li><pre>logits +4-5     {{acc_test_3}}</pre></li>\n",
    "<li><pre>logits +3-5     {{acc_test_2}}</pre></li>\n",
    "<li><pre>logits +2-5     {{acc_test_1}}</pre></li>\n",
    "<li><pre>logits +1-5     {{acc_test_0}}</pre></li>\n",
    "</ol>\n",
    "There seems to be a increase in accuracy when unlocking more layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-02-01T13:09:02.151Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 483,
   "position": {
    "height": "505px",
    "left": "1542px",
    "right": "20px",
    "top": "67px",
    "width": "334px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
